import{S as ro,i as ao,s as io,e as n,k as s,w as S,t as a,M as so,c as o,d as l,m as c,a as r,x as F,h as i,b as f,G as t,g as p,y as Q,L as co,q as z,o as O,B as q,v as fo}from"../chunks/vendor-hf-doc-builder.js";import{I as po}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ne}from"../chunks/CodeBlock-hf-doc-builder.js";function ho(sn){let g,ze,I,P,Xe,k,vt,$e,Ut,Oe,K,Et,qe,ee,gt,Ke,v,N,It,Pt,X,Rt,jt,$,Dt,xt,et,E,Lt,Ae,kt,Nt,Be,Xt,$t,tt,R,At,A,Bt,Gt,lt,te,Wt,nt,le,Vt,ot,ne,Yt,rt,j,Ge,h,oe,Ht,Zt,re,St,Ft,ae,Qt,zt,ie,Ot,qt,se,Kt,el,ce,tl,ll,U,u,de,nl,ol,We,rl,fe,al,il,pe,sl,cl,he,dl,fl,ue,pl,hl,T,Te,ul,Tl,Ve,ml,me,wl,_l,we,yl,Ml,_e,bl,Cl,ye,Jl,vl,m,Me,Ul,El,Ye,gl,be,Il,Pl,Ce,Rl,jl,Je,Dl,xl,ve,Ll,kl,w,Ue,Nl,Xl,Ee,$l,Al,ge,Bl,Gl,Ie,Wl,Vl,Pe,Yl,Hl,He,at,B,it,_,Zl,Ze,Sl,Fl,Se,Ql,zl,Fe,Ol,ql,st,Re,Kl,ct,G,dt,je,en,ft,W,pt,De,tn,ht,V,ut,xe,ln,Tt,Le,nn,mt,Y,wt,D,on,Qe,rn,an,_t,H,yt;return k=new po({}),B=new Ne({props:{code:"cGlwJTIwaW5zdGFsbCUyMG9uZWNjbF9iaW5kX3B0JTNEJTNEJTdCcHl0b3JjaF92ZXJzaW9uJTdEJTIwLWYlMjBodHRwcyUzQSUyRiUyRnNvZnR3YXJlLmludGVsLmNvbSUyRmlwZXgtd2hsLXN0YWJsZQ==",highlighted:'pip install oneccl_bind_pt=={<span class="hljs-attribute">pytorch_version} -f https</span>://software<span class="hljs-variable">.intel</span><span class="hljs-variable">.com</span>/ipex-whl-stable'}}),G=new Ne({props:{code:"b25lY2NsX2JpbmRpbmdzX2Zvcl9weXRvcmNoX3BhdGglM0QlMjQocHl0aG9uJTIwLWMlMjAlMjJmcm9tJTIwb25lY2NsX2JpbmRpbmdzX2Zvcl9weXRvcmNoJTIwaW1wb3J0JTIwY3dkJTNCJTIwcHJpbnQoY3dkKSUyMiklMEFzb3VyY2UlMjAlMjRvbmVjY2xfYmluZGluZ3NfZm9yX3B5dG9yY2hfcGF0aCUyRmVudiUyRnNldHZhcnMuc2g=",highlighted:`oneccl_bindings_for_pytorch_path=$(<span class="hljs-keyword">python</span> -c <span class="hljs-string">&quot;from oneccl_bindings_for_pytorch import cwd; print(cwd)&quot;</span>)
<span class="hljs-keyword">source</span> $oneccl_bindings_for_pytorch_path/<span class="hljs-keyword">env</span>/setvars.sh`}}),W=new Ne({props:{code:"dG9yY2hfY2NsX3BhdGglM0QlMjQocHl0aG9uJTIwLWMlMjAlMjJpbXBvcnQlMjB0b3JjaCUzQiUyMGltcG9ydCUyMHRvcmNoX2NjbCUzQiUyMGltcG9ydCUyMG9zJTNCJTIwJTIwcHJpbnQob3MucGF0aC5hYnNwYXRoKG9zLnBhdGguZGlybmFtZSh0b3JjaF9jY2wuX19maWxlX18pKSklMjIpJTBBc291cmNlJTIwJTI0dG9yY2hfY2NsX3BhdGglMkZlbnYlMkZzZXR2YXJzLnNo",highlighted:`torch_ccl_path=$(<span class="hljs-keyword">python</span> -c <span class="hljs-string">&quot;import torch; import torch_ccl; import os;  print(os.path.abspath(os.path.dirname(torch_ccl.__file__)))&quot;</span>)
<span class="hljs-keyword">source</span> $torch_ccl_path/<span class="hljs-keyword">env</span>/setvars.sh`}}),V=new Ne({props:{code:"ZXhwb3J0JTIwQ0NMX1dPUktFUl9DT1VOVCUzRDElMEFleHBvcnQlMjBNQVNURVJfQUREUiUzRDEyNy4wLjAuMSUwQW1waXJ1biUyMC1uJTIwMiUyMC1nZW52JTIwT01QX05VTV9USFJFQURTJTNEMjMlMjAlNUMlMEFweXRob24zJTIwcnVuX3FhLnB5JTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1tb2RlbF9uYW1lX29yX3BhdGglMjBkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1kaXN0aWxsZWQtc3F1YWQlMjAlNUMlMEElMjAlMjAlMjAlMjAtLWRhdGFzZXRfbmFtZSUyMHNxdWFkJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1hcHBseV9xdWFudGl6YXRpb24lMjAlNUMlMEElMjAlMjAlMjAlMjAtLXF1YW50aXphdGlvbl9hcHByb2FjaCUyMHN0YXRpYyUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tZG9fdHJhaW4lMjAlNUMlMEElMjAlMjAlMjAlMjAtLWRvX2V2YWwlMjAlNUMlMEElMjAlMjAlMjAlMjAtLXZlcmlmeV9sb2FkaW5nJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1vdXRwdXRfZGlyJTIwJTJGdG1wJTJGc3F1YWRfb3V0cHV0JTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1ub19jdWRhJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS14cHVfYmFja2VuZCUyMGNjbA==",highlighted:`export CCL_WORKER_COUNT=1
export MASTER_ADDR=127.0.0.1
mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
python3 run_qa.py \\
    --model_name_or_path distilbert-base-uncased-distilled-squad \\
    --dataset_name squad \\
    --apply_quantization \\
    --quantization_approach static \\
    --do_train \\
    --do_eval \\
    --verify_loading \\
    --output_dir /tmp/squad_output \\
    --no_cuda \\
    --xpu_backend ccl`}}),Y=new Ne({props:{code:"JTIwY2F0JTIwaG9zdGZpbGUlMEElMjB4eHgueHh4Lnh4eC54eHglMjAlMjNub2RlMCUyMGlwJTBBJTIweHh4Lnh4eC54eHgueHh4JTIwJTIzbm9kZTElMjBpcA==",highlighted:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`}}),H=new Ne({props:{code:"ZXhwb3J0JTIwQ0NMX1dPUktFUl9DT1VOVCUzRDElMEFleHBvcnQlMjBNQVNURVJfQUREUiUzRHh4eC54eHgueHh4Lnh4eCUyMCUyM25vZGUwJTIwaXAlMEFtcGlydW4lMjAtZiUyMGhvc3RmaWxlJTIwLW4lMjA0JTIwLXBwbiUyMDIlMjAlNUMlMEEtZ2VudiUyME9NUF9OVU1fVEhSRUFEUyUzRDIzJTIwJTVDJTBBcHl0aG9uMyUyMHJ1bl9xYS5weSUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbW9kZWxfbmFtZV9vcl9wYXRoJTIwZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQtZGlzdGlsbGVkLXNxdWFkJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1kYXRhc2V0X25hbWUlMjBzcXVhZCUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tYXBwbHlfcXVhbnRpemF0aW9uJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1xdWFudGl6YXRpb25fYXBwcm9hY2glMjBzdGF0aWMlMjAlNUMlMEElMjAlMjAlMjAlMjAtLWRvX3RyYWluJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1kb19ldmFsJTIwJTVDJTBBJTIwJTIwJTIwJTIwLS12ZXJpZnlfbG9hZGluZyUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tb3V0cHV0X2RpciUyMCUyRnRtcCUyRnNxdWFkX291dHB1dCUyMCU1QyUwQSUyMCUyMCUyMCUyMC0tbm9fY3VkYSUyMCU1QyUwQSUyMCUyMCUyMCUyMC0teHB1X2JhY2tlbmQlMjBjY2w=",highlighted:`export CCL_WORKER_COUNT=1
export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
mpirun -f hostfile -n 4 -ppn 2 \\
-genv OMP_NUM_THREADS=23 \\
python3 run_qa.py \\
    --model_name_or_path distilbert-base-uncased-distilled-squad \\
    --dataset_name squad \\
    --apply_quantization \\
    --quantization_approach static \\
    --do_train \\
    --do_eval \\
    --verify_loading \\
    --output_dir /tmp/squad_output \\
    --no_cuda \\
    --xpu_backend ccl`}}),{c(){g=n("meta"),ze=s(),I=n("h1"),P=n("a"),Xe=n("span"),S(k.$$.fragment),vt=s(),$e=n("span"),Ut=a("Distributed training"),Oe=s(),K=n("p"),Et=a("When training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently."),qe=s(),ee=n("p"),gt=a("Distributed training on multiple CPUs is launched by mpirun which supports both Gloo and oneCCL as collective communication backends. And for performance seek, Intel recommends to use oneCCL backend."),Ke=s(),v=n("p"),N=n("a"),It=a("Intel\xAE oneCCL"),Pt=a(" (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the "),X=n("a"),Rt=a("oneCCL documentation"),jt=a(" and "),$=n("a"),Dt=a("oneCCL specification"),xt=a("."),et=s(),E=n("p"),Lt=a("Module "),Ae=n("code"),kt=a("oneccl_bindings_for_pytorch"),Nt=a(" ("),Be=n("code"),Xt=a("torch_ccl"),$t=a(" before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),tt=s(),R=n("p"),At=a("Check more detailed information for "),A=n("a"),Bt=a("oneccl_bind_pt"),Gt=a("."),lt=s(),te=n("p"),Wt=a("We will show how to use oneCCL backend-ed distributed training as below steps."),nt=s(),le=n("p"),Vt=a("Intel\xAE oneCCL Bindings for PyTorch installation:"),ot=s(),ne=n("p"),Yt=a("Wheel files are available for the following Python versions:"),rt=s(),j=n("table"),Ge=n("thead"),h=n("tr"),oe=n("th"),Ht=a("Extension Version"),Zt=s(),re=n("th"),St=a("Python 3.6"),Ft=s(),ae=n("th"),Qt=a("Python 3.7"),zt=s(),ie=n("th"),Ot=a("Python 3.8"),qt=s(),se=n("th"),Kt=a("Python 3.9"),el=s(),ce=n("th"),tl=a("Python 3.10"),ll=s(),U=n("tbody"),u=n("tr"),de=n("td"),nl=a("1.12.1"),ol=s(),We=n("td"),rl=s(),fe=n("td"),al=a("\u221A"),il=s(),pe=n("td"),sl=a("\u221A"),cl=s(),he=n("td"),dl=a("\u221A"),fl=s(),ue=n("td"),pl=a("\u221A"),hl=s(),T=n("tr"),Te=n("td"),ul=a("1.12.0"),Tl=s(),Ve=n("td"),ml=s(),me=n("td"),wl=a("\u221A"),_l=s(),we=n("td"),yl=a("\u221A"),Ml=s(),_e=n("td"),bl=a("\u221A"),Cl=s(),ye=n("td"),Jl=a("\u221A"),vl=s(),m=n("tr"),Me=n("td"),Ul=a("1.11.0"),El=s(),Ye=n("td"),gl=s(),be=n("td"),Il=a("\u221A"),Pl=s(),Ce=n("td"),Rl=a("\u221A"),jl=s(),Je=n("td"),Dl=a("\u221A"),xl=s(),ve=n("td"),Ll=a("\u221A"),kl=s(),w=n("tr"),Ue=n("td"),Nl=a("1.10.0"),Xl=s(),Ee=n("td"),$l=a("\u221A"),Al=s(),ge=n("td"),Bl=a("\u221A"),Gl=s(),Ie=n("td"),Wl=a("\u221A"),Vl=s(),Pe=n("td"),Yl=a("\u221A"),Hl=s(),He=n("td"),at=s(),S(B.$$.fragment),it=s(),_=n("p"),Zl=a("where "),Ze=n("code"),Sl=a("{pytorch_version}"),Fl=a(` should be your PyTorch version, for instance 1.12.0.
Versions of oneCCL and PyTorch must match.
`),Se=n("code"),Ql=a("oneccl_bindings_for_pytorch"),zl=a(` 1.12.0 prebuilt wheel does not work with PyTorch 1.12.1 (it is for PyTorch 1.12.0)
PyTorch 1.12.1 should work with `),Fe=n("code"),Ol=a("oneccl_bindings_for_pytorch"),ql=a(" 1.12.1"),st=s(),Re=n("p"),Kl=a("MPI tool set for Intel\xAE oneCCL 1.12.0"),ct=s(),S(G.$$.fragment),dt=s(),je=n("p"),en=a("for Intel\xAE oneCCL whose version < 1.12.0"),ft=s(),S(W.$$.fragment),pt=s(),De=n("p"),tn=a("The following command enables training with 2 processes on one node, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),ht=s(),S(V.$$.fragment),ut=s(),xe=n("p"),ln=a("The following command enables training with a total of four processes on two nodes (node0 and node1, taking node0 as the main process), ppn (processes per node) is set to 2, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Tt=s(),Le=n("p"),nn=a("In node0, you need to create a configuration file which contains the IP addresses of each node (for example hostfile) and pass that configuration file path as an argument."),mt=s(),S(Y.$$.fragment),wt=s(),D=n("p"),on=a("Now, run the following command in node0 and "),Qe=n("strong"),rn=a("4DDP"),an=a(" will be enabled in node0 and node1:"),_t=s(),S(H.$$.fragment),this.h()},l(e){const d=so('[data-svelte="svelte-1phssyn"]',document.head);g=o(d,"META",{name:!0,content:!0}),d.forEach(l),ze=c(e),I=o(e,"H1",{class:!0});var Mt=r(I);P=o(Mt,"A",{id:!0,class:!0,href:!0});var cn=r(P);Xe=o(cn,"SPAN",{});var dn=r(Xe);F(k.$$.fragment,dn),dn.forEach(l),cn.forEach(l),vt=c(Mt),$e=o(Mt,"SPAN",{});var fn=r($e);Ut=i(fn,"Distributed training"),fn.forEach(l),Mt.forEach(l),Oe=c(e),K=o(e,"P",{});var pn=r(K);Et=i(pn,"When training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently."),pn.forEach(l),qe=c(e),ee=o(e,"P",{});var hn=r(ee);gt=i(hn,"Distributed training on multiple CPUs is launched by mpirun which supports both Gloo and oneCCL as collective communication backends. And for performance seek, Intel recommends to use oneCCL backend."),hn.forEach(l),Ke=c(e),v=o(e,"P",{});var Z=r(v);N=o(Z,"A",{href:!0,rel:!0});var un=r(N);It=i(un,"Intel\xAE oneCCL"),un.forEach(l),Pt=i(Z," (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the "),X=o(Z,"A",{href:!0,rel:!0});var Tn=r(X);Rt=i(Tn,"oneCCL documentation"),Tn.forEach(l),jt=i(Z," and "),$=o(Z,"A",{href:!0,rel:!0});var mn=r($);Dt=i(mn,"oneCCL specification"),mn.forEach(l),xt=i(Z,"."),Z.forEach(l),et=c(e),E=o(e,"P",{});var ke=r(E);Lt=i(ke,"Module "),Ae=o(ke,"CODE",{});var wn=r(Ae);kt=i(wn,"oneccl_bindings_for_pytorch"),wn.forEach(l),Nt=i(ke," ("),Be=o(ke,"CODE",{});var _n=r(Be);Xt=i(_n,"torch_ccl"),_n.forEach(l),$t=i(ke," before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),ke.forEach(l),tt=c(e),R=o(e,"P",{});var bt=r(R);At=i(bt,"Check more detailed information for "),A=o(bt,"A",{href:!0,rel:!0});var yn=r(A);Bt=i(yn,"oneccl_bind_pt"),yn.forEach(l),Gt=i(bt,"."),bt.forEach(l),lt=c(e),te=o(e,"P",{});var Mn=r(te);Wt=i(Mn,"We will show how to use oneCCL backend-ed distributed training as below steps."),Mn.forEach(l),nt=c(e),le=o(e,"P",{});var bn=r(le);Vt=i(bn,"Intel\xAE oneCCL Bindings for PyTorch installation:"),bn.forEach(l),ot=c(e),ne=o(e,"P",{});var Cn=r(ne);Yt=i(Cn,"Wheel files are available for the following Python versions:"),Cn.forEach(l),rt=c(e),j=o(e,"TABLE",{});var Ct=r(j);Ge=o(Ct,"THEAD",{});var Jn=r(Ge);h=o(Jn,"TR",{});var y=r(h);oe=o(y,"TH",{align:!0});var vn=r(oe);Ht=i(vn,"Extension Version"),vn.forEach(l),Zt=c(y),re=o(y,"TH",{align:!0});var Un=r(re);St=i(Un,"Python 3.6"),Un.forEach(l),Ft=c(y),ae=o(y,"TH",{align:!0});var En=r(ae);Qt=i(En,"Python 3.7"),En.forEach(l),zt=c(y),ie=o(y,"TH",{align:!0});var gn=r(ie);Ot=i(gn,"Python 3.8"),gn.forEach(l),qt=c(y),se=o(y,"TH",{align:!0});var In=r(se);Kt=i(In,"Python 3.9"),In.forEach(l),el=c(y),ce=o(y,"TH",{align:!0});var Pn=r(ce);tl=i(Pn,"Python 3.10"),Pn.forEach(l),y.forEach(l),Jn.forEach(l),ll=c(Ct),U=o(Ct,"TBODY",{});var x=r(U);u=o(x,"TR",{});var M=r(u);de=o(M,"TD",{align:!0});var Rn=r(de);nl=i(Rn,"1.12.1"),Rn.forEach(l),ol=c(M),We=o(M,"TD",{align:!0}),r(We).forEach(l),rl=c(M),fe=o(M,"TD",{align:!0});var jn=r(fe);al=i(jn,"\u221A"),jn.forEach(l),il=c(M),pe=o(M,"TD",{align:!0});var Dn=r(pe);sl=i(Dn,"\u221A"),Dn.forEach(l),cl=c(M),he=o(M,"TD",{align:!0});var xn=r(he);dl=i(xn,"\u221A"),xn.forEach(l),fl=c(M),ue=o(M,"TD",{align:!0});var Ln=r(ue);pl=i(Ln,"\u221A"),Ln.forEach(l),M.forEach(l),hl=c(x),T=o(x,"TR",{});var b=r(T);Te=o(b,"TD",{align:!0});var kn=r(Te);ul=i(kn,"1.12.0"),kn.forEach(l),Tl=c(b),Ve=o(b,"TD",{align:!0}),r(Ve).forEach(l),ml=c(b),me=o(b,"TD",{align:!0});var Nn=r(me);wl=i(Nn,"\u221A"),Nn.forEach(l),_l=c(b),we=o(b,"TD",{align:!0});var Xn=r(we);yl=i(Xn,"\u221A"),Xn.forEach(l),Ml=c(b),_e=o(b,"TD",{align:!0});var $n=r(_e);bl=i($n,"\u221A"),$n.forEach(l),Cl=c(b),ye=o(b,"TD",{align:!0});var An=r(ye);Jl=i(An,"\u221A"),An.forEach(l),b.forEach(l),vl=c(x),m=o(x,"TR",{});var C=r(m);Me=o(C,"TD",{align:!0});var Bn=r(Me);Ul=i(Bn,"1.11.0"),Bn.forEach(l),El=c(C),Ye=o(C,"TD",{align:!0}),r(Ye).forEach(l),gl=c(C),be=o(C,"TD",{align:!0});var Gn=r(be);Il=i(Gn,"\u221A"),Gn.forEach(l),Pl=c(C),Ce=o(C,"TD",{align:!0});var Wn=r(Ce);Rl=i(Wn,"\u221A"),Wn.forEach(l),jl=c(C),Je=o(C,"TD",{align:!0});var Vn=r(Je);Dl=i(Vn,"\u221A"),Vn.forEach(l),xl=c(C),ve=o(C,"TD",{align:!0});var Yn=r(ve);Ll=i(Yn,"\u221A"),Yn.forEach(l),C.forEach(l),kl=c(x),w=o(x,"TR",{});var J=r(w);Ue=o(J,"TD",{align:!0});var Hn=r(Ue);Nl=i(Hn,"1.10.0"),Hn.forEach(l),Xl=c(J),Ee=o(J,"TD",{align:!0});var Zn=r(Ee);$l=i(Zn,"\u221A"),Zn.forEach(l),Al=c(J),ge=o(J,"TD",{align:!0});var Sn=r(ge);Bl=i(Sn,"\u221A"),Sn.forEach(l),Gl=c(J),Ie=o(J,"TD",{align:!0});var Fn=r(Ie);Wl=i(Fn,"\u221A"),Fn.forEach(l),Vl=c(J),Pe=o(J,"TD",{align:!0});var Qn=r(Pe);Yl=i(Qn,"\u221A"),Qn.forEach(l),Hl=c(J),He=o(J,"TD",{align:!0}),r(He).forEach(l),J.forEach(l),x.forEach(l),Ct.forEach(l),at=c(e),F(B.$$.fragment,e),it=c(e),_=o(e,"P",{});var L=r(_);Zl=i(L,"where "),Ze=o(L,"CODE",{});var zn=r(Ze);Sl=i(zn,"{pytorch_version}"),zn.forEach(l),Fl=i(L,` should be your PyTorch version, for instance 1.12.0.
Versions of oneCCL and PyTorch must match.
`),Se=o(L,"CODE",{});var On=r(Se);Ql=i(On,"oneccl_bindings_for_pytorch"),On.forEach(l),zl=i(L,` 1.12.0 prebuilt wheel does not work with PyTorch 1.12.1 (it is for PyTorch 1.12.0)
PyTorch 1.12.1 should work with `),Fe=o(L,"CODE",{});var qn=r(Fe);Ol=i(qn,"oneccl_bindings_for_pytorch"),qn.forEach(l),ql=i(L," 1.12.1"),L.forEach(l),st=c(e),Re=o(e,"P",{});var Kn=r(Re);Kl=i(Kn,"MPI tool set for Intel\xAE oneCCL 1.12.0"),Kn.forEach(l),ct=c(e),F(G.$$.fragment,e),dt=c(e),je=o(e,"P",{});var eo=r(je);en=i(eo,"for Intel\xAE oneCCL whose version < 1.12.0"),eo.forEach(l),ft=c(e),F(W.$$.fragment,e),pt=c(e),De=o(e,"P",{});var to=r(De);tn=i(to,"The following command enables training with 2 processes on one node, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),to.forEach(l),ht=c(e),F(V.$$.fragment,e),ut=c(e),xe=o(e,"P",{});var lo=r(xe);ln=i(lo,"The following command enables training with a total of four processes on two nodes (node0 and node1, taking node0 as the main process), ppn (processes per node) is set to 2, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),lo.forEach(l),Tt=c(e),Le=o(e,"P",{});var no=r(Le);nn=i(no,"In node0, you need to create a configuration file which contains the IP addresses of each node (for example hostfile) and pass that configuration file path as an argument."),no.forEach(l),mt=c(e),F(Y.$$.fragment,e),wt=c(e),D=o(e,"P",{});var Jt=r(D);on=i(Jt,"Now, run the following command in node0 and "),Qe=o(Jt,"STRONG",{});var oo=r(Qe);rn=i(oo,"4DDP"),oo.forEach(l),an=i(Jt," will be enabled in node0 and node1:"),Jt.forEach(l),_t=c(e),F(H.$$.fragment,e),this.h()},h(){f(g,"name","hf:doc:metadata"),f(g,"content",JSON.stringify(uo)),f(P,"id","distributed-training"),f(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(P,"href","#distributed-training"),f(I,"class","relative group"),f(N,"href","https://github.com/oneapi-src/oneCCL"),f(N,"rel","nofollow"),f(X,"href","https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html"),f(X,"rel","nofollow"),f($,"href","https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html"),f($,"rel","nofollow"),f(A,"href","https://github.com/intel/torch-ccl"),f(A,"rel","nofollow"),f(oe,"align","center"),f(re,"align","center"),f(ae,"align","center"),f(ie,"align","center"),f(se,"align","center"),f(ce,"align","center"),f(de,"align","center"),f(We,"align","center"),f(fe,"align","center"),f(pe,"align","center"),f(he,"align","center"),f(ue,"align","center"),f(Te,"align","center"),f(Ve,"align","center"),f(me,"align","center"),f(we,"align","center"),f(_e,"align","center"),f(ye,"align","center"),f(Me,"align","center"),f(Ye,"align","center"),f(be,"align","center"),f(Ce,"align","center"),f(Je,"align","center"),f(ve,"align","center"),f(Ue,"align","center"),f(Ee,"align","center"),f(ge,"align","center"),f(Ie,"align","center"),f(Pe,"align","center"),f(He,"align","center")},m(e,d){t(document.head,g),p(e,ze,d),p(e,I,d),t(I,P),t(P,Xe),Q(k,Xe,null),t(I,vt),t(I,$e),t($e,Ut),p(e,Oe,d),p(e,K,d),t(K,Et),p(e,qe,d),p(e,ee,d),t(ee,gt),p(e,Ke,d),p(e,v,d),t(v,N),t(N,It),t(v,Pt),t(v,X),t(X,Rt),t(v,jt),t(v,$),t($,Dt),t(v,xt),p(e,et,d),p(e,E,d),t(E,Lt),t(E,Ae),t(Ae,kt),t(E,Nt),t(E,Be),t(Be,Xt),t(E,$t),p(e,tt,d),p(e,R,d),t(R,At),t(R,A),t(A,Bt),t(R,Gt),p(e,lt,d),p(e,te,d),t(te,Wt),p(e,nt,d),p(e,le,d),t(le,Vt),p(e,ot,d),p(e,ne,d),t(ne,Yt),p(e,rt,d),p(e,j,d),t(j,Ge),t(Ge,h),t(h,oe),t(oe,Ht),t(h,Zt),t(h,re),t(re,St),t(h,Ft),t(h,ae),t(ae,Qt),t(h,zt),t(h,ie),t(ie,Ot),t(h,qt),t(h,se),t(se,Kt),t(h,el),t(h,ce),t(ce,tl),t(j,ll),t(j,U),t(U,u),t(u,de),t(de,nl),t(u,ol),t(u,We),t(u,rl),t(u,fe),t(fe,al),t(u,il),t(u,pe),t(pe,sl),t(u,cl),t(u,he),t(he,dl),t(u,fl),t(u,ue),t(ue,pl),t(U,hl),t(U,T),t(T,Te),t(Te,ul),t(T,Tl),t(T,Ve),t(T,ml),t(T,me),t(me,wl),t(T,_l),t(T,we),t(we,yl),t(T,Ml),t(T,_e),t(_e,bl),t(T,Cl),t(T,ye),t(ye,Jl),t(U,vl),t(U,m),t(m,Me),t(Me,Ul),t(m,El),t(m,Ye),t(m,gl),t(m,be),t(be,Il),t(m,Pl),t(m,Ce),t(Ce,Rl),t(m,jl),t(m,Je),t(Je,Dl),t(m,xl),t(m,ve),t(ve,Ll),t(U,kl),t(U,w),t(w,Ue),t(Ue,Nl),t(w,Xl),t(w,Ee),t(Ee,$l),t(w,Al),t(w,ge),t(ge,Bl),t(w,Gl),t(w,Ie),t(Ie,Wl),t(w,Vl),t(w,Pe),t(Pe,Yl),t(w,Hl),t(w,He),p(e,at,d),Q(B,e,d),p(e,it,d),p(e,_,d),t(_,Zl),t(_,Ze),t(Ze,Sl),t(_,Fl),t(_,Se),t(Se,Ql),t(_,zl),t(_,Fe),t(Fe,Ol),t(_,ql),p(e,st,d),p(e,Re,d),t(Re,Kl),p(e,ct,d),Q(G,e,d),p(e,dt,d),p(e,je,d),t(je,en),p(e,ft,d),Q(W,e,d),p(e,pt,d),p(e,De,d),t(De,tn),p(e,ht,d),Q(V,e,d),p(e,ut,d),p(e,xe,d),t(xe,ln),p(e,Tt,d),p(e,Le,d),t(Le,nn),p(e,mt,d),Q(Y,e,d),p(e,wt,d),p(e,D,d),t(D,on),t(D,Qe),t(Qe,rn),t(D,an),p(e,_t,d),Q(H,e,d),yt=!0},p:co,i(e){yt||(z(k.$$.fragment,e),z(B.$$.fragment,e),z(G.$$.fragment,e),z(W.$$.fragment,e),z(V.$$.fragment,e),z(Y.$$.fragment,e),z(H.$$.fragment,e),yt=!0)},o(e){O(k.$$.fragment,e),O(B.$$.fragment,e),O(G.$$.fragment,e),O(W.$$.fragment,e),O(V.$$.fragment,e),O(Y.$$.fragment,e),O(H.$$.fragment,e),yt=!1},d(e){l(g),e&&l(ze),e&&l(I),q(k),e&&l(Oe),e&&l(K),e&&l(qe),e&&l(ee),e&&l(Ke),e&&l(v),e&&l(et),e&&l(E),e&&l(tt),e&&l(R),e&&l(lt),e&&l(te),e&&l(nt),e&&l(le),e&&l(ot),e&&l(ne),e&&l(rt),e&&l(j),e&&l(at),q(B,e),e&&l(it),e&&l(_),e&&l(st),e&&l(Re),e&&l(ct),q(G,e),e&&l(dt),e&&l(je),e&&l(ft),q(W,e),e&&l(pt),e&&l(De),e&&l(ht),q(V,e),e&&l(ut),e&&l(xe),e&&l(Tt),e&&l(Le),e&&l(mt),q(Y,e),e&&l(wt),e&&l(D),e&&l(_t),q(H,e)}}}const uo={local:"distributed-training",title:"Distributed training"};function To(sn){return fo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class yo extends ro{constructor(g){super();ao(this,g,To,ho,io,{})}}export{yo as default,uo as metadata};
