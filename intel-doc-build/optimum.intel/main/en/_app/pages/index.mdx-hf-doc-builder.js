import{S as Ie,i as _e,s as be,e as r,k as g,w as Oe,t as l,M as xe,c as i,d as t,m as v,a as o,x as qe,h as p,b as a,G as n,g as u,y as Ee,L as Ne,q as ke,o as ze,B as Ve,v as Pe}from"../chunks/vendor-hf-doc-builder.js";import{I as De}from"../chunks/IconCopyLink-hf-doc-builder.js";function Ce(se){let d,R,c,w,P,I,H,D,J,$,E,j,A,_,b,F,K,L,f,O,Q,W,x,X,Y,S,q,y,h,N,Z,ee,k,te,ne,m,z,ae,re,V,ie,T;return I=new De({}),{c(){d=r("meta"),R=g(),c=r("h1"),w=r("a"),P=r("span"),Oe(I.$$.fragment),H=g(),D=r("span"),J=l("\u{1F917} Optimum Intel"),$=g(),E=r("p"),j=l("\u{1F917} Optimum Intel is the interface between the \u{1F917} Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures."),A=g(),_=r("p"),b=r("a"),F=l("Intel Neural Compressor"),K=l(" is an open-source library enabling the usage of the most popular compression techniques such as quantization, pruning and knowledge distillation. It supports automatic accuracy-driven tuning strategies in order for users to easily generate quantized model. The users can easily apply static, dynamic and aware-training quantization approaches while giving an expected accuracy criteria. It also supports different weight pruning techniques enabling the creation of pruned model giving a predefined sparsity target."),L=g(),f=r("p"),O=r("a"),Q=l("OpenVINO"),W=l(" is an open-source toolkit that enables high performance inference capabilities for Intel CPUs, GPUs, and special DL inference accelerators ("),x=r("a"),X=l("see"),Y=l(" the full list of supported devices). It is supplied with a set of tools to optimize your models with compression techniques such as quantization, pruning and knowledge distillation. Optimum Intel provides a simple interface to optimize your Transformers and Diffusers models, convert them to the OpenVINO Intermediate Representation (IR) format and run inference using OpenVINO Runtime."),S=g(),q=r("div"),y=r("div"),h=r("a"),N=r("div"),Z=l("Neural Compressor"),ee=g(),k=r("p"),te=l("Learn how to apply compression techniques such as quantization, pruning and knowledge distillation to speed up inference with Intel Neural Compressor."),ne=g(),m=r("a"),z=r("div"),ae=l("OpenVINO"),re=g(),V=r("p"),ie=l("Learn how to run inference with OpenVINO Runtime and to apply quantization, pruning and knowledge distillation on your model to further speed up inference."),this.h()},l(e){const s=xe('[data-svelte="svelte-1phssyn"]',document.head);d=i(s,"META",{name:!0,content:!0}),s.forEach(t),R=v(e),c=i(e,"H1",{class:!0});var U=o(c);w=i(U,"A",{id:!0,class:!0,href:!0});var le=o(w);P=i(le,"SPAN",{});var pe=o(P);qe(I.$$.fragment,pe),pe.forEach(t),le.forEach(t),H=v(U),D=i(U,"SPAN",{});var ue=o(D);J=p(ue,"\u{1F917} Optimum Intel"),ue.forEach(t),U.forEach(t),$=v(e),E=i(e,"P",{});var de=o(E);j=p(de,"\u{1F917} Optimum Intel is the interface between the \u{1F917} Transformers and Diffusers libraries and the different tools and libraries provided by Intel to accelerate end-to-end pipelines on Intel architectures."),de.forEach(t),A=v(e),_=i(e,"P",{});var oe=o(_);b=i(oe,"A",{href:!0,rel:!0});var ce=o(b);F=p(ce,"Intel Neural Compressor"),ce.forEach(t),K=p(oe," is an open-source library enabling the usage of the most popular compression techniques such as quantization, pruning and knowledge distillation. It supports automatic accuracy-driven tuning strategies in order for users to easily generate quantized model. The users can easily apply static, dynamic and aware-training quantization approaches while giving an expected accuracy criteria. It also supports different weight pruning techniques enabling the creation of pruned model giving a predefined sparsity target."),oe.forEach(t),L=v(e),f=i(e,"P",{});var C=o(f);O=i(C,"A",{href:!0,rel:!0});var fe=o(O);Q=p(fe,"OpenVINO"),fe.forEach(t),W=p(C," is an open-source toolkit that enables high performance inference capabilities for Intel CPUs, GPUs, and special DL inference accelerators ("),x=i(C,"A",{href:!0,rel:!0});var he=o(x);X=p(he,"see"),he.forEach(t),Y=p(C," the full list of supported devices). It is supplied with a set of tools to optimize your models with compression techniques such as quantization, pruning and knowledge distillation. Optimum Intel provides a simple interface to optimize your Transformers and Diffusers models, convert them to the OpenVINO Intermediate Representation (IR) format and run inference using OpenVINO Runtime."),C.forEach(t),S=v(e),q=i(e,"DIV",{class:!0});var me=o(q);y=i(me,"DIV",{class:!0});var G=o(y);h=i(G,"A",{class:!0,href:!0});var M=o(h);N=i(M,"DIV",{class:!0});var ge=o(N);Z=p(ge,"Neural Compressor"),ge.forEach(t),ee=v(M),k=i(M,"P",{class:!0});var ve=o(k);te=p(ve,"Learn how to apply compression techniques such as quantization, pruning and knowledge distillation to speed up inference with Intel Neural Compressor."),ve.forEach(t),M.forEach(t),ne=v(G),m=i(G,"A",{class:!0,href:!0});var B=o(m);z=i(B,"DIV",{class:!0});var we=o(z);ae=p(we,"OpenVINO"),we.forEach(t),re=v(B),V=i(B,"P",{class:!0});var ye=o(V);ie=p(ye,"Learn how to run inference with OpenVINO Runtime and to apply quantization, pruning and knowledge distillation on your model to further speed up inference."),ye.forEach(t),B.forEach(t),G.forEach(t),me.forEach(t),this.h()},h(){a(d,"name","hf:doc:metadata"),a(d,"content",JSON.stringify(Re)),a(w,"id","optimum-intel"),a(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(w,"href","#optimum-intel"),a(c,"class","relative group"),a(b,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),a(b,"rel","nofollow"),a(O,"href","https://docs.openvino.ai/latest/index.html"),a(O,"rel","nofollow"),a(x,"href","https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html"),a(x,"rel","nofollow"),a(N,"class","w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),a(k,"class","text-gray-700"),a(h,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),a(h,"href","./optimization_inc"),a(z,"class","w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),a(V,"class","text-gray-700"),a(m,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),a(m,"href","./inference"),a(y,"class","w-full flex flex-col space-x-4 md:grid md:grid-cols-2 md:gap-x-5"),a(q,"class","mt-10")},m(e,s){n(document.head,d),u(e,R,s),u(e,c,s),n(c,w),n(w,P),Ee(I,P,null),n(c,H),n(c,D),n(D,J),u(e,$,s),u(e,E,s),n(E,j),u(e,A,s),u(e,_,s),n(_,b),n(b,F),n(_,K),u(e,L,s),u(e,f,s),n(f,O),n(O,Q),n(f,W),n(f,x),n(x,X),n(f,Y),u(e,S,s),u(e,q,s),n(q,y),n(y,h),n(h,N),n(N,Z),n(h,ee),n(h,k),n(k,te),n(y,ne),n(y,m),n(m,z),n(z,ae),n(m,re),n(m,V),n(V,ie),T=!0},p:Ne,i(e){T||(ke(I.$$.fragment,e),T=!0)},o(e){ze(I.$$.fragment,e),T=!1},d(e){t(d),e&&t(R),e&&t(c),Ve(I),e&&t($),e&&t(E),e&&t(A),e&&t(_),e&&t(L),e&&t(f),e&&t(S),e&&t(q)}}}const Re={local:"optimum-intel",title:"\u{1F917} Optimum Intel"};function $e(se){return Pe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Se extends Ie{constructor(d){super();_e(this,d,$e,Ce,be,{})}}export{Se as default,Re as metadata};
