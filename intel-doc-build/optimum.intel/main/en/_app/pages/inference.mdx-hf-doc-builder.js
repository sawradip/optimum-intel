import{S as Ih,i as Gh,s as Vh,e as a,k as p,w as h,t as i,M as Bh,c as s,d as l,m as d,a as o,x as f,h as r,b as m,$ as St,G as t,g as c,y as u,L as kh,q as b,o as y,B as M,v as Xh}from"../chunks/vendor-hf-doc-builder.js";import{I as U}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../chunks/CodeBlock-hf-doc-builder.js";function xh(_d){let B,Js,k,z,Kt,_e,Dn,el,Yn,Zs,q,zn,ge,qn,Hn,Us,X,H,tl,Ee,An,ll,Pn,Ws,J,Ln,je,Kn,ei,al,ti,li,sl,ai,si,ol,oi,ni,_s,Nt,ii,gs,Ie,Es,A,ri,$t,pi,di,js,P,ci,nl,mi,hi,Is,Ge,Gs,L,fi,il,ui,bi,Vs,Ve,Bs,K,yi,rl,Mi,wi,ks,Be,Xs,E,vi,pl,Ti,Ji,ke,Zi,Ui,xs,Xe,Rs,W,Wi,dl,_i,gi,cl,Ei,ji,ml,Ii,Gi,Ss,xe,Ns,j,Vi,hl,Bi,ki,fl,Xi,xi,$s,Re,Fs,Z,Ri,ul,Si,Ni,Se,$i,Fi,bl,Ci,Oi,yl,Qi,Di,Cs,Ne,Os,x,ee,Ml,$e,Yi,wl,zi,Qs,I,qi,vl,Hi,Ai,Tl,Pi,Li,Ds,Ft,Ki,Ys,Fe,zs,R,te,Jl,Ce,er,Zl,tr,qs,Ct,lr,Hs,_,Ul,ar,sr,Wl,or,nr,_l,ir,rr,gl,pr,As,Ot,dr,Ps,le,cr,El,mr,hr,Ls,Oe,Ks,S,ae,jl,Qe,fr,Il,ur,eo,De,to,se,br,Gl,yr,Mr,lo,Ye,ao,Qt,wr,so,ze,oo,Dt,vr,no,qe,Vl,gd,io,N,oe,Bl,He,Tr,kl,Jr,ro,Yt,Zr,po,Ae,co,ne,Ur,Pe,Wr,_r,mo,Le,ho,zt,gr,fo,ie,Xl,Ke,uo,Er,bo,jr,xl,et,Rl,Sl,Ed,Ir,Nl,$l,jd,yo,$,re,Fl,tt,Gr,Cl,Vr,Mo,lt,wo,F,pe,Ol,at,Br,Ql,kr,vo,g,Xr,Dl,xr,Rr,Yl,Sr,Nr,zl,$r,Fr,To,st,Jo,C,de,ql,ot,Cr,Hl,Or,Zo,ce,Qr,nt,Dr,Yr,Uo,it,Wo,me,Al,rt,_o,zr,go,qr,Pl,pt,Ll,Kl,Id,Hr,ea,ta,Gd,Eo,O,he,la,dt,Ar,aa,Pr,jo,fe,Lr,ct,Kr,ep,Io,qt,tp,Go,mt,Vo,ue,lp,ht,ap,sp,Bo,ft,ko,be,sa,ut,Xo,op,xo,np,oa,bt,na,ia,Vd,ip,ra,pa,Bd,Ro,Q,ye,da,yt,rp,ca,pp,So,Me,dp,ma,cp,mp,No,Mt,$o,D,we,ha,wt,hp,fa,fp,Fo,ve,up,vt,bp,yp,Co,Tt,Oo,Y,Te,ua,Jt,Mp,ba,wp,Qo,Ht,vp,Do,Je,ya,Zt,Ma,Tp,Jp,wa,Zp,Up,w,Ut,va,Ta,Wp,_p,Ja,Za,gp,Ep,Wt,Ua,Wa,jp,Ip,_a,ga,Gp,Vp,_t,Ea,ja,Bp,kp,Ia,Ga,Xp,xp,gt,Va,Ba,Rp,Sp,ka,Xa,Np,$p,Et,xa,Ra,Fp,Cp,Sa,Na,Op,Qp,jt,$a,Fa,Dp,Yp,Ca,Oa,zp,qp,It,Qa,Da,Hp,Ap,Ya,za,Pp,Lp,Gt,qa,Ha,Kp,ed,Aa,Pa,td,ld,Vt,La,Ka,ad,sd,es,ts,od,nd,Bt,ls,as,id,rd,ss,os,pd,dd,kt,ns,is,cd,md,rs,ps,hd,fd,Xt,ds,cs,ud,bd,ms,hs,yd,Md,xt,fs,us,wd,vd,bs,ys,Td,Jd,Rt,Ms,ws,Zd,Ud,vs,Ts,Wd,Yo;return _e=new U({}),Ee=new U({}),Ie=new T({props:{code:"LSUyMGZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTJCJTIwZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvVG9rZW5pemVyJTJDJTIwcGlwZWxpbmUlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkLWZpbmV0dW5lZC1zc3QtMi1lbmdsaXNoJTIyJTBBLSUyMG1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBJTJCJTIwbW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBleHBvcnQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQWNsc19waXBlJTIwJTNEJTIwcGlwZWxpbmUoJTIydGV4dC1jbGFzc2lmaWNhdGlvbiUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBb3V0cHV0cyUyMCUzRCUyMGNsc19waXBlKCUyMkhlJ3MlMjBhJTIwZHJlYWRmdWwlMjBtYWdpY2lhbi4lMjIpJTBBJTBBJTVCJTdCJ2xhYmVsJyUzQSUyMCdORUdBVElWRSclMkMlMjAnc2NvcmUnJTNBJTIwMC45OTE5NTAzOTI3MjMwODM1JTdEJTVE",highlighted:`<span class="hljs-deletion">- from transformers import AutoModelForSequenceClassification</span>
<span class="hljs-addition">+ from optimum.intel import OVModelForSequenceClassification</span>
from transformers import AutoTokenizer, pipeline

model_id = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
<span class="hljs-deletion">- model = AutoModelForSequenceClassification.from_pretrained(model_id)</span>
<span class="hljs-addition">+ model = OVModelForSequenceClassification.from_pretrained(model_id, export=True)</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)
cls_pipe = pipeline(&quot;text-classification&quot;, model=model, tokenizer=tokenizer)
outputs = cls_pipe(&quot;He&#x27;s a dreadful magician.&quot;)

[{&#x27;label&#x27;: &#x27;NEGATIVE&#x27;, &#x27;score&#x27;: 0.9919503927230835}]`}}),Ge=new T({props:{code:"JTIzJTIwU2F2ZSUyMHRoZSUyMGV4cG9ydGVkJTIwbW9kZWwlMEFzYXZlX2RpcmVjdG9yeSUyMCUzRCUyMCUyMm9wZW52aW5vX2Rpc3RpbGJlcnQlMjIlMEFtb2RlbC5zYXZlX3ByZXRyYWluZWQoc2F2ZV9kaXJlY3RvcnkpJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChzYXZlX2RpcmVjdG9yeSk=",highlighted:`<span class="hljs-comment"># Save the exported model</span>
save_directory = <span class="hljs-string">&quot;openvino_distilbert&quot;</span>
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)`}}),Ve=new T({props:{code:"JTIzJTIwRml4JTIwdGhlJTIwYmF0Y2glMjBzaXplJTIwdG8lMjAxJTIwYW5kJTIwdGhlJTIwc2VxdWVuY2UlMjBsZW5ndGglMjB0byUyMDklMEFtb2RlbC5yZXNoYXBlKDElMkMlMjA5KSUwQSUyMyUyMENvbXBpbGUlMjB0aGUlMjBtb2RlbCUyMGJlZm9yZSUyMHRoZSUyMGZpcnN0JTIwaW5mZXJlbmNlJTBBbW9kZWwuY29tcGlsZSgp",highlighted:`<span class="hljs-comment"># Fix the batch size to 1 and the sequence length to 9</span>
model.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">9</span>)
<span class="hljs-comment"># Compile the model before the first inference</span>
model.<span class="hljs-built_in">compile</span>()`}}),Be=new T({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBldmFsdWF0ZSUyMGltcG9ydCUyMGV2YWx1YXRvciUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJkaXN0aWxiZXJ0LWJhc2UtY2FzZWQtZGlzdGlsbGVkLXNxdWFkJTIyJTBBbW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEFtb2RlbC5yZXNoYXBlKDElMkMlMjAzODQpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBZXZhbF9kYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMnNxdWFkJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKS5zZWxlY3QocmFuZ2UoNTApKSUwQXRhc2tfZXZhbHVhdG9yJTIwJTNEJTIwZXZhbHVhdG9yKCUyMnF1ZXN0aW9uLWFuc3dlcmluZyUyMiklMEFxYV9waXBlJTIwJTNEJTIwcGlwZWxpbmUoJTBBJTIwJTIwJTIwJTIwJTIycXVlc3Rpb24tYW5zd2VyaW5nJTIyJTJDJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMG1heF9zZXFfbGVuJTNEMzg0JTJDJTBBJTIwJTIwJTIwJTIwcGFkZGluZyUzRCUyMm1heF9sZW5ndGglMjIlMkMlMEElMjAlMjAlMjAlMjB0cnVuY2F0aW9uJTNEVHJ1ZSUyQyUwQSklMEFtZXRyaWMlMjAlM0QlMjB0YXNrX2V2YWx1YXRvci5jb21wdXRlKG1vZGVsX29yX3BpcGVsaW5lJTNEcWFfcGlwZSUyQyUyMGRhdGElM0RldmFsX2RhdGFzZXQlMkMlMjBtZXRyaWMlM0QlMjJzcXVhZCUyMik=",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForQuestionAnswering

model_id = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
model = OVModelForQuestionAnswering.from_pretrained(model_id, export=<span class="hljs-literal">True</span>)
model.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">384</span>)
tokenizer = AutoTokenizer.from_pretrained(model_id)
eval_dataset = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>))
task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
qa_pipe = pipeline(
    <span class="hljs-string">&quot;question-answering&quot;</span>,
    model=model,
    tokenizer=tokenizer,
    max_seq_len=<span class="hljs-number">384</span>,
    padding=<span class="hljs-string">&quot;max_length&quot;</span>,
    truncation=<span class="hljs-literal">True</span>,
)
metric = task_evaluator.compute(model_or_pipeline=qa_pipe, data=eval_dataset, metric=<span class="hljs-string">&quot;squad&quot;</span>)`}}),Xe=new T({props:{code:"JTIzJTIwU3RhdGljJTIwc2hhcGVzJTIwc3BlZWQlMjB1cCUyMGluZmVyZW5jZSUwQW1vZGVsLnJlc2hhcGUoMSUyQyUyMDkpJTBBbW9kZWwudG8oJTIyZ3B1JTIyKSUwQSUyMyUyMENvbXBpbGUlMjB0aGUlMjBtb2RlbCUyMGJlZm9yZSUyMHRoZSUyMGZpcnN0JTIwaW5mZXJlbmNlJTBBbW9kZWwuY29tcGlsZSgp",highlighted:`<span class="hljs-comment"># Static shapes speed up inference</span>
model.reshape(<span class="hljs-number">1</span>, <span class="hljs-number">9</span>)
model.to(<span class="hljs-string">&quot;gpu&quot;</span>)
<span class="hljs-comment"># Compile the model before the first inference</span>
model.<span class="hljs-built_in">compile</span>()`}}),xe=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQtZmluZXR1bmVkLXNzdC0yLWVuZ2xpc2glMjIlMEElMjMlMjBMb2FkJTIwdGhlJTIwbW9kZWwlMjBhbmQlMjBkaXNhYmxlJTIwdGhlJTIwbW9kZWwlMjBjb21waWxhdGlvbiUwQW1vZGVsJTIwJTNEJTIwT1ZNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZXhwb3J0JTNEVHJ1ZSUyQyUyMGNvbXBpbGUlM0RGYWxzZSklMEElMjMlMjBSZXNoYXBlJTIwdG8lMjBhJTIwc3RhdGljJTIwc2VxdWVuY2UlMjBsZW5ndGglMjBvZiUyMDEyOCUwQW1vZGVsLnJlc2hhcGUoMSUyQzEyOCklMEElMjMlMjBDb21waWxlJTIwdGhlJTIwbW9kZWwlMjBiZWZvcmUlMjB0aGUlMjBmaXJzdCUyMGluZmVyZW5jZSUwQW1vZGVsLmNvbXBpbGUoKQ==",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForSequenceClassification

model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-comment"># Load the model and disable the model compilation</span>
model = OVModelForSequenceClassification.from_pretrained(model_id, export=<span class="hljs-literal">True</span>, <span class="hljs-built_in">compile</span>=<span class="hljs-literal">False</span>)
<span class="hljs-comment"># Reshape to a static sequence length of 128</span>
model.reshape(<span class="hljs-number">1</span>,<span class="hljs-number">128</span>)
<span class="hljs-comment"># Compile the model before the first inference</span>
model.<span class="hljs-built_in">compile</span>()`}}),Re=new T({props:{code:"bW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBvdl9jb25maWclM0QlN0IlMjJJTkZFUkVOQ0VfUFJFQ0lTSU9OX0hJTlQlMjIlM0ElMjJmMzIlMjIlN0Qp",highlighted:'model = OVModelForSequenceClassification.from_pretrained(model_id, ov_config={<span class="hljs-string">&quot;INFERENCE_PRECISION_HINT&quot;</span>:<span class="hljs-string">&quot;f32&quot;</span>})'}}),Ne=new T({props:{code:"bW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBvdl9jb25maWclM0QlN0IlMjJDQUNIRV9ESVIlMjIlM0ElMjIlMjIlN0Qp",highlighted:'model = OVModelForSequenceClassification.from_pretrained(model_id, ov_config={<span class="hljs-string">&quot;CACHE_DIR&quot;</span>:<span class="hljs-string">&quot;&quot;</span>})'}}),$e=new U({}),Fe=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvclNlcTJTZXFMTSUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIydDUtc21hbGwlMjIlMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEF0cmFuc2xhdGlvbl9waXBlJTIwJTNEJTIwcGlwZWxpbmUoJTIydHJhbnNsYXRpb25fZW5fdG9fZnIlMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUwQXRleHQlMjAlM0QlMjAlMjJIZSUyMG5ldmVyJTIwd2VudCUyMG91dCUyMHdpdGhvdXQlMjBhJTIwYm9vayUyMHVuZGVyJTIwaGlzJTIwYXJtJTJDJTIwYW5kJTIwaGUlMjBvZnRlbiUyMGNhbWUlMjBiYWNrJTIwd2l0aCUyMHR3by4lMjIlMEFyZXN1bHQlMjAlM0QlMjB0cmFuc2xhdGlvbl9waXBlKHRleHQpJTBBJTBBJTIzJTIwU2F2ZSUyMHRoZSUyMGV4cG9ydGVkJTIwbW9kZWwlMEFzYXZlX2RpcmVjdG9yeSUyMCUzRCUyMCUyMm9wZW52aW5vX3Q1JTIyJTBBbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKHNhdmVfZGlyZWN0b3J5KSUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoc2F2ZV9kaXJlY3RvcnkpJTBBJTBBJTVCJTdCJ3RyYW5zbGF0aW9uX3RleHQnJTNBJTIwJTIySWwlMjBuJ2VzdCUyMGphbWFpcyUyMHNvcnRpJTIwc2FucyUyMHVuJTIwbGl2cmUlMjBzb3VzJTIwc29uJTIwYnJhcyUyQyUyMGV0JTIwaWwlMjBlc3QlMjBzb3V2ZW50JTIwcmV2ZW51JTIwYXZlYyUyMGRldXguJTIyJTdEJTVE",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForSeq2SeqLM

model_id = <span class="hljs-string">&quot;t5-small&quot;</span>
model = OVModelForSeq2SeqLM.from_pretrained(model_id, export=<span class="hljs-literal">True</span>)
tokenizer = AutoTokenizer.from_pretrained(model_id)
translation_pipe = pipeline(<span class="hljs-string">&quot;translation_en_to_fr&quot;</span>, model=model, tokenizer=tokenizer)
text = <span class="hljs-string">&quot;He never went out without a book under his arm, and he often came back with two.&quot;</span>
result = translation_pipe(text)

<span class="hljs-comment"># Save the exported model</span>
save_directory = <span class="hljs-string">&quot;openvino_t5&quot;</span>
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&quot;Il n&#x27;est jamais sorti sans un livre sous son bras, et il est souvent revenu avec deux.&quot;</span>}]`}}),Ce=new U({}),Oe=new T({props:{code:"cGlwJTIwaW5zdGFsbCUyMG9wdGltdW0lNUJkaWZmdXNlcnMlNUQ=",highlighted:"pip install optimum[diffusers]"}}),Qe=new U({}),De=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVlN0YWJsZURpZmZ1c2lvblBpcGVsaW5lJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJlY2hhcmxhaXglMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUtb3BlbnZpbm8lMjIlMEFwaXBlbGluZSUyMCUzRCUyME9WU3RhYmxlRGlmZnVzaW9uUGlwZWxpbmUuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQXByb21wdCUyMCUzRCUyMCUyMnNhaWxpbmclMjBzaGlwJTIwaW4lMjBzdG9ybSUyMGJ5JTIwUmVtYnJhbmR0JTIyJTBBaW1hZ2VzJTIwJTNEJTIwcGlwZWxpbmUocHJvbXB0KS5pbWFnZXM=",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionPipeline

model_id = <span class="hljs-string">&quot;echarlaix/stable-diffusion-v1-5-openvino&quot;</span>
pipeline = OVStableDiffusionPipeline.from_pretrained(model_id)
prompt = <span class="hljs-string">&quot;sailing ship in storm by Rembrandt&quot;</span>
images = pipeline(prompt).images`}}),Ye=new T({props:{code:"bW9kZWxfaWQlMjAlM0QlMjAlMjJydW53YXltbCUyRnN0YWJsZS1kaWZmdXNpb24tdjEtNSUyMiUwQXBpcGVsaW5lJTIwJTNEJTIwT1ZTdGFibGVEaWZmdXNpb25QaXBlbGluZS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBleHBvcnQlM0RUcnVlKSUwQSUyMyUyMERvbid0JTIwZm9yZ2V0JTIwdG8lMjBzYXZlJTIwdGhlJTIwZXhwb3J0ZWQlMjBtb2RlbCUwQXBpcGVsaW5lLnNhdmVfcHJldHJhaW5lZCglMjJvcGVudmluby1zZC12MS01JTIyKQ==",highlighted:`model_id = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>
pipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Don&#x27;t forget to save the exported model</span>
pipeline.save_pretrained(<span class="hljs-string">&quot;openvino-sd-v1-5&quot;</span>)`}}),ze=new T({props:{code:"JTIzJTIwRGVmaW5lJTIwdGhlJTIwc2hhcGVzJTIwcmVsYXRlZCUyMHRvJTIwdGhlJTIwaW5wdXRzJTIwYW5kJTIwZGVzaXJlZCUyMG91dHB1dHMlMEFiYXRjaF9zaXplJTIwJTNEJTIwMSUwQW51bV9pbWFnZXNfcGVyX3Byb21wdCUyMCUzRCUyMDElMEFoZWlnaHQlMjAlM0QlMjA1MTIlMEF3aWR0aCUyMCUzRCUyMDUxMiUwQSUwQSUyMyUyMFN0YXRpY2FsbHklMjByZXNoYXBlJTIwdGhlJTIwbW9kZWwlMEFwaXBlbGluZS5yZXNoYXBlKGJhdGNoX3NpemUlM0RiYXRjaF9zaXplJTJDJTIwaGVpZ2h0JTNEaGVpZ2h0JTJDJTIwd2lkdGglM0R3aWR0aCUyQyUyMG51bV9pbWFnZXNfcGVyX3Byb21wdCUzRG51bV9pbWFnZXNfcGVyX3Byb21wdCklMEElMjMlMjBDb21waWxlJTIwdGhlJTIwbW9kZWwlMjBiZWZvcmUlMjB0aGUlMjBmaXJzdCUyMGluZmVyZW5jZSUwQXBpcGVsaW5lLmNvbXBpbGUoKSUwQSUwQSUyMyUyMFJ1biUyMGluZmVyZW5jZSUwQWltYWdlcyUyMCUzRCUyMHBpcGVsaW5lKHByb21wdCUyQyUyMGhlaWdodCUzRGhlaWdodCUyQyUyMHdpZHRoJTNEd2lkdGglMkMlMjBudW1faW1hZ2VzX3Blcl9wcm9tcHQlM0RudW1faW1hZ2VzX3Blcl9wcm9tcHQpLmltYWdlcw==",highlighted:`<span class="hljs-comment"># Define the shapes related to the inputs and desired outputs</span>
batch_size = <span class="hljs-number">1</span>
num_images_per_prompt = <span class="hljs-number">1</span>
height = <span class="hljs-number">512</span>
width = <span class="hljs-number">512</span>

<span class="hljs-comment"># Statically reshape the model</span>
pipeline.reshape(batch_size=batch_size, height=height, width=width, num_images_per_prompt=num_images_per_prompt)
<span class="hljs-comment"># Compile the model before the first inference</span>
pipeline.<span class="hljs-built_in">compile</span>()

<span class="hljs-comment"># Run inference</span>
images = pipeline(prompt, height=height, width=width, num_images_per_prompt=num_images_per_prompt).images`}}),He=new U({}),Ae=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVlN0YWJsZURpZmZ1c2lvblBpcGVsaW5lJTBBaW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJlY2hhcmxhaXglMkZzdGFibGUtZGlmZnVzaW9uLXYxLTUtb3BlbnZpbm8lMjIlMEFwcm9tcHQlMjAlM0QlMjAlMjJBJTIwJTNDY2F0LXRveSUzRSUyMGJhY2stcGFjayUyMiUwQSUyMyUyMFNldCUyMGElMjByYW5kb20lMjBzZWVkJTIwZm9yJTIwYmV0dGVyJTIwY29tcGFyaXNvbiUwQW5wLnJhbmRvbS5zZWVkKDQyKSUwQSUwQXBpcGVsaW5lJTIwJTNEJTIwT1ZTdGFibGVEaWZmdXNpb25QaXBlbGluZS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBleHBvcnQlM0RGYWxzZSUyQyUyMGNvbXBpbGUlM0RGYWxzZSklMEFwaXBlbGluZS5jb21waWxlKCklMEFpbWFnZTElMjAlM0QlMjBwaXBlbGluZShwcm9tcHQlMkMlMjBudW1faW5mZXJlbmNlX3N0ZXBzJTNENTApLmltYWdlcyU1QjAlNUQlMEFpbWFnZTEuc2F2ZSglMjJzdGFibGVfZGlmZnVzaW9uX3YxXzVfd2l0aG91dF90ZXh0dWFsX2ludmVyc2lvbi5wbmclMjIp",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionPipeline
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

model_id = <span class="hljs-string">&quot;echarlaix/stable-diffusion-v1-5-openvino&quot;</span>
prompt = <span class="hljs-string">&quot;A &lt;cat-toy&gt; back-pack&quot;</span>
<span class="hljs-comment"># Set a random seed for better comparison</span>
np.random.seed(<span class="hljs-number">42</span>)

pipeline = OVStableDiffusionPipeline.from_pretrained(model_id, export=<span class="hljs-literal">False</span>, <span class="hljs-built_in">compile</span>=<span class="hljs-literal">False</span>)
pipeline.<span class="hljs-built_in">compile</span>()
image1 = pipeline(prompt, num_inference_steps=<span class="hljs-number">50</span>).images[<span class="hljs-number">0</span>]
image1.save(<span class="hljs-string">&quot;stable_diffusion_v1_5_without_textual_inversion.png&quot;</span>)`}}),Le=new T({props:{code:"JTIzJTIwUmVzZXQlMjBzdGFibGUlMjBkaWZmdXNpb24lMjBwaXBlbGluZSUwQXBpcGVsaW5lLmNsZWFyX3JlcXVlc3RzKCklMEElMEElMjMlMjBMb2FkJTIwdGV4dHVhbCUyMGludmVyc2lvbiUyMGludG8lMjBzdGFibGUlMjBkaWZmdXNpb24lMjBwaXBlbGluZSUwQXBpcGVsaW5lLmxvYWRfdGV4dHVhbF9pbnZlcnNpb24oJTIyc2QtY29uY2VwdHMtbGlicmFyeSUyRmNhdC10b3klMjIlMkMlMjAlMjIlM0NjYXQtdG95JTNFJTIyKSUwQSUwQSUyMyUyMENvbXBpbGUlMjB0aGUlMjBtb2RlbCUyMGJlZm9yZSUyMHRoZSUyMGZpcnN0JTIwaW5mZXJlbmNlJTBBcGlwZWxpbmUuY29tcGlsZSgpJTBBaW1hZ2UyJTIwJTNEJTIwcGlwZWxpbmUocHJvbXB0JTJDJTIwbnVtX2luZmVyZW5jZV9zdGVwcyUzRDUwKS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2UyLnNhdmUoJTIyc3RhYmxlX2RpZmZ1c2lvbl92MV81X3dpdGhfdGV4dHVhbF9pbnZlcnNpb24ucG5nJTIyKQ==",highlighted:`<span class="hljs-comment"># Reset stable diffusion pipeline</span>
pipeline.clear_requests()

<span class="hljs-comment"># Load textual inversion into stable diffusion pipeline</span>
pipeline.load_textual_inversion(<span class="hljs-string">&quot;sd-concepts-library/cat-toy&quot;</span>, <span class="hljs-string">&quot;&lt;cat-toy&gt;&quot;</span>)

<span class="hljs-comment"># Compile the model before the first inference</span>
pipeline.<span class="hljs-built_in">compile</span>()
image2 = pipeline(prompt, num_inference_steps=<span class="hljs-number">50</span>).images[<span class="hljs-number">0</span>]
image2.save(<span class="hljs-string">&quot;stable_diffusion_v1_5_with_textual_inversion.png&quot;</span>)`}}),tt=new U({}),lt=new T({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjBpbyUyMGltcG9ydCUyMEJ5dGVzSU8lMEFmcm9tJTIwb3B0aW11bS5pbnRlbCUyMGltcG9ydCUyME9WU3RhYmxlRGlmZnVzaW9uSW1nMkltZ1BpcGVsaW5lJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJydW53YXltbCUyRnN0YWJsZS1kaWZmdXNpb24tdjEtNSUyMiUwQXBpcGVsaW5lJTIwJTNEJTIwT1ZTdGFibGVEaWZmdXNpb25JbWcySW1nUGlwZWxpbmUuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRnJhdy5naXRodWJ1c2VyY29udGVudC5jb20lMkZDb21wVmlzJTJGc3RhYmxlLWRpZmZ1c2lvbiUyRm1haW4lMkZhc3NldHMlMkZzdGFibGUtc2FtcGxlcyUyRmltZzJpbWclMkZza2V0Y2gtbW91bnRhaW5zLWlucHV0LmpwZyUyMiUwQXJlc3BvbnNlJTIwJTNEJTIwcmVxdWVzdHMuZ2V0KHVybCklMEFpbml0X2ltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihCeXRlc0lPKHJlc3BvbnNlLmNvbnRlbnQpKS5jb252ZXJ0KCUyMlJHQiUyMiklMEFpbml0X2ltYWdlJTIwJTNEJTIwaW5pdF9pbWFnZS5yZXNpemUoKDc2OCUyQyUyMDUxMikpJTBBcHJvbXB0JTIwJTNEJTIwJTIyQSUyMGZhbnRhc3klMjBsYW5kc2NhcGUlMkMlMjB0cmVuZGluZyUyMG9uJTIwYXJ0c3RhdGlvbiUyMiUwQWltYWdlJTIwJTNEJTIwcGlwZWxpbmUocHJvbXB0JTNEcHJvbXB0JTJDJTIwaW1hZ2UlM0Rpbml0X2ltYWdlJTJDJTIwc3RyZW5ndGglM0QwLjc1JTJDJTIwZ3VpZGFuY2Vfc2NhbGUlM0Q3LjUpLmltYWdlcyU1QjAlNUQlMEFpbWFnZS5zYXZlKCUyMmZhbnRhc3lfbGFuZHNjYXBlLnBuZyUyMik=",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionImg2ImgPipeline

model_id = <span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>
pipeline = OVStableDiffusionImg2ImgPipeline.from_pretrained(model_id, export=<span class="hljs-literal">True</span>)

url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg&quot;</span>
response = requests.get(url)
init_image = Image.<span class="hljs-built_in">open</span>(BytesIO(response.content)).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
init_image = init_image.resize((<span class="hljs-number">768</span>, <span class="hljs-number">512</span>))
prompt = <span class="hljs-string">&quot;A fantasy landscape, trending on artstation&quot;</span>
image = pipeline(prompt=prompt, image=init_image, strength=<span class="hljs-number">0.75</span>, guidance_scale=<span class="hljs-number">7.5</span>).images[<span class="hljs-number">0</span>]
image.save(<span class="hljs-string">&quot;fantasy_landscape.png&quot;</span>)`}}),at=new U({}),st=new T({props:{code:"cGlwJTIwaW5zdGFsbCUyMGRpZmZ1c2VycyUwQXBpcCUyMGluc3RhbGwlMjBpbnZpc2libGUtd2F0ZXJtYXJrJTNFJTNEMC4yLjA=",highlighted:`pip install diffusers
pip install invisible-watermark&gt;=0.2.0`}}),ot=new U({}),it=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVlN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi14bC1iYXNlLTEuMCUyMiUwQWJhc2UlMjAlM0QlMjBPVlN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQXByb21wdCUyMCUzRCUyMCUyMnRyYWluJTIwc3RhdGlvbiUyMGJ5JTIwQ2FzcGFyJTIwRGF2aWQlMjBGcmllZHJpY2glMjIlMEFpbWFnZSUyMCUzRCUyMGJhc2UocHJvbXB0KS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2Uuc2F2ZSglMjJ0cmFpbl9zdGF0aW9uLnBuZyUyMik=",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionXLPipeline

model_id = <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>
base = OVStableDiffusionXLPipeline.from_pretrained(model_id)
prompt = <span class="hljs-string">&quot;train station by Caspar David Friedrich&quot;</span>
image = base(prompt).images[<span class="hljs-number">0</span>]
image.save(<span class="hljs-string">&quot;train_station.png&quot;</span>)`}}),dt=new U({}),mt=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVlN0YWJsZURpZmZ1c2lvblhMUGlwZWxpbmUlMEFpbXBvcnQlMjBudW1weSUyMGFzJTIwbnAlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi14bC1iYXNlLTEuMCUyMiUwQXByb21wdCUyMCUzRCUyMCUyMmNoYXJ0dXJuZXJ2MiUyQyUyMG11bHRpcGxlJTIwdmlld3MlMjBvZiUyMHRoZSUyMHNhbWUlMjBjaGFyYWN0ZXIlMjBpbiUyMHRoZSUyMHNhbWUlMjBvdXRmaXQlMkMlMjBhJTIwY2hhcmFjdGVyJTIwdHVybmFyb3VuZCUyMG9mJTIwYSUyMGJlYXV0aWZ1bCUyMHdvbWFuJTIwd2VhcmluZyUyMGElMjByZWQlMjBqYWNrZXQlMjBhbmQlMjBibGFjayUyMHNoaXJ0JTJDJTIwYmVzdCUyMHF1YWxpdHklMkMlMjBpbnRyaWNhdGUlMjBkZXRhaWxzLiUyMiUwQSUyMyUyMFNldCUyMGElMjByYW5kb20lMjBzZWVkJTIwZm9yJTIwYmV0dGVyJTIwY29tcGFyaXNvbiUwQW5wLnJhbmRvbS5zZWVkKDExMiklMEElMEFiYXNlJTIwJTNEJTIwT1ZTdGFibGVEaWZmdXNpb25YTFBpcGVsaW5lLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMGV4cG9ydCUzREZhbHNlJTJDJTIwY29tcGlsZSUzREZhbHNlKSUwQWJhc2UuY29tcGlsZSgpJTBBaW1hZ2UxJTIwJTNEJTIwYmFzZShwcm9tcHQlMkMlMjBudW1faW5mZXJlbmNlX3N0ZXBzJTNENTApLmltYWdlcyU1QjAlNUQlMEFpbWFnZTEuc2F2ZSglMjJzZHhsX3dpdGhvdXRfdGV4dHVhbF9pbnZlcnNpb24ucG5nJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionXLPipeline
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

model_id = <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;</span>
prompt = <span class="hljs-string">&quot;charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a beautiful woman wearing a red jacket and black shirt, best quality, intricate details.&quot;</span>
<span class="hljs-comment"># Set a random seed for better comparison</span>
np.random.seed(<span class="hljs-number">112</span>)

base = OVStableDiffusionXLPipeline.from_pretrained(model_id, export=<span class="hljs-literal">False</span>, <span class="hljs-built_in">compile</span>=<span class="hljs-literal">False</span>)
base.<span class="hljs-built_in">compile</span>()
image1 = base(prompt, num_inference_steps=<span class="hljs-number">50</span>).images[<span class="hljs-number">0</span>]
image1.save(<span class="hljs-string">&quot;sdxl_without_textual_inversion.png&quot;</span>)`}}),ft=new T({props:{code:"JTIzJTIwUmVzZXQlMjBzdGFibGUlMjBkaWZmdXNpb24lMjBwaXBlbGluZSUwQWJhc2UuY2xlYXJfcmVxdWVzdHMoKSUwQSUwQSUyMyUyMExvYWQlMjB0ZXh0dWFsJTIwaW52ZXJzaW9uJTIwaW50byUyMHN0YWJsZSUyMGRpZmZ1c2lvbiUyMHBpcGVsaW5lJTBBYmFzZS5sb2FkX3RleHR1YWxfaW52ZXJzaW9uKCUyMi4lMkZjaGFydHVybmVydjIucHQlMjIlMkMlMjAlMjJjaGFydHVybmVydjIlMjIpJTBBJTBBJTIzJTIwQ29tcGlsZSUyMHRoZSUyMG1vZGVsJTIwYmVmb3JlJTIwdGhlJTIwZmlyc3QlMjBpbmZlcmVuY2UlMEFiYXNlLmNvbXBpbGUoKSUwQWltYWdlMiUyMCUzRCUyMGJhc2UocHJvbXB0JTJDJTIwbnVtX2luZmVyZW5jZV9zdGVwcyUzRDUwKS5pbWFnZXMlNUIwJTVEJTBBaW1hZ2UyLnNhdmUoJTIyc2R4bF93aXRoX3RleHR1YWxfaW52ZXJzaW9uLnBuZyUyMiklMEElMEFUaGUlMjBsZWZ0JTIwaW1hZ2UlMjBzaG93cyUyMHRoZSUyMGdlbmVyYXRpb24lMjByZXN1bHQlMjBvZiUyMHRoZSUyMG9yaWdpbmFsJTIwU0RYTCUyMGJhc2UlMjAxLjAlMkMlMjB0aGUlMjByaWdodCUyMGltYWdlJTIwc2hvd3MlMjB0aGUlMjBnZW5lcmF0aW9uJTIwcmVzdWx0JTIwb2YlMjBTRFhMJTIwYmFzZSUyMDEuMCUyMHdpdGglMjB0ZXh0dWFsJTIwaW52ZXJzaW9uLg==",highlighted:`<span class="hljs-comment"># Reset stable diffusion pipeline</span>
base.clear_requests()

<span class="hljs-comment"># Load textual inversion into stable diffusion pipeline</span>
base.load_textual_inversion(<span class="hljs-string">&quot;./charturnerv2.pt&quot;</span>, <span class="hljs-string">&quot;charturnerv2&quot;</span>)

<span class="hljs-comment"># Compile the model before the first inference</span>
base.<span class="hljs-built_in">compile</span>()
image2 = base(prompt, num_inference_steps=<span class="hljs-number">50</span>).images[<span class="hljs-number">0</span>]
image2.save(<span class="hljs-string">&quot;sdxl_with_textual_inversion.png&quot;</span>)

The left image shows the generation result of the original SDXL base <span class="hljs-number">1.0</span>, the right image shows the generation result of SDXL base <span class="hljs-number">1.0</span> <span class="hljs-keyword">with</span> textual inversion.`}}),yt=new U({}),Mt=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVlN0YWJsZURpZmZ1c2lvblhMSW1nMkltZ1BpcGVsaW5lJTBBZnJvbSUyMGRpZmZ1c2Vycy51dGlscyUyMGltcG9ydCUyMGxvYWRfaW1hZ2UlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlLWRpZmZ1c2lvbi14bC1yZWZpbmVyLTEuMCUyMiUwQXBpcGVsaW5lJTIwJTNEJTIwT1ZTdGFibGVEaWZmdXNpb25YTEltZzJJbWdQaXBlbGluZS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBleHBvcnQlM0RUcnVlKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGaHVnZ2luZ2ZhY2UuY28lMkZkYXRhc2V0cyUyRm9wdGltdW0lMkZkb2N1bWVudGF0aW9uLWltYWdlcyUyRnJlc29sdmUlMkZtYWluJTJGaW50ZWwlMkZvcGVudmlubyUyRnNkX3hsJTJGY2FzdGxlX2ZyaWVkcmljaC5wbmclMjIlMEFpbWFnZSUyMCUzRCUyMGxvYWRfaW1hZ2UodXJsKS5jb252ZXJ0KCUyMlJHQiUyMiklMEFwcm9tcHQlMjAlM0QlMjAlMjJtZWRpZXZhbCUyMGNhc3RsZSUyMGJ5JTIwQ2FzcGFyJTIwRGF2aWQlMjBGcmllZHJpY2glMjIlMEFpbWFnZSUyMCUzRCUyMHBpcGVsaW5lKHByb21wdCUyQyUyMGltYWdlJTNEaW1hZ2UpLmltYWdlcyU1QjAlNUQlMEElMjMlMjBEb24ndCUyMGZvcmdldCUyMHRvJTIwc2F2ZSUyMHlvdXIlMjBPcGVuVklOTyUyMG1vZGVsJTIwc28lMjB0aGF0JTIweW91JTIwY2FuJTIwbG9hZCUyMGl0JTIwd2l0aG91dCUyMGV4cG9ydGluZyUyMGl0JTIwd2l0aCUyMCU2MGV4cG9ydCUzRFRydWUlNjAlMEFwaXBlbGluZS5zYXZlX3ByZXRyYWluZWQoJTIyb3BlbnZpbm8tc2QteGwtcmVmaW5lci0xLjAlMjIp",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionXLImg2ImgPipeline
<span class="hljs-keyword">from</span> diffusers.utils <span class="hljs-keyword">import</span> load_image

model_id = <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;</span>
pipeline = OVStableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, export=<span class="hljs-literal">True</span>)

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/sd_xl/castle_friedrich.png&quot;</span>
image = load_image(url).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
prompt = <span class="hljs-string">&quot;medieval castle by Caspar David Friedrich&quot;</span>
image = pipeline(prompt, image=image).images[<span class="hljs-number">0</span>]
<span class="hljs-comment"># Don&#x27;t forget to save your OpenVINO model so that you can load it without exporting it with \`export=True\`</span>
pipeline.save_pretrained(<span class="hljs-string">&quot;openvino-sd-xl-refiner-1.0&quot;</span>)`}}),wt=new U({}),Tt=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVlN0YWJsZURpZmZ1c2lvblhMSW1nMkltZ1BpcGVsaW5lJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJzdGFiaWxpdHlhaSUyRnN0YWJsZS1kaWZmdXNpb24teGwtcmVmaW5lci0xLjAlMjIlMEFyZWZpbmVyJTIwJTNEJTIwT1ZTdGFibGVEaWZmdXNpb25YTEltZzJJbWdQaXBlbGluZS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBleHBvcnQlM0RUcnVlKSUwQSUwQWltYWdlJTIwJTNEJTIwYmFzZShwcm9tcHQlM0Rwcm9tcHQlMkMlMjBvdXRwdXRfdHlwZSUzRCUyMmxhdGVudCUyMikuaW1hZ2VzJTVCMCU1RCUwQWltYWdlJTIwJTNEJTIwcmVmaW5lcihwcm9tcHQlM0Rwcm9tcHQlMkMlMjBpbWFnZSUzRGltYWdlJTVCTm9uZSUyQyUyMCUzQSU1RCkuaW1hZ2VzJTVCMCU1RA==",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVStableDiffusionXLImg2ImgPipeline

model_id = <span class="hljs-string">&quot;stabilityai/stable-diffusion-xl-refiner-1.0&quot;</span>
refiner = OVStableDiffusionXLImg2ImgPipeline.from_pretrained(model_id, export=<span class="hljs-literal">True</span>)

image = base(prompt=prompt, output_type=<span class="hljs-string">&quot;latent&quot;</span>).images[<span class="hljs-number">0</span>]
image = refiner(prompt=prompt, image=image[<span class="hljs-literal">None</span>, :]).images[<span class="hljs-number">0</span>]`}}),Jt=new U({}),{c(){B=a("meta"),Js=p(),k=a("h1"),z=a("a"),Kt=a("span"),h(_e.$$.fragment),Dn=p(),el=a("span"),Yn=i("Optimum Inference with OpenVINO"),Zs=p(),q=a("p"),zn=i("Optimum Intel can be used to load optimized models from the "),ge=a("a"),qn=i("Hugging Face Hub"),Hn=i(" and create pipelines to run inference with OpenVINO Runtime without rewriting your APIs."),Us=p(),X=a("h2"),H=a("a"),tl=a("span"),h(Ee.$$.fragment),An=p(),ll=a("span"),Pn=i("Switching from Transformers to Optimum"),Ws=p(),J=a("p"),Ln=i("You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ("),je=a("a"),Kn=i("see"),ei=i(` the full list of supported devices).
For that, just replace the `),al=a("code"),ti=i("AutoModelForXxx"),li=i(" class with the corresponding "),sl=a("code"),ai=i("OVModelForXxx"),si=i(` class.
To load a Transformers model and convert it to the OpenVINO format on-the-fly, you can set `),ol=a("code"),oi=i("export=True"),ni=i(" when loading your model."),_s=p(),Nt=a("p"),ii=i("Here is an example on how to perform inference with OpenVINO Runtime for a text classification class:"),gs=p(),h(Ie.$$.fragment),Es=p(),A=a("p"),ri=i("See the "),$t=a("a"),pi=i("reference documentation"),di=i(" for more information about parameters, and examples for different tasks."),js=p(),P=a("p"),ci=i("To easily save the resulting model, you can use the "),nl=a("code"),mi=i("save_pretrained()"),hi=i(" method, which will save both the BIN and XML files describing the graph. It is useful to save the tokenizer to the same directory, to enable easy loading of the tokenizer for the model."),Is=p(),h(Ge.$$.fragment),Gs=p(),L=a("p"),fi=i("By default, "),il=a("code"),ui=i("OVModelForXxx"),bi=i(" support dynamic shapes, enabling inputs of every shapes. To speed up inference, static shapes can be enabled by giving the desired inputs shapes."),Vs=p(),h(Ve.$$.fragment),Bs=p(),K=a("p"),yi=i("When fixing the shapes with the "),rl=a("code"),Mi=i("reshape()"),wi=i(" method, inference cannot be performed with an input of a different shape. When instantiating your pipeline, you can specify the maximum total input sequence length after tokenization in order for shorter sequences to be padded and for longer sequences to be truncated."),ks=p(),h(Be.$$.fragment),Xs=p(),E=a("p"),vi=i("To run inference on Intel integrated or discrete GPU, use "),pl=a("code"),Ti=i('.to("gpu")'),Ji=i(". On GPU, models run in FP16 precision by default. (See "),ke=a("a"),Zi=i("OpenVINO documentation"),Ui=i(" about installing drivers for GPU inference)."),xs=p(),h(Xe.$$.fragment),Rs=p(),W=a("p"),Wi=i("By default the model will be compiled when instantiating our "),dl=a("code"),_i=i("OVModel"),gi=i(". In the case where the model is reshaped or placed to another device, the model will need to be recompiled again, which will happen by default before the first inference (thus inflating the latency of the first inference). To avoid an unnecessary compilation, you can disable the first compilation by setting "),cl=a("code"),Ei=i("compile=False"),ji=i(". The model can be compiled before the first inference with "),ml=a("code"),Ii=i("model.compile()"),Gi=i("."),Ss=p(),h(xe.$$.fragment),Ns=p(),j=a("p"),Vi=i("It is possible to pass an "),hl=a("code"),Bi=i("ov_config"),ki=i(" parameter to "),fl=a("code"),Xi=i("from_pretrained()"),xi=i(" with custom OpenVINO configuration values. This can be used for example to enable full precision inference on devices where FP16 or BF16 inference precision is used by default."),$s=p(),h(Re.$$.fragment),Fs=p(),Z=a("p"),Ri=i("Optimum Intel leverages OpenVINO\u2019s model caching to speed up model compiling. By default a "),ul=a("code"),Si=i("model_cache"),Ni=i(" directory is created in the model\u2019s directory in the "),Se=a("a"),$i=i("Hugging Face Hub cache"),Fi=i(". To override this, use the ov_config parameter and set "),bl=a("code"),Ci=i("CACHE_DIR"),Oi=i(" to a different value. To disable model caching, set "),yl=a("code"),Qi=i("CACHE_DIR"),Di=i(" to an empty string."),Cs=p(),h(Ne.$$.fragment),Os=p(),x=a("h2"),ee=a("a"),Ml=a("span"),h($e.$$.fragment),Yi=p(),wl=a("span"),zi=i("Sequence-to-sequence models"),Qs=p(),I=a("p"),qi=i(`Sequence-to-sequence (Seq2Seq) models, that generate a new sequence from an input, can also be used when running inference with OpenVINO. When Seq2Seq models are exported to the OpenVINO IR, they are decomposed into two parts : the encoder and the \u201Cdecoder\u201D (which actually consists of the decoder with the language modeling head), that are later combined during inference.
To speed up sequential decoding, a cache with pre-computed key/values hidden-states will be used by default. An additional model component will be exported: the \u201Cdecoder\u201D with pre-computed key/values as one of its inputs.  This specific export comes from the fact that during the first pass, the decoder has no pre-computed key/values hidden-states, while during the rest of the generation past key/values will be used to speed up sequential decoding. To disable this cache, set `),vl=a("code"),Hi=i("use_cache=False"),Ai=i(" in the "),Tl=a("code"),Pi=i("from_pretrained()"),Li=i(" method."),Ds=p(),Ft=a("p"),Ki=i("Here is an example on how you can run inference for a translation task using a T5 model and then export it to OpenVINO IR:"),Ys=p(),h(Fe.$$.fragment),zs=p(),R=a("h2"),te=a("a"),Jl=a("span"),h(Ce.$$.fragment),er=p(),Zl=a("span"),tr=i("Stable Diffusion"),qs=p(),Ct=a("p"),lr=i(`Stable Diffusion models can also be used when running inference with OpenVINO. When Stable Diffusion models
are exported to the OpenVINO format, they are decomposed into three components that are later combined during inference:`),Hs=p(),_=a("ul"),Ul=a("li"),ar=i("The text encoder"),sr=p(),Wl=a("li"),or=i("The U-NET"),nr=p(),_l=a("li"),ir=i("The VAE encoder"),rr=p(),gl=a("li"),pr=i("The VAE decoder"),As=p(),Ot=a("p"),dr=i("Make sure you have \u{1F917} Diffusers installed."),Ps=p(),le=a("p"),cr=i("To install "),El=a("code"),mr=i("diffusers"),hr=i(":"),Ls=p(),h(Oe.$$.fragment),Ks=p(),S=a("h3"),ae=a("a"),jl=a("span"),h(Qe.$$.fragment),fr=p(),Il=a("span"),ur=i("Text-to-Image"),eo=i(`

Here is an example of how you can load an OpenVINO Stable Diffusion model and run inference using OpenVINO Runtime:

	`),h(De.$$.fragment),to=p(),se=a("p"),br=i("To load your PyTorch model and convert it to OpenVINO on-the-fly, you can set "),Gl=a("code"),yr=i("export=True"),Mr=i("."),lo=p(),h(Ye.$$.fragment),ao=p(),Qt=a("p"),wr=i("To further speed up inference, the model can be statically reshaped :"),so=p(),h(ze.$$.fragment),oo=p(),Dt=a("p"),vr=i("In case you want to change any parameters such as the outputs height or width, you\u2019ll need to statically reshape your model once again."),no=p(),qe=a("div"),Vl=a("img"),io=p(),N=a("h3"),oe=a("a"),Bl=a("span"),h(He.$$.fragment),Tr=p(),kl=a("span"),Jr=i("Text-to-Image with Textual Inversion"),ro=i(`

Here is an example of how you can load an OpenVINO Stable Diffusion model with pre-trained textual inversion embeddings and run inference using OpenVINO Runtime:
`),Yt=a("p"),Zr=i("First, you can run original pipeline without textual inversion"),po=p(),h(Ae.$$.fragment),co=p(),ne=a("p"),Ur=i("Then, you can load "),Pe=a("a"),Wr=i("sd-concepts-library/cat-toy"),_r=i(" textual inversion embedding and run pipeline with same prompt again"),mo=p(),h(Le.$$.fragment),ho=p(),zt=a("p"),gr=i("The left image shows the generation result of original stable diffusion v1.5, the right image shows the generation result of stable diffusion v1.5 with textual inversion."),fo=p(),ie=a("table"),Xl=a("thead"),Ke=a("tr"),uo=a("th"),Er=p(),bo=a("th"),jr=p(),xl=a("tbody"),et=a("tr"),Rl=a("td"),Sl=a("img"),Ir=p(),Nl=a("td"),$l=a("img"),yo=p(),$=a("h3"),re=a("a"),Fl=a("span"),h(tt.$$.fragment),Gr=p(),Cl=a("span"),Vr=i("Image-to-Image"),Mo=p(),h(lt.$$.fragment),wo=p(),F=a("h2"),pe=a("a"),Ol=a("span"),h(at.$$.fragment),Br=p(),Ql=a("span"),kr=i("Stable Diffusion XL"),vo=p(),g=a("p"),Xr=i("Before using "),Dl=a("code"),xr=i("OVtableDiffusionXLPipeline"),Rr=i(" make sure to have "),Yl=a("code"),Sr=i("diffusers"),Nr=i(" and "),zl=a("code"),$r=i("invisible_watermark"),Fr=i(" installed. You can install the libraries as follows:"),To=p(),h(st.$$.fragment),Jo=p(),C=a("h3"),de=a("a"),ql=a("span"),h(ot.$$.fragment),Cr=p(),Hl=a("span"),Or=i("Text-to-Image"),Zo=p(),ce=a("p"),Qr=i("Here is an example of how you can load a SDXL OpenVINO model from "),nt=a("a"),Dr=i("stabilityai/stable-diffusion-xl-base-1.0"),Yr=i(" and run inference using OpenVINO Runtime:"),Uo=p(),h(it.$$.fragment),Wo=p(),me=a("table"),Al=a("thead"),rt=a("tr"),_o=a("th"),zr=p(),go=a("th"),qr=p(),Pl=a("tbody"),pt=a("tr"),Ll=a("td"),Kl=a("img"),Hr=p(),ea=a("td"),ta=a("img"),Eo=p(),O=a("h3"),he=a("a"),la=a("span"),h(dt.$$.fragment),Ar=p(),aa=a("span"),Pr=i("Text-to-Image with Textual Inversion"),jo=p(),fe=a("p"),Lr=i("Here is an example of how you can load an SDXL OpenVINO model from "),ct=a("a"),Kr=i("stabilityai/stable-diffusion-xl-base-1.0"),ep=i(" with pre-trained textual inversion embeddings and run inference using OpenVINO Runtime:"),Io=p(),qt=a("p"),tp=i("First, you can run original pipeline without textual inversion"),Go=p(),h(mt.$$.fragment),Vo=p(),ue=a("p"),lp=i("Then, you can load "),ht=a("a"),ap=i("charturnerv2"),sp=i(" textual inversion embedding and run pipeline with same prompt again"),Bo=p(),h(ft.$$.fragment),ko=p(),be=a("table"),sa=a("thead"),ut=a("tr"),Xo=a("th"),op=p(),xo=a("th"),np=p(),oa=a("tbody"),bt=a("tr"),na=a("td"),ia=a("img"),ip=p(),ra=a("td"),pa=a("img"),Ro=p(),Q=a("h3"),ye=a("a"),da=a("span"),h(yt.$$.fragment),rp=p(),ca=a("span"),pp=i("Image-to-Image"),So=p(),Me=a("p"),dp=i("Here is an example of how you can load a PyTorch SDXL model, convert it to OpenVINO on-the-fly and run inference using OpenVINO Runtime for "),ma=a("em"),cp=i("image-to-image"),mp=i(":"),No=p(),h(Mt.$$.fragment),$o=p(),D=a("h3"),we=a("a"),ha=a("span"),h(wt.$$.fragment),hp=p(),fa=a("span"),fp=i("Refining the image output"),Fo=p(),ve=a("p"),up=i("The image can be refined by making use of a model like "),vt=a("a"),bp=i("stabilityai/stable-diffusion-xl-refiner-1.0"),yp=i(". In this case, you only have to output the latents from the base model."),Co=p(),h(Tt.$$.fragment),Oo=p(),Y=a("h2"),Te=a("a"),ua=a("span"),h(Jt.$$.fragment),Mp=p(),ba=a("span"),wp=i("Supported tasks"),Qo=p(),Ht=a("p"),vp=i("As shown in the table below, each task is associated with a class enabling to automatically load your model."),Do=p(),Je=a("table"),ya=a("thead"),Zt=a("tr"),Ma=a("th"),Tp=i("Task"),Jp=p(),wa=a("th"),Zp=i("Auto Class"),Up=p(),w=a("tbody"),Ut=a("tr"),va=a("td"),Ta=a("code"),Wp=i("text-classification"),_p=p(),Ja=a("td"),Za=a("code"),gp=i("OVModelForSequenceClassification"),Ep=p(),Wt=a("tr"),Ua=a("td"),Wa=a("code"),jp=i("token-classification"),Ip=p(),_a=a("td"),ga=a("code"),Gp=i("OVModelForTokenClassification"),Vp=p(),_t=a("tr"),Ea=a("td"),ja=a("code"),Bp=i("question-answering"),kp=p(),Ia=a("td"),Ga=a("code"),Xp=i("OVModelForQuestionAnswering"),xp=p(),gt=a("tr"),Va=a("td"),Ba=a("code"),Rp=i("audio-classification"),Sp=p(),ka=a("td"),Xa=a("code"),Np=i("OVModelForAudioClassification"),$p=p(),Et=a("tr"),xa=a("td"),Ra=a("code"),Fp=i("image-classification"),Cp=p(),Sa=a("td"),Na=a("code"),Op=i("OVModelForImageClassification"),Qp=p(),jt=a("tr"),$a=a("td"),Fa=a("code"),Dp=i("feature-extraction"),Yp=p(),Ca=a("td"),Oa=a("code"),zp=i("OVModelForFeatureExtraction"),qp=p(),It=a("tr"),Qa=a("td"),Da=a("code"),Hp=i("fill-mask"),Ap=p(),Ya=a("td"),za=a("code"),Pp=i("OVModelForMaskedLM"),Lp=p(),Gt=a("tr"),qa=a("td"),Ha=a("code"),Kp=i("text-generation"),ed=p(),Aa=a("td"),Pa=a("code"),td=i("OVModelForCausalLM"),ld=p(),Vt=a("tr"),La=a("td"),Ka=a("code"),ad=i("text2text-generation"),sd=p(),es=a("td"),ts=a("code"),od=i("OVModelForSeq2SeqLM"),nd=p(),Bt=a("tr"),ls=a("td"),as=a("code"),id=i("text-to-image"),rd=p(),ss=a("td"),os=a("code"),pd=i("OVStableDiffusionPipeline"),dd=p(),kt=a("tr"),ns=a("td"),is=a("code"),cd=i("text-to-image"),md=p(),rs=a("td"),ps=a("code"),hd=i("OVStableDiffusionXLPipeline"),fd=p(),Xt=a("tr"),ds=a("td"),cs=a("code"),ud=i("image-to-image"),bd=p(),ms=a("td"),hs=a("code"),yd=i("OVStableDiffusionImg2ImgPipeline"),Md=p(),xt=a("tr"),fs=a("td"),us=a("code"),wd=i("image-to-image"),vd=p(),bs=a("td"),ys=a("code"),Td=i("OVStableDiffusionXLImg2ImgPipeline"),Jd=p(),Rt=a("tr"),Ms=a("td"),ws=a("code"),Zd=i("inpaint"),Ud=p(),vs=a("td"),Ts=a("code"),Wd=i("OVStableDiffusionInpaintPipeline"),this.h()},l(e){const n=Bh('[data-svelte="svelte-1phssyn"]',document.head);B=s(n,"META",{name:!0,content:!0}),n.forEach(l),Js=d(e),k=s(e,"H1",{class:!0});var zo=o(k);z=s(zo,"A",{id:!0,class:!0,href:!0});var kd=o(z);Kt=s(kd,"SPAN",{});var Xd=o(Kt);f(_e.$$.fragment,Xd),Xd.forEach(l),kd.forEach(l),Dn=d(zo),el=s(zo,"SPAN",{});var xd=o(el);Yn=r(xd,"Optimum Inference with OpenVINO"),xd.forEach(l),zo.forEach(l),Zs=d(e),q=s(e,"P",{});var qo=o(q);zn=r(qo,"Optimum Intel can be used to load optimized models from the "),ge=s(qo,"A",{href:!0,rel:!0});var Rd=o(ge);qn=r(Rd,"Hugging Face Hub"),Rd.forEach(l),Hn=r(qo," and create pipelines to run inference with OpenVINO Runtime without rewriting your APIs."),qo.forEach(l),Us=d(e),X=s(e,"H2",{class:!0});var Ho=o(X);H=s(Ho,"A",{id:!0,class:!0,href:!0});var Sd=o(H);tl=s(Sd,"SPAN",{});var Nd=o(tl);f(Ee.$$.fragment,Nd),Nd.forEach(l),Sd.forEach(l),An=d(Ho),ll=s(Ho,"SPAN",{});var $d=o(ll);Pn=r($d,"Switching from Transformers to Optimum"),$d.forEach(l),Ho.forEach(l),Ws=d(e),J=s(e,"P",{});var G=o(J);Ln=r(G,"You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors ("),je=s(G,"A",{href:!0,rel:!0});var Fd=o(je);Kn=r(Fd,"see"),Fd.forEach(l),ei=r(G,` the full list of supported devices).
For that, just replace the `),al=s(G,"CODE",{});var Cd=o(al);ti=r(Cd,"AutoModelForXxx"),Cd.forEach(l),li=r(G," class with the corresponding "),sl=s(G,"CODE",{});var Od=o(sl);ai=r(Od,"OVModelForXxx"),Od.forEach(l),si=r(G,` class.
To load a Transformers model and convert it to the OpenVINO format on-the-fly, you can set `),ol=s(G,"CODE",{});var Qd=o(ol);oi=r(Qd,"export=True"),Qd.forEach(l),ni=r(G," when loading your model."),G.forEach(l),_s=d(e),Nt=s(e,"P",{});var Dd=o(Nt);ii=r(Dd,"Here is an example on how to perform inference with OpenVINO Runtime for a text classification class:"),Dd.forEach(l),gs=d(e),f(Ie.$$.fragment,e),Es=d(e),A=s(e,"P",{});var Ao=o(A);ri=r(Ao,"See the "),$t=s(Ao,"A",{href:!0});var Yd=o($t);pi=r(Yd,"reference documentation"),Yd.forEach(l),di=r(Ao," for more information about parameters, and examples for different tasks."),Ao.forEach(l),js=d(e),P=s(e,"P",{});var Po=o(P);ci=r(Po,"To easily save the resulting model, you can use the "),nl=s(Po,"CODE",{});var zd=o(nl);mi=r(zd,"save_pretrained()"),zd.forEach(l),hi=r(Po," method, which will save both the BIN and XML files describing the graph. It is useful to save the tokenizer to the same directory, to enable easy loading of the tokenizer for the model."),Po.forEach(l),Is=d(e),f(Ge.$$.fragment,e),Gs=d(e),L=s(e,"P",{});var Lo=o(L);fi=r(Lo,"By default, "),il=s(Lo,"CODE",{});var qd=o(il);ui=r(qd,"OVModelForXxx"),qd.forEach(l),bi=r(Lo," support dynamic shapes, enabling inputs of every shapes. To speed up inference, static shapes can be enabled by giving the desired inputs shapes."),Lo.forEach(l),Vs=d(e),f(Ve.$$.fragment,e),Bs=d(e),K=s(e,"P",{});var Ko=o(K);yi=r(Ko,"When fixing the shapes with the "),rl=s(Ko,"CODE",{});var Hd=o(rl);Mi=r(Hd,"reshape()"),Hd.forEach(l),wi=r(Ko," method, inference cannot be performed with an input of a different shape. When instantiating your pipeline, you can specify the maximum total input sequence length after tokenization in order for shorter sequences to be padded and for longer sequences to be truncated."),Ko.forEach(l),ks=d(e),f(Be.$$.fragment,e),Xs=d(e),E=s(e,"P",{});var At=o(E);vi=r(At,"To run inference on Intel integrated or discrete GPU, use "),pl=s(At,"CODE",{});var Ad=o(pl);Ti=r(Ad,'.to("gpu")'),Ad.forEach(l),Ji=r(At,". On GPU, models run in FP16 precision by default. (See "),ke=s(At,"A",{href:!0,rel:!0});var Pd=o(ke);Zi=r(Pd,"OpenVINO documentation"),Pd.forEach(l),Ui=r(At," about installing drivers for GPU inference)."),At.forEach(l),xs=d(e),f(Xe.$$.fragment,e),Rs=d(e),W=s(e,"P",{});var Ze=o(W);Wi=r(Ze,"By default the model will be compiled when instantiating our "),dl=s(Ze,"CODE",{});var Ld=o(dl);_i=r(Ld,"OVModel"),Ld.forEach(l),gi=r(Ze,". In the case where the model is reshaped or placed to another device, the model will need to be recompiled again, which will happen by default before the first inference (thus inflating the latency of the first inference). To avoid an unnecessary compilation, you can disable the first compilation by setting "),cl=s(Ze,"CODE",{});var Kd=o(cl);Ei=r(Kd,"compile=False"),Kd.forEach(l),ji=r(Ze,". The model can be compiled before the first inference with "),ml=s(Ze,"CODE",{});var ec=o(ml);Ii=r(ec,"model.compile()"),ec.forEach(l),Gi=r(Ze,"."),Ze.forEach(l),Ss=d(e),f(xe.$$.fragment,e),Ns=d(e),j=s(e,"P",{});var Pt=o(j);Vi=r(Pt,"It is possible to pass an "),hl=s(Pt,"CODE",{});var tc=o(hl);Bi=r(tc,"ov_config"),tc.forEach(l),ki=r(Pt," parameter to "),fl=s(Pt,"CODE",{});var lc=o(fl);Xi=r(lc,"from_pretrained()"),lc.forEach(l),xi=r(Pt," with custom OpenVINO configuration values. This can be used for example to enable full precision inference on devices where FP16 or BF16 inference precision is used by default."),Pt.forEach(l),$s=d(e),f(Re.$$.fragment,e),Fs=d(e),Z=s(e,"P",{});var V=o(Z);Ri=r(V,"Optimum Intel leverages OpenVINO\u2019s model caching to speed up model compiling. By default a "),ul=s(V,"CODE",{});var ac=o(ul);Si=r(ac,"model_cache"),ac.forEach(l),Ni=r(V," directory is created in the model\u2019s directory in the "),Se=s(V,"A",{href:!0,rel:!0});var sc=o(Se);$i=r(sc,"Hugging Face Hub cache"),sc.forEach(l),Fi=r(V,". To override this, use the ov_config parameter and set "),bl=s(V,"CODE",{});var oc=o(bl);Ci=r(oc,"CACHE_DIR"),oc.forEach(l),Oi=r(V," to a different value. To disable model caching, set "),yl=s(V,"CODE",{});var nc=o(yl);Qi=r(nc,"CACHE_DIR"),nc.forEach(l),Di=r(V," to an empty string."),V.forEach(l),Cs=d(e),f(Ne.$$.fragment,e),Os=d(e),x=s(e,"H2",{class:!0});var en=o(x);ee=s(en,"A",{id:!0,class:!0,href:!0});var ic=o(ee);Ml=s(ic,"SPAN",{});var rc=o(Ml);f($e.$$.fragment,rc),rc.forEach(l),ic.forEach(l),Yi=d(en),wl=s(en,"SPAN",{});var pc=o(wl);zi=r(pc,"Sequence-to-sequence models"),pc.forEach(l),en.forEach(l),Qs=d(e),I=s(e,"P",{});var Lt=o(I);qi=r(Lt,`Sequence-to-sequence (Seq2Seq) models, that generate a new sequence from an input, can also be used when running inference with OpenVINO. When Seq2Seq models are exported to the OpenVINO IR, they are decomposed into two parts : the encoder and the \u201Cdecoder\u201D (which actually consists of the decoder with the language modeling head), that are later combined during inference.
To speed up sequential decoding, a cache with pre-computed key/values hidden-states will be used by default. An additional model component will be exported: the \u201Cdecoder\u201D with pre-computed key/values as one of its inputs.  This specific export comes from the fact that during the first pass, the decoder has no pre-computed key/values hidden-states, while during the rest of the generation past key/values will be used to speed up sequential decoding. To disable this cache, set `),vl=s(Lt,"CODE",{});var dc=o(vl);Hi=r(dc,"use_cache=False"),dc.forEach(l),Ai=r(Lt," in the "),Tl=s(Lt,"CODE",{});var cc=o(Tl);Pi=r(cc,"from_pretrained()"),cc.forEach(l),Li=r(Lt," method."),Lt.forEach(l),Ds=d(e),Ft=s(e,"P",{});var mc=o(Ft);Ki=r(mc,"Here is an example on how you can run inference for a translation task using a T5 model and then export it to OpenVINO IR:"),mc.forEach(l),Ys=d(e),f(Fe.$$.fragment,e),zs=d(e),R=s(e,"H2",{class:!0});var tn=o(R);te=s(tn,"A",{id:!0,class:!0,href:!0});var hc=o(te);Jl=s(hc,"SPAN",{});var fc=o(Jl);f(Ce.$$.fragment,fc),fc.forEach(l),hc.forEach(l),er=d(tn),Zl=s(tn,"SPAN",{});var uc=o(Zl);tr=r(uc,"Stable Diffusion"),uc.forEach(l),tn.forEach(l),qs=d(e),Ct=s(e,"P",{});var bc=o(Ct);lr=r(bc,`Stable Diffusion models can also be used when running inference with OpenVINO. When Stable Diffusion models
are exported to the OpenVINO format, they are decomposed into three components that are later combined during inference:`),bc.forEach(l),Hs=d(e),_=s(e,"UL",{});var Ue=o(_);Ul=s(Ue,"LI",{});var yc=o(Ul);ar=r(yc,"The text encoder"),yc.forEach(l),sr=d(Ue),Wl=s(Ue,"LI",{});var Mc=o(Wl);or=r(Mc,"The U-NET"),Mc.forEach(l),nr=d(Ue),_l=s(Ue,"LI",{});var wc=o(_l);ir=r(wc,"The VAE encoder"),wc.forEach(l),rr=d(Ue),gl=s(Ue,"LI",{});var vc=o(gl);pr=r(vc,"The VAE decoder"),vc.forEach(l),Ue.forEach(l),As=d(e),Ot=s(e,"P",{});var Tc=o(Ot);dr=r(Tc,"Make sure you have \u{1F917} Diffusers installed."),Tc.forEach(l),Ps=d(e),le=s(e,"P",{});var ln=o(le);cr=r(ln,"To install "),El=s(ln,"CODE",{});var Jc=o(El);mr=r(Jc,"diffusers"),Jc.forEach(l),hr=r(ln,":"),ln.forEach(l),Ls=d(e),f(Oe.$$.fragment,e),Ks=d(e),S=s(e,"H3",{class:!0});var an=o(S);ae=s(an,"A",{id:!0,class:!0,href:!0});var Zc=o(ae);jl=s(Zc,"SPAN",{});var Uc=o(jl);f(Qe.$$.fragment,Uc),Uc.forEach(l),Zc.forEach(l),fr=d(an),Il=s(an,"SPAN",{});var Wc=o(Il);ur=r(Wc,"Text-to-Image"),Wc.forEach(l),an.forEach(l),eo=r(e,`

Here is an example of how you can load an OpenVINO Stable Diffusion model and run inference using OpenVINO Runtime:

	`),f(De.$$.fragment,e),to=d(e),se=s(e,"P",{});var sn=o(se);br=r(sn,"To load your PyTorch model and convert it to OpenVINO on-the-fly, you can set "),Gl=s(sn,"CODE",{});var _c=o(Gl);yr=r(_c,"export=True"),_c.forEach(l),Mr=r(sn,"."),sn.forEach(l),lo=d(e),f(Ye.$$.fragment,e),ao=d(e),Qt=s(e,"P",{});var gc=o(Qt);wr=r(gc,"To further speed up inference, the model can be statically reshaped :"),gc.forEach(l),so=d(e),f(ze.$$.fragment,e),oo=d(e),Dt=s(e,"P",{});var Ec=o(Dt);vr=r(Ec,"In case you want to change any parameters such as the outputs height or width, you\u2019ll need to statically reshape your model once again."),Ec.forEach(l),no=d(e),qe=s(e,"DIV",{class:!0});var jc=o(qe);Vl=s(jc,"IMG",{src:!0}),jc.forEach(l),io=d(e),N=s(e,"H3",{class:!0});var on=o(N);oe=s(on,"A",{id:!0,class:!0,href:!0});var Ic=o(oe);Bl=s(Ic,"SPAN",{});var Gc=o(Bl);f(He.$$.fragment,Gc),Gc.forEach(l),Ic.forEach(l),Tr=d(on),kl=s(on,"SPAN",{});var Vc=o(kl);Jr=r(Vc,"Text-to-Image with Textual Inversion"),Vc.forEach(l),on.forEach(l),ro=r(e,`

Here is an example of how you can load an OpenVINO Stable Diffusion model with pre-trained textual inversion embeddings and run inference using OpenVINO Runtime:
`),Yt=s(e,"P",{});var Bc=o(Yt);Zr=r(Bc,"First, you can run original pipeline without textual inversion"),Bc.forEach(l),po=d(e),f(Ae.$$.fragment,e),co=d(e),ne=s(e,"P",{});var nn=o(ne);Ur=r(nn,"Then, you can load "),Pe=s(nn,"A",{href:!0,rel:!0});var kc=o(Pe);Wr=r(kc,"sd-concepts-library/cat-toy"),kc.forEach(l),_r=r(nn," textual inversion embedding and run pipeline with same prompt again"),nn.forEach(l),mo=d(e),f(Le.$$.fragment,e),ho=d(e),zt=s(e,"P",{});var Xc=o(zt);gr=r(Xc,"The left image shows the generation result of original stable diffusion v1.5, the right image shows the generation result of stable diffusion v1.5 with textual inversion."),Xc.forEach(l),fo=d(e),ie=s(e,"TABLE",{});var rn=o(ie);Xl=s(rn,"THEAD",{});var xc=o(Xl);Ke=s(xc,"TR",{});var pn=o(Ke);uo=s(pn,"TH",{}),o(uo).forEach(l),Er=d(pn),bo=s(pn,"TH",{}),o(bo).forEach(l),pn.forEach(l),xc.forEach(l),jr=d(rn),xl=s(rn,"TBODY",{});var Rc=o(xl);et=s(Rc,"TR",{});var dn=o(et);Rl=s(dn,"TD",{});var Sc=o(Rl);Sl=s(Sc,"IMG",{src:!0}),Sc.forEach(l),Ir=d(dn),Nl=s(dn,"TD",{});var Nc=o(Nl);$l=s(Nc,"IMG",{src:!0}),Nc.forEach(l),dn.forEach(l),Rc.forEach(l),rn.forEach(l),yo=d(e),$=s(e,"H3",{class:!0});var cn=o($);re=s(cn,"A",{id:!0,class:!0,href:!0});var $c=o(re);Fl=s($c,"SPAN",{});var Fc=o(Fl);f(tt.$$.fragment,Fc),Fc.forEach(l),$c.forEach(l),Gr=d(cn),Cl=s(cn,"SPAN",{});var Cc=o(Cl);Vr=r(Cc,"Image-to-Image"),Cc.forEach(l),cn.forEach(l),Mo=d(e),f(lt.$$.fragment,e),wo=d(e),F=s(e,"H2",{class:!0});var mn=o(F);pe=s(mn,"A",{id:!0,class:!0,href:!0});var Oc=o(pe);Ol=s(Oc,"SPAN",{});var Qc=o(Ol);f(at.$$.fragment,Qc),Qc.forEach(l),Oc.forEach(l),Br=d(mn),Ql=s(mn,"SPAN",{});var Dc=o(Ql);kr=r(Dc,"Stable Diffusion XL"),Dc.forEach(l),mn.forEach(l),vo=d(e),g=s(e,"P",{});var We=o(g);Xr=r(We,"Before using "),Dl=s(We,"CODE",{});var Yc=o(Dl);xr=r(Yc,"OVtableDiffusionXLPipeline"),Yc.forEach(l),Rr=r(We," make sure to have "),Yl=s(We,"CODE",{});var zc=o(Yl);Sr=r(zc,"diffusers"),zc.forEach(l),Nr=r(We," and "),zl=s(We,"CODE",{});var qc=o(zl);$r=r(qc,"invisible_watermark"),qc.forEach(l),Fr=r(We," installed. You can install the libraries as follows:"),We.forEach(l),To=d(e),f(st.$$.fragment,e),Jo=d(e),C=s(e,"H3",{class:!0});var hn=o(C);de=s(hn,"A",{id:!0,class:!0,href:!0});var Hc=o(de);ql=s(Hc,"SPAN",{});var Ac=o(ql);f(ot.$$.fragment,Ac),Ac.forEach(l),Hc.forEach(l),Cr=d(hn),Hl=s(hn,"SPAN",{});var Pc=o(Hl);Or=r(Pc,"Text-to-Image"),Pc.forEach(l),hn.forEach(l),Zo=d(e),ce=s(e,"P",{});var fn=o(ce);Qr=r(fn,"Here is an example of how you can load a SDXL OpenVINO model from "),nt=s(fn,"A",{href:!0,rel:!0});var Lc=o(nt);Dr=r(Lc,"stabilityai/stable-diffusion-xl-base-1.0"),Lc.forEach(l),Yr=r(fn," and run inference using OpenVINO Runtime:"),fn.forEach(l),Uo=d(e),f(it.$$.fragment,e),Wo=d(e),me=s(e,"TABLE",{});var un=o(me);Al=s(un,"THEAD",{});var Kc=o(Al);rt=s(Kc,"TR",{});var bn=o(rt);_o=s(bn,"TH",{}),o(_o).forEach(l),zr=d(bn),go=s(bn,"TH",{}),o(go).forEach(l),bn.forEach(l),Kc.forEach(l),qr=d(un),Pl=s(un,"TBODY",{});var em=o(Pl);pt=s(em,"TR",{});var yn=o(pt);Ll=s(yn,"TD",{});var tm=o(Ll);Kl=s(tm,"IMG",{src:!0}),tm.forEach(l),Hr=d(yn),ea=s(yn,"TD",{});var lm=o(ea);ta=s(lm,"IMG",{src:!0}),lm.forEach(l),yn.forEach(l),em.forEach(l),un.forEach(l),Eo=d(e),O=s(e,"H3",{class:!0});var Mn=o(O);he=s(Mn,"A",{id:!0,class:!0,href:!0});var am=o(he);la=s(am,"SPAN",{});var sm=o(la);f(dt.$$.fragment,sm),sm.forEach(l),am.forEach(l),Ar=d(Mn),aa=s(Mn,"SPAN",{});var om=o(aa);Pr=r(om,"Text-to-Image with Textual Inversion"),om.forEach(l),Mn.forEach(l),jo=d(e),fe=s(e,"P",{});var wn=o(fe);Lr=r(wn,"Here is an example of how you can load an SDXL OpenVINO model from "),ct=s(wn,"A",{href:!0,rel:!0});var nm=o(ct);Kr=r(nm,"stabilityai/stable-diffusion-xl-base-1.0"),nm.forEach(l),ep=r(wn," with pre-trained textual inversion embeddings and run inference using OpenVINO Runtime:"),wn.forEach(l),Io=d(e),qt=s(e,"P",{});var im=o(qt);tp=r(im,"First, you can run original pipeline without textual inversion"),im.forEach(l),Go=d(e),f(mt.$$.fragment,e),Vo=d(e),ue=s(e,"P",{});var vn=o(ue);lp=r(vn,"Then, you can load "),ht=s(vn,"A",{href:!0,rel:!0});var rm=o(ht);ap=r(rm,"charturnerv2"),rm.forEach(l),sp=r(vn," textual inversion embedding and run pipeline with same prompt again"),vn.forEach(l),Bo=d(e),f(ft.$$.fragment,e),ko=d(e),be=s(e,"TABLE",{});var Tn=o(be);sa=s(Tn,"THEAD",{});var pm=o(sa);ut=s(pm,"TR",{});var Jn=o(ut);Xo=s(Jn,"TH",{}),o(Xo).forEach(l),op=d(Jn),xo=s(Jn,"TH",{}),o(xo).forEach(l),Jn.forEach(l),pm.forEach(l),np=d(Tn),oa=s(Tn,"TBODY",{});var dm=o(oa);bt=s(dm,"TR",{});var Zn=o(bt);na=s(Zn,"TD",{});var cm=o(na);ia=s(cm,"IMG",{src:!0}),cm.forEach(l),ip=d(Zn),ra=s(Zn,"TD",{});var mm=o(ra);pa=s(mm,"IMG",{src:!0}),mm.forEach(l),Zn.forEach(l),dm.forEach(l),Tn.forEach(l),Ro=d(e),Q=s(e,"H3",{class:!0});var Un=o(Q);ye=s(Un,"A",{id:!0,class:!0,href:!0});var hm=o(ye);da=s(hm,"SPAN",{});var fm=o(da);f(yt.$$.fragment,fm),fm.forEach(l),hm.forEach(l),rp=d(Un),ca=s(Un,"SPAN",{});var um=o(ca);pp=r(um,"Image-to-Image"),um.forEach(l),Un.forEach(l),So=d(e),Me=s(e,"P",{});var Wn=o(Me);dp=r(Wn,"Here is an example of how you can load a PyTorch SDXL model, convert it to OpenVINO on-the-fly and run inference using OpenVINO Runtime for "),ma=s(Wn,"EM",{});var bm=o(ma);cp=r(bm,"image-to-image"),bm.forEach(l),mp=r(Wn,":"),Wn.forEach(l),No=d(e),f(Mt.$$.fragment,e),$o=d(e),D=s(e,"H3",{class:!0});var _n=o(D);we=s(_n,"A",{id:!0,class:!0,href:!0});var ym=o(we);ha=s(ym,"SPAN",{});var Mm=o(ha);f(wt.$$.fragment,Mm),Mm.forEach(l),ym.forEach(l),hp=d(_n),fa=s(_n,"SPAN",{});var wm=o(fa);fp=r(wm,"Refining the image output"),wm.forEach(l),_n.forEach(l),Fo=d(e),ve=s(e,"P",{});var gn=o(ve);up=r(gn,"The image can be refined by making use of a model like "),vt=s(gn,"A",{href:!0,rel:!0});var vm=o(vt);bp=r(vm,"stabilityai/stable-diffusion-xl-refiner-1.0"),vm.forEach(l),yp=r(gn,". In this case, you only have to output the latents from the base model."),gn.forEach(l),Co=d(e),f(Tt.$$.fragment,e),Oo=d(e),Y=s(e,"H2",{class:!0});var En=o(Y);Te=s(En,"A",{id:!0,class:!0,href:!0});var Tm=o(Te);ua=s(Tm,"SPAN",{});var Jm=o(ua);f(Jt.$$.fragment,Jm),Jm.forEach(l),Tm.forEach(l),Mp=d(En),ba=s(En,"SPAN",{});var Zm=o(ba);wp=r(Zm,"Supported tasks"),Zm.forEach(l),En.forEach(l),Qo=d(e),Ht=s(e,"P",{});var Um=o(Ht);vp=r(Um,"As shown in the table below, each task is associated with a class enabling to automatically load your model."),Um.forEach(l),Do=d(e),Je=s(e,"TABLE",{});var jn=o(Je);ya=s(jn,"THEAD",{});var Wm=o(ya);Zt=s(Wm,"TR",{});var In=o(Zt);Ma=s(In,"TH",{});var _m=o(Ma);Tp=r(_m,"Task"),_m.forEach(l),Jp=d(In),wa=s(In,"TH",{});var gm=o(wa);Zp=r(gm,"Auto Class"),gm.forEach(l),In.forEach(l),Wm.forEach(l),Up=d(jn),w=s(jn,"TBODY",{});var v=o(w);Ut=s(v,"TR",{});var Gn=o(Ut);va=s(Gn,"TD",{});var Em=o(va);Ta=s(Em,"CODE",{});var jm=o(Ta);Wp=r(jm,"text-classification"),jm.forEach(l),Em.forEach(l),_p=d(Gn),Ja=s(Gn,"TD",{});var Im=o(Ja);Za=s(Im,"CODE",{});var Gm=o(Za);gp=r(Gm,"OVModelForSequenceClassification"),Gm.forEach(l),Im.forEach(l),Gn.forEach(l),Ep=d(v),Wt=s(v,"TR",{});var Vn=o(Wt);Ua=s(Vn,"TD",{});var Vm=o(Ua);Wa=s(Vm,"CODE",{});var Bm=o(Wa);jp=r(Bm,"token-classification"),Bm.forEach(l),Vm.forEach(l),Ip=d(Vn),_a=s(Vn,"TD",{});var km=o(_a);ga=s(km,"CODE",{});var Xm=o(ga);Gp=r(Xm,"OVModelForTokenClassification"),Xm.forEach(l),km.forEach(l),Vn.forEach(l),Vp=d(v),_t=s(v,"TR",{});var Bn=o(_t);Ea=s(Bn,"TD",{});var xm=o(Ea);ja=s(xm,"CODE",{});var Rm=o(ja);Bp=r(Rm,"question-answering"),Rm.forEach(l),xm.forEach(l),kp=d(Bn),Ia=s(Bn,"TD",{});var Sm=o(Ia);Ga=s(Sm,"CODE",{});var Nm=o(Ga);Xp=r(Nm,"OVModelForQuestionAnswering"),Nm.forEach(l),Sm.forEach(l),Bn.forEach(l),xp=d(v),gt=s(v,"TR",{});var kn=o(gt);Va=s(kn,"TD",{});var $m=o(Va);Ba=s($m,"CODE",{});var Fm=o(Ba);Rp=r(Fm,"audio-classification"),Fm.forEach(l),$m.forEach(l),Sp=d(kn),ka=s(kn,"TD",{});var Cm=o(ka);Xa=s(Cm,"CODE",{});var Om=o(Xa);Np=r(Om,"OVModelForAudioClassification"),Om.forEach(l),Cm.forEach(l),kn.forEach(l),$p=d(v),Et=s(v,"TR",{});var Xn=o(Et);xa=s(Xn,"TD",{});var Qm=o(xa);Ra=s(Qm,"CODE",{});var Dm=o(Ra);Fp=r(Dm,"image-classification"),Dm.forEach(l),Qm.forEach(l),Cp=d(Xn),Sa=s(Xn,"TD",{});var Ym=o(Sa);Na=s(Ym,"CODE",{});var zm=o(Na);Op=r(zm,"OVModelForImageClassification"),zm.forEach(l),Ym.forEach(l),Xn.forEach(l),Qp=d(v),jt=s(v,"TR",{});var xn=o(jt);$a=s(xn,"TD",{});var qm=o($a);Fa=s(qm,"CODE",{});var Hm=o(Fa);Dp=r(Hm,"feature-extraction"),Hm.forEach(l),qm.forEach(l),Yp=d(xn),Ca=s(xn,"TD",{});var Am=o(Ca);Oa=s(Am,"CODE",{});var Pm=o(Oa);zp=r(Pm,"OVModelForFeatureExtraction"),Pm.forEach(l),Am.forEach(l),xn.forEach(l),qp=d(v),It=s(v,"TR",{});var Rn=o(It);Qa=s(Rn,"TD",{});var Lm=o(Qa);Da=s(Lm,"CODE",{});var Km=o(Da);Hp=r(Km,"fill-mask"),Km.forEach(l),Lm.forEach(l),Ap=d(Rn),Ya=s(Rn,"TD",{});var eh=o(Ya);za=s(eh,"CODE",{});var th=o(za);Pp=r(th,"OVModelForMaskedLM"),th.forEach(l),eh.forEach(l),Rn.forEach(l),Lp=d(v),Gt=s(v,"TR",{});var Sn=o(Gt);qa=s(Sn,"TD",{});var lh=o(qa);Ha=s(lh,"CODE",{});var ah=o(Ha);Kp=r(ah,"text-generation"),ah.forEach(l),lh.forEach(l),ed=d(Sn),Aa=s(Sn,"TD",{});var sh=o(Aa);Pa=s(sh,"CODE",{});var oh=o(Pa);td=r(oh,"OVModelForCausalLM"),oh.forEach(l),sh.forEach(l),Sn.forEach(l),ld=d(v),Vt=s(v,"TR",{});var Nn=o(Vt);La=s(Nn,"TD",{});var nh=o(La);Ka=s(nh,"CODE",{});var ih=o(Ka);ad=r(ih,"text2text-generation"),ih.forEach(l),nh.forEach(l),sd=d(Nn),es=s(Nn,"TD",{});var rh=o(es);ts=s(rh,"CODE",{});var ph=o(ts);od=r(ph,"OVModelForSeq2SeqLM"),ph.forEach(l),rh.forEach(l),Nn.forEach(l),nd=d(v),Bt=s(v,"TR",{});var $n=o(Bt);ls=s($n,"TD",{});var dh=o(ls);as=s(dh,"CODE",{});var ch=o(as);id=r(ch,"text-to-image"),ch.forEach(l),dh.forEach(l),rd=d($n),ss=s($n,"TD",{});var mh=o(ss);os=s(mh,"CODE",{});var hh=o(os);pd=r(hh,"OVStableDiffusionPipeline"),hh.forEach(l),mh.forEach(l),$n.forEach(l),dd=d(v),kt=s(v,"TR",{});var Fn=o(kt);ns=s(Fn,"TD",{});var fh=o(ns);is=s(fh,"CODE",{});var uh=o(is);cd=r(uh,"text-to-image"),uh.forEach(l),fh.forEach(l),md=d(Fn),rs=s(Fn,"TD",{});var bh=o(rs);ps=s(bh,"CODE",{});var yh=o(ps);hd=r(yh,"OVStableDiffusionXLPipeline"),yh.forEach(l),bh.forEach(l),Fn.forEach(l),fd=d(v),Xt=s(v,"TR",{});var Cn=o(Xt);ds=s(Cn,"TD",{});var Mh=o(ds);cs=s(Mh,"CODE",{});var wh=o(cs);ud=r(wh,"image-to-image"),wh.forEach(l),Mh.forEach(l),bd=d(Cn),ms=s(Cn,"TD",{});var vh=o(ms);hs=s(vh,"CODE",{});var Th=o(hs);yd=r(Th,"OVStableDiffusionImg2ImgPipeline"),Th.forEach(l),vh.forEach(l),Cn.forEach(l),Md=d(v),xt=s(v,"TR",{});var On=o(xt);fs=s(On,"TD",{});var Jh=o(fs);us=s(Jh,"CODE",{});var Zh=o(us);wd=r(Zh,"image-to-image"),Zh.forEach(l),Jh.forEach(l),vd=d(On),bs=s(On,"TD",{});var Uh=o(bs);ys=s(Uh,"CODE",{});var Wh=o(ys);Td=r(Wh,"OVStableDiffusionXLImg2ImgPipeline"),Wh.forEach(l),Uh.forEach(l),On.forEach(l),Jd=d(v),Rt=s(v,"TR",{});var Qn=o(Rt);Ms=s(Qn,"TD",{});var _h=o(Ms);ws=s(_h,"CODE",{});var gh=o(ws);Zd=r(gh,"inpaint"),gh.forEach(l),_h.forEach(l),Ud=d(Qn),vs=s(Qn,"TD",{});var Eh=o(vs);Ts=s(Eh,"CODE",{});var jh=o(Ts);Wd=r(jh,"OVStableDiffusionInpaintPipeline"),jh.forEach(l),Eh.forEach(l),Qn.forEach(l),v.forEach(l),jn.forEach(l),this.h()},h(){m(B,"name","hf:doc:metadata"),m(B,"content",JSON.stringify(Rh)),m(z,"id","optimum-inference-with-openvino"),m(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(z,"href","#optimum-inference-with-openvino"),m(k,"class","relative group"),m(ge,"href","https://huggingface.co/models?library=openvino&sort=downloads"),m(ge,"rel","nofollow"),m(H,"id","switching-from-transformers-to-optimum"),m(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(H,"href","#switching-from-transformers-to-optimum"),m(X,"class","relative group"),m(je,"href","https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html"),m(je,"rel","nofollow"),m($t,"href","reference_ov"),m(ke,"href","https://docs.openvino.ai/nightly/openvino_docs_install_guides_configurations_for_intel_gpu.html"),m(ke,"rel","nofollow"),m(Se,"href","https://huggingface.co/docs/huggingface_hub/main/en/guides/manage-cache"),m(Se,"rel","nofollow"),m(ee,"id","sequencetosequence-models"),m(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ee,"href","#sequencetosequence-models"),m(x,"class","relative group"),m(te,"id","stable-diffusion"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#stable-diffusion"),m(R,"class","relative group"),m(ae,"id","texttoimage"),m(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ae,"href","#texttoimage"),m(S,"class","relative group"),St(Vl.src,gd="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/stable_diffusion_v1_5_sail_boat_rembrandt.png")||m(Vl,"src",gd),m(qe,"class","flex justify-center"),m(oe,"id","texttoimage-with-textual-inversion"),m(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(oe,"href","#texttoimage-with-textual-inversion"),m(N,"class","relative group"),m(Pe,"href","https://huggingface.co/sd-concepts-library/cat-toy"),m(Pe,"rel","nofollow"),St(Sl.src,Ed="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/textual_inversion/stable_diffusion_v1_5_without_textual_inversion.png")||m(Sl,"src",Ed),St($l.src,jd="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/textual_inversion/stable_diffusion_v1_5_with_textual_inversion.png")||m($l,"src",jd),m(re,"id","imagetoimage"),m(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(re,"href","#imagetoimage"),m($,"class","relative group"),m(pe,"id","stable-diffusion-xl"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#stable-diffusion-xl"),m(F,"class","relative group"),m(de,"id","texttoimage"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#texttoimage"),m(C,"class","relative group"),m(nt,"href","https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"),m(nt,"rel","nofollow"),St(Kl.src,Id="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/sd_xl/train_station_friedrich.png")||m(Kl,"src",Id),St(ta.src,Gd="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/sd_xl/train_station_friedrich_2.png")||m(ta,"src",Gd),m(he,"id","texttoimage-with-textual-inversion"),m(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(he,"href","#texttoimage-with-textual-inversion"),m(O,"class","relative group"),m(ct,"href","https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"),m(ct,"rel","nofollow"),m(ht,"href","https://civitai.com/models/3036/charturner-character-turnaround-helper-for-15-and-21"),m(ht,"rel","nofollow"),St(ia.src,Vd="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/textual_inversion/sdxl_without_textual_inversion.png")||m(ia,"src",Vd),St(pa.src,Bd="https://huggingface.co/datasets/optimum/documentation-images/resolve/main/intel/openvino/textual_inversion/sdxl_with_textual_inversion.png")||m(pa,"src",Bd),m(ye,"id","imagetoimage"),m(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ye,"href","#imagetoimage"),m(Q,"class","relative group"),m(we,"id","refining-the-image-output"),m(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(we,"href","#refining-the-image-output"),m(D,"class","relative group"),m(vt,"href","https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0"),m(vt,"rel","nofollow"),m(Te,"id","supported-tasks"),m(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Te,"href","#supported-tasks"),m(Y,"class","relative group")},m(e,n){t(document.head,B),c(e,Js,n),c(e,k,n),t(k,z),t(z,Kt),u(_e,Kt,null),t(k,Dn),t(k,el),t(el,Yn),c(e,Zs,n),c(e,q,n),t(q,zn),t(q,ge),t(ge,qn),t(q,Hn),c(e,Us,n),c(e,X,n),t(X,H),t(H,tl),u(Ee,tl,null),t(X,An),t(X,ll),t(ll,Pn),c(e,Ws,n),c(e,J,n),t(J,Ln),t(J,je),t(je,Kn),t(J,ei),t(J,al),t(al,ti),t(J,li),t(J,sl),t(sl,ai),t(J,si),t(J,ol),t(ol,oi),t(J,ni),c(e,_s,n),c(e,Nt,n),t(Nt,ii),c(e,gs,n),u(Ie,e,n),c(e,Es,n),c(e,A,n),t(A,ri),t(A,$t),t($t,pi),t(A,di),c(e,js,n),c(e,P,n),t(P,ci),t(P,nl),t(nl,mi),t(P,hi),c(e,Is,n),u(Ge,e,n),c(e,Gs,n),c(e,L,n),t(L,fi),t(L,il),t(il,ui),t(L,bi),c(e,Vs,n),u(Ve,e,n),c(e,Bs,n),c(e,K,n),t(K,yi),t(K,rl),t(rl,Mi),t(K,wi),c(e,ks,n),u(Be,e,n),c(e,Xs,n),c(e,E,n),t(E,vi),t(E,pl),t(pl,Ti),t(E,Ji),t(E,ke),t(ke,Zi),t(E,Ui),c(e,xs,n),u(Xe,e,n),c(e,Rs,n),c(e,W,n),t(W,Wi),t(W,dl),t(dl,_i),t(W,gi),t(W,cl),t(cl,Ei),t(W,ji),t(W,ml),t(ml,Ii),t(W,Gi),c(e,Ss,n),u(xe,e,n),c(e,Ns,n),c(e,j,n),t(j,Vi),t(j,hl),t(hl,Bi),t(j,ki),t(j,fl),t(fl,Xi),t(j,xi),c(e,$s,n),u(Re,e,n),c(e,Fs,n),c(e,Z,n),t(Z,Ri),t(Z,ul),t(ul,Si),t(Z,Ni),t(Z,Se),t(Se,$i),t(Z,Fi),t(Z,bl),t(bl,Ci),t(Z,Oi),t(Z,yl),t(yl,Qi),t(Z,Di),c(e,Cs,n),u(Ne,e,n),c(e,Os,n),c(e,x,n),t(x,ee),t(ee,Ml),u($e,Ml,null),t(x,Yi),t(x,wl),t(wl,zi),c(e,Qs,n),c(e,I,n),t(I,qi),t(I,vl),t(vl,Hi),t(I,Ai),t(I,Tl),t(Tl,Pi),t(I,Li),c(e,Ds,n),c(e,Ft,n),t(Ft,Ki),c(e,Ys,n),u(Fe,e,n),c(e,zs,n),c(e,R,n),t(R,te),t(te,Jl),u(Ce,Jl,null),t(R,er),t(R,Zl),t(Zl,tr),c(e,qs,n),c(e,Ct,n),t(Ct,lr),c(e,Hs,n),c(e,_,n),t(_,Ul),t(Ul,ar),t(_,sr),t(_,Wl),t(Wl,or),t(_,nr),t(_,_l),t(_l,ir),t(_,rr),t(_,gl),t(gl,pr),c(e,As,n),c(e,Ot,n),t(Ot,dr),c(e,Ps,n),c(e,le,n),t(le,cr),t(le,El),t(El,mr),t(le,hr),c(e,Ls,n),u(Oe,e,n),c(e,Ks,n),c(e,S,n),t(S,ae),t(ae,jl),u(Qe,jl,null),t(S,fr),t(S,Il),t(Il,ur),c(e,eo,n),u(De,e,n),c(e,to,n),c(e,se,n),t(se,br),t(se,Gl),t(Gl,yr),t(se,Mr),c(e,lo,n),u(Ye,e,n),c(e,ao,n),c(e,Qt,n),t(Qt,wr),c(e,so,n),u(ze,e,n),c(e,oo,n),c(e,Dt,n),t(Dt,vr),c(e,no,n),c(e,qe,n),t(qe,Vl),c(e,io,n),c(e,N,n),t(N,oe),t(oe,Bl),u(He,Bl,null),t(N,Tr),t(N,kl),t(kl,Jr),c(e,ro,n),c(e,Yt,n),t(Yt,Zr),c(e,po,n),u(Ae,e,n),c(e,co,n),c(e,ne,n),t(ne,Ur),t(ne,Pe),t(Pe,Wr),t(ne,_r),c(e,mo,n),u(Le,e,n),c(e,ho,n),c(e,zt,n),t(zt,gr),c(e,fo,n),c(e,ie,n),t(ie,Xl),t(Xl,Ke),t(Ke,uo),t(Ke,Er),t(Ke,bo),t(ie,jr),t(ie,xl),t(xl,et),t(et,Rl),t(Rl,Sl),t(et,Ir),t(et,Nl),t(Nl,$l),c(e,yo,n),c(e,$,n),t($,re),t(re,Fl),u(tt,Fl,null),t($,Gr),t($,Cl),t(Cl,Vr),c(e,Mo,n),u(lt,e,n),c(e,wo,n),c(e,F,n),t(F,pe),t(pe,Ol),u(at,Ol,null),t(F,Br),t(F,Ql),t(Ql,kr),c(e,vo,n),c(e,g,n),t(g,Xr),t(g,Dl),t(Dl,xr),t(g,Rr),t(g,Yl),t(Yl,Sr),t(g,Nr),t(g,zl),t(zl,$r),t(g,Fr),c(e,To,n),u(st,e,n),c(e,Jo,n),c(e,C,n),t(C,de),t(de,ql),u(ot,ql,null),t(C,Cr),t(C,Hl),t(Hl,Or),c(e,Zo,n),c(e,ce,n),t(ce,Qr),t(ce,nt),t(nt,Dr),t(ce,Yr),c(e,Uo,n),u(it,e,n),c(e,Wo,n),c(e,me,n),t(me,Al),t(Al,rt),t(rt,_o),t(rt,zr),t(rt,go),t(me,qr),t(me,Pl),t(Pl,pt),t(pt,Ll),t(Ll,Kl),t(pt,Hr),t(pt,ea),t(ea,ta),c(e,Eo,n),c(e,O,n),t(O,he),t(he,la),u(dt,la,null),t(O,Ar),t(O,aa),t(aa,Pr),c(e,jo,n),c(e,fe,n),t(fe,Lr),t(fe,ct),t(ct,Kr),t(fe,ep),c(e,Io,n),c(e,qt,n),t(qt,tp),c(e,Go,n),u(mt,e,n),c(e,Vo,n),c(e,ue,n),t(ue,lp),t(ue,ht),t(ht,ap),t(ue,sp),c(e,Bo,n),u(ft,e,n),c(e,ko,n),c(e,be,n),t(be,sa),t(sa,ut),t(ut,Xo),t(ut,op),t(ut,xo),t(be,np),t(be,oa),t(oa,bt),t(bt,na),t(na,ia),t(bt,ip),t(bt,ra),t(ra,pa),c(e,Ro,n),c(e,Q,n),t(Q,ye),t(ye,da),u(yt,da,null),t(Q,rp),t(Q,ca),t(ca,pp),c(e,So,n),c(e,Me,n),t(Me,dp),t(Me,ma),t(ma,cp),t(Me,mp),c(e,No,n),u(Mt,e,n),c(e,$o,n),c(e,D,n),t(D,we),t(we,ha),u(wt,ha,null),t(D,hp),t(D,fa),t(fa,fp),c(e,Fo,n),c(e,ve,n),t(ve,up),t(ve,vt),t(vt,bp),t(ve,yp),c(e,Co,n),u(Tt,e,n),c(e,Oo,n),c(e,Y,n),t(Y,Te),t(Te,ua),u(Jt,ua,null),t(Y,Mp),t(Y,ba),t(ba,wp),c(e,Qo,n),c(e,Ht,n),t(Ht,vp),c(e,Do,n),c(e,Je,n),t(Je,ya),t(ya,Zt),t(Zt,Ma),t(Ma,Tp),t(Zt,Jp),t(Zt,wa),t(wa,Zp),t(Je,Up),t(Je,w),t(w,Ut),t(Ut,va),t(va,Ta),t(Ta,Wp),t(Ut,_p),t(Ut,Ja),t(Ja,Za),t(Za,gp),t(w,Ep),t(w,Wt),t(Wt,Ua),t(Ua,Wa),t(Wa,jp),t(Wt,Ip),t(Wt,_a),t(_a,ga),t(ga,Gp),t(w,Vp),t(w,_t),t(_t,Ea),t(Ea,ja),t(ja,Bp),t(_t,kp),t(_t,Ia),t(Ia,Ga),t(Ga,Xp),t(w,xp),t(w,gt),t(gt,Va),t(Va,Ba),t(Ba,Rp),t(gt,Sp),t(gt,ka),t(ka,Xa),t(Xa,Np),t(w,$p),t(w,Et),t(Et,xa),t(xa,Ra),t(Ra,Fp),t(Et,Cp),t(Et,Sa),t(Sa,Na),t(Na,Op),t(w,Qp),t(w,jt),t(jt,$a),t($a,Fa),t(Fa,Dp),t(jt,Yp),t(jt,Ca),t(Ca,Oa),t(Oa,zp),t(w,qp),t(w,It),t(It,Qa),t(Qa,Da),t(Da,Hp),t(It,Ap),t(It,Ya),t(Ya,za),t(za,Pp),t(w,Lp),t(w,Gt),t(Gt,qa),t(qa,Ha),t(Ha,Kp),t(Gt,ed),t(Gt,Aa),t(Aa,Pa),t(Pa,td),t(w,ld),t(w,Vt),t(Vt,La),t(La,Ka),t(Ka,ad),t(Vt,sd),t(Vt,es),t(es,ts),t(ts,od),t(w,nd),t(w,Bt),t(Bt,ls),t(ls,as),t(as,id),t(Bt,rd),t(Bt,ss),t(ss,os),t(os,pd),t(w,dd),t(w,kt),t(kt,ns),t(ns,is),t(is,cd),t(kt,md),t(kt,rs),t(rs,ps),t(ps,hd),t(w,fd),t(w,Xt),t(Xt,ds),t(ds,cs),t(cs,ud),t(Xt,bd),t(Xt,ms),t(ms,hs),t(hs,yd),t(w,Md),t(w,xt),t(xt,fs),t(fs,us),t(us,wd),t(xt,vd),t(xt,bs),t(bs,ys),t(ys,Td),t(w,Jd),t(w,Rt),t(Rt,Ms),t(Ms,ws),t(ws,Zd),t(Rt,Ud),t(Rt,vs),t(vs,Ts),t(Ts,Wd),Yo=!0},p:kh,i(e){Yo||(b(_e.$$.fragment,e),b(Ee.$$.fragment,e),b(Ie.$$.fragment,e),b(Ge.$$.fragment,e),b(Ve.$$.fragment,e),b(Be.$$.fragment,e),b(Xe.$$.fragment,e),b(xe.$$.fragment,e),b(Re.$$.fragment,e),b(Ne.$$.fragment,e),b($e.$$.fragment,e),b(Fe.$$.fragment,e),b(Ce.$$.fragment,e),b(Oe.$$.fragment,e),b(Qe.$$.fragment,e),b(De.$$.fragment,e),b(Ye.$$.fragment,e),b(ze.$$.fragment,e),b(He.$$.fragment,e),b(Ae.$$.fragment,e),b(Le.$$.fragment,e),b(tt.$$.fragment,e),b(lt.$$.fragment,e),b(at.$$.fragment,e),b(st.$$.fragment,e),b(ot.$$.fragment,e),b(it.$$.fragment,e),b(dt.$$.fragment,e),b(mt.$$.fragment,e),b(ft.$$.fragment,e),b(yt.$$.fragment,e),b(Mt.$$.fragment,e),b(wt.$$.fragment,e),b(Tt.$$.fragment,e),b(Jt.$$.fragment,e),Yo=!0)},o(e){y(_e.$$.fragment,e),y(Ee.$$.fragment,e),y(Ie.$$.fragment,e),y(Ge.$$.fragment,e),y(Ve.$$.fragment,e),y(Be.$$.fragment,e),y(Xe.$$.fragment,e),y(xe.$$.fragment,e),y(Re.$$.fragment,e),y(Ne.$$.fragment,e),y($e.$$.fragment,e),y(Fe.$$.fragment,e),y(Ce.$$.fragment,e),y(Oe.$$.fragment,e),y(Qe.$$.fragment,e),y(De.$$.fragment,e),y(Ye.$$.fragment,e),y(ze.$$.fragment,e),y(He.$$.fragment,e),y(Ae.$$.fragment,e),y(Le.$$.fragment,e),y(tt.$$.fragment,e),y(lt.$$.fragment,e),y(at.$$.fragment,e),y(st.$$.fragment,e),y(ot.$$.fragment,e),y(it.$$.fragment,e),y(dt.$$.fragment,e),y(mt.$$.fragment,e),y(ft.$$.fragment,e),y(yt.$$.fragment,e),y(Mt.$$.fragment,e),y(wt.$$.fragment,e),y(Tt.$$.fragment,e),y(Jt.$$.fragment,e),Yo=!1},d(e){l(B),e&&l(Js),e&&l(k),M(_e),e&&l(Zs),e&&l(q),e&&l(Us),e&&l(X),M(Ee),e&&l(Ws),e&&l(J),e&&l(_s),e&&l(Nt),e&&l(gs),M(Ie,e),e&&l(Es),e&&l(A),e&&l(js),e&&l(P),e&&l(Is),M(Ge,e),e&&l(Gs),e&&l(L),e&&l(Vs),M(Ve,e),e&&l(Bs),e&&l(K),e&&l(ks),M(Be,e),e&&l(Xs),e&&l(E),e&&l(xs),M(Xe,e),e&&l(Rs),e&&l(W),e&&l(Ss),M(xe,e),e&&l(Ns),e&&l(j),e&&l($s),M(Re,e),e&&l(Fs),e&&l(Z),e&&l(Cs),M(Ne,e),e&&l(Os),e&&l(x),M($e),e&&l(Qs),e&&l(I),e&&l(Ds),e&&l(Ft),e&&l(Ys),M(Fe,e),e&&l(zs),e&&l(R),M(Ce),e&&l(qs),e&&l(Ct),e&&l(Hs),e&&l(_),e&&l(As),e&&l(Ot),e&&l(Ps),e&&l(le),e&&l(Ls),M(Oe,e),e&&l(Ks),e&&l(S),M(Qe),e&&l(eo),M(De,e),e&&l(to),e&&l(se),e&&l(lo),M(Ye,e),e&&l(ao),e&&l(Qt),e&&l(so),M(ze,e),e&&l(oo),e&&l(Dt),e&&l(no),e&&l(qe),e&&l(io),e&&l(N),M(He),e&&l(ro),e&&l(Yt),e&&l(po),M(Ae,e),e&&l(co),e&&l(ne),e&&l(mo),M(Le,e),e&&l(ho),e&&l(zt),e&&l(fo),e&&l(ie),e&&l(yo),e&&l($),M(tt),e&&l(Mo),M(lt,e),e&&l(wo),e&&l(F),M(at),e&&l(vo),e&&l(g),e&&l(To),M(st,e),e&&l(Jo),e&&l(C),M(ot),e&&l(Zo),e&&l(ce),e&&l(Uo),M(it,e),e&&l(Wo),e&&l(me),e&&l(Eo),e&&l(O),M(dt),e&&l(jo),e&&l(fe),e&&l(Io),e&&l(qt),e&&l(Go),M(mt,e),e&&l(Vo),e&&l(ue),e&&l(Bo),M(ft,e),e&&l(ko),e&&l(be),e&&l(Ro),e&&l(Q),M(yt),e&&l(So),e&&l(Me),e&&l(No),M(Mt,e),e&&l($o),e&&l(D),M(wt),e&&l(Fo),e&&l(ve),e&&l(Co),M(Tt,e),e&&l(Oo),e&&l(Y),M(Jt),e&&l(Qo),e&&l(Ht),e&&l(Do),e&&l(Je)}}}const Rh={local:"optimum-inference-with-openvino",sections:[{local:"switching-from-transformers-to-optimum",title:"Switching from Transformers to Optimum"},{local:"sequencetosequence-models",title:"Sequence-to-sequence models"},{local:"stable-diffusion",sections:[{local:"texttoimage",title:"Text-to-Image"},{local:"texttoimage-with-textual-inversion",title:"Text-to-Image with Textual Inversion"},{local:"imagetoimage",title:"Image-to-Image"}],title:"Stable Diffusion"},{local:"stable-diffusion-xl",sections:[{local:"texttoimage",title:"Text-to-Image"},{local:"texttoimage-with-textual-inversion",title:"Text-to-Image with Textual Inversion"},{local:"imagetoimage",title:"Image-to-Image"},{local:"refining-the-image-output",title:"Refining the image output"}],title:"Stable Diffusion XL"},{local:"supported-tasks",title:"Supported tasks"}],title:"Optimum Inference with OpenVINO"};function Sh(_d){return Xh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ch extends Ih{constructor(B){super();Gh(this,B,Sh,xh,Vh,{})}}export{Ch as default,Rh as metadata};
