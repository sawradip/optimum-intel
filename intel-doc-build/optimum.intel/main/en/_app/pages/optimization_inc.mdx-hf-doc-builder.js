import{S as Ti,i as Zi,s as Wi,e as l,k as d,w as m,t as p,M as vi,c as n,d as t,m as u,a as i,x as h,h as c,b as s,G as a,g as r,y,L as ji,q as M,o as f,B as b,v as gi}from"../chunks/vendor-hf-doc-builder.js";import{I as U}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../chunks/CodeBlock-hf-doc-builder.js";function Xi(cn){let v,It,j,N,De,oe,Na,Ke,ka,Rt,$e,Va,_t,g,k,Le,se,Qa,Oe,Ya,Ft,V,qa,re,et,$a,Sa,Et,X,Q,tt,pe,Aa,at,Ha,Ct,Se,xa,Bt,ce,Nt,Ae,Pa,kt,de,Vt,z,Y,lt,ue,Da,nt,Ka,Qt,He,La,Yt,me,qt,G,q,it,he,Oa,ot,el,$t,$,tl,ye,al,ll,St,Me,At,Z,nl,fe,il,ol,be,sl,rl,Ht,I,S,st,we,pl,rt,cl,xt,w,dl,pt,ul,ml,ct,hl,yl,dt,Ml,fl,Pt,Je,Dt,Ue,Kt,W,bl,Te,wl,Jl,Ze,Ul,Tl,Lt,R,A,ut,We,Zl,mt,Wl,Ot,J,vl,ve,ht,jl,gl,yt,Xl,zl,je,Mt,Gl,Il,ea,_,H,ft,ge,Rl,bt,_l,ta,x,Fl,wt,El,Cl,aa,Xe,la,F,P,Jt,ze,Bl,Ut,Nl,na,D,kl,Ge,Vl,Ql,ia,Ie,oa,E,K,Tt,Re,Yl,Zt,ql,sa,_e,$l,Fe,Sl,ra,Ee,pa,C,L,Wt,Ce,Al,vt,Hl,ca,O,xl,Be,jt,Pl,Dl,da,Ne,ua,ee,Kl,ke,gt,Ll,Ol,ma,B,te,Xt,Ve,en,zt,tn,ha,ae,an,Qe,ln,nn,ya,Ye,Ma,le,on,qe,Gt,sn,rn,fa;return oe=new U({}),se=new U({}),pe=new U({}),ce=new T({props:{code:"b3B0aW11bS1jbGklMjBpbmMlMjBxdWFudGl6ZSUyMC0tbW9kZWwlMjBkaXN0aWxiZXJ0LWJhc2UtY2FzZWQtZGlzdGlsbGVkLXNxdWFkJTIwLS1vdXRwdXQlMjBxdWFudGl6ZWRfZGlzdGlsYmVydA==",highlighted:"optimum-cli inc quantize --model distilbert-base-cased-distilled-squad --output quantized_distilbert"}}),de=new T({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> evaluate
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> INCQuantizer
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForQuestionAnswering, AutoTokenizer, pipeline
<span class="hljs-keyword">from</span> neural_compressor.config <span class="hljs-keyword">import</span> AccuracyCriterion, TuningCriterion, PostTrainingQuantConfig

model_name = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
model = AutoModelForQuestionAnswering.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
eval_dataset = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">64</span>))
task_evaluator = evaluate.evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
qa_pipeline = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)

<span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_fn</span>(<span class="hljs-params">model</span>):
    qa_pipeline.model = model
    metrics = task_evaluator.compute(model_or_pipeline=qa_pipeline, data=eval_dataset, metric=<span class="hljs-string">&quot;squad&quot;</span>)
    <span class="hljs-keyword">return</span> metrics[<span class="hljs-string">&quot;f1&quot;</span>]

<span class="hljs-comment"># Set the accepted accuracy loss to 5%</span>
accuracy_criterion = AccuracyCriterion(tolerable_loss=<span class="hljs-number">0.05</span>)
<span class="hljs-comment"># Set the maximum number of trials to 10</span>
tuning_criterion = TuningCriterion(max_trials=<span class="hljs-number">10</span>)
quantization_config = PostTrainingQuantConfig(
    approach=<span class="hljs-string">&quot;dynamic&quot;</span>, accuracy_criterion=accuracy_criterion, tuning_criterion=tuning_criterion
)
quantizer = INCQuantizer.from_pretrained(model, eval_fn=eval_fn)
quantizer.quantize(quantization_config=quantization_config, save_directory=<span class="hljs-string">&quot;dynamic_quantization&quot;</span>)`}}),ue=new U({}),me=new T({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-keyword">from</span> neural_compressor.config <span class="hljs-keyword">import</span> PostTrainingQuantConfig
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> INCQuantizer

model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
<span class="hljs-comment"># The directory where the quantized model will be saved</span>
save_dir = <span class="hljs-string">&quot;static_quantization&quot;</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples, tokenizer</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;sentence&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Load the quantization configuration detailing the quantization we wish to apply</span>
quantization_config = PostTrainingQuantConfig(approach=<span class="hljs-string">&quot;static&quot;</span>)
quantizer = INCQuantizer.from_pretrained(model)
<span class="hljs-comment"># Generate the calibration dataset needed for the calibration step</span>
calibration_dataset = quantizer.get_calibration_dataset(
    <span class="hljs-string">&quot;glue&quot;</span>,
    dataset_config_name=<span class="hljs-string">&quot;sst2&quot;</span>,
    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),
    num_samples=<span class="hljs-number">100</span>,
    dataset_split=<span class="hljs-string">&quot;train&quot;</span>,
)
quantizer = INCQuantizer.from_pretrained(model)
<span class="hljs-comment"># Apply static quantization and save the resulting model</span>
quantizer.quantize(
    quantization_config=quantization_config,
    calibration_dataset=calibration_dataset,
    save_directory=save_dir,
)`}}),he=new U({}),Me=new T({props:{code:"LSUyMHF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBQb3N0VHJhaW5pbmdRdWFudENvbmZpZyhhcHByb2FjaCUzRCUyMnN0YXRpYyUyMiklMEElMkIlMjByZWNpcGVzJTNEJTdCJTIyc21vb3RoX3F1YW50JTIyJTNBJTIwVHJ1ZSUyQyUyMCUyMCUyMnNtb290aF9xdWFudF9hcmdzJTIyJTNBJTIwJTdCJTIyYWxwaGElMjIlM0ElMjAwLjUlMkMlMjAlMjJmb2xkaW5nJTIyJTNBJTIwVHJ1ZSU3RCU3RCUwQSUyQiUyMHF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBQb3N0VHJhaW5pbmdRdWFudENvbmZpZyhhcHByb2FjaCUzRCUyMnN0YXRpYyUyMiUyQyUyMGJhY2tlbmQlM0QlMjJpcGV4JTIyJTJDJTIwcmVjaXBlcyUzRHJlY2lwZXMp",highlighted:`<span class="hljs-deletion">- quantization_config = PostTrainingQuantConfig(approach=&quot;static&quot;)</span>
<span class="hljs-addition">+ recipes={&quot;smooth_quant&quot;: True,  &quot;smooth_quant_args&quot;: {&quot;alpha&quot;: 0.5, &quot;folding&quot;: True}}</span>
<span class="hljs-addition">+ quantization_config = PostTrainingQuantConfig(approach=&quot;static&quot;, backend=&quot;ipex&quot;, recipes=recipes)</span>`}}),we=new U({}),Je=new T({props:{code:"LSUyMHF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBQb3N0VHJhaW5pbmdRdWFudENvbmZpZyhhcHByb2FjaCUzRCUyMnN0YXRpYyUyMiklMEElMkIlMjBxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwUG9zdFRyYWluaW5nUXVhbnRDb25maWcoYXBwcm9hY2glM0QlMjJzdGF0aWMlMjIlMkMlMjBxdWFudF9sZXZlbCUzRDEp",highlighted:`<span class="hljs-deletion">- quantization_config = PostTrainingQuantConfig(approach=&quot;static&quot;)</span>
<span class="hljs-addition">+ quantization_config = PostTrainingQuantConfig(approach=&quot;static&quot;, quant_level=1)</span>`}}),Ue=new T({props:{code:"bXBpcnVuJTIwLW5wJTIwJTNDbnVtYmVyX29mX3Byb2Nlc3NlcyUzRSUyMCUzQ1JVTl9DTUQlM0U=",highlighted:"mpirun -np &lt;number_of_processes&gt; &lt;RUN_CMD&gt;"}}),We=new U({}),ge=new U({}),Xe=new T({props:{code:"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",highlighted:`import evaluate
import numpy as np
from datasets import load_dataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, default_data_collator
<span class="hljs-deletion">- from transformers import Trainer</span>
<span class="hljs-addition">+ from optimum.intel import INCModelForSequenceClassification, INCTrainer</span>
<span class="hljs-addition">+ from neural_compressor import QuantizationAwareTrainingConfig</span>

model_id = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
dataset = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)
dataset = dataset.map(lambda examples: tokenizer(examples[&quot;sentence&quot;], padding=True, max_length=128), batched=True)
metric = evaluate.load(&quot;glue&quot;, &quot;sst2&quot;)
compute_metrics = lambda p: metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)

# The directory where the quantized model will be saved
save_dir = &quot;quantized_model&quot;

# The configuration detailing the quantization process
<span class="hljs-addition">+quantization_config = QuantizationAwareTrainingConfig()</span>

<span class="hljs-deletion">- trainer = Trainer(</span>
<span class="hljs-addition">+ trainer = INCTrainer(</span>
    model=model,
<span class="hljs-addition">+   quantization_config=quantization_config,</span>
    args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=False),
    train_dataset=dataset[&quot;train&quot;].select(range(300)),
    eval_dataset=dataset[&quot;validation&quot;],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

train_result = trainer.train()
metrics = trainer.evaluate()
trainer.save_model()

<span class="hljs-deletion">- model = AutoModelForSequenceClassification.from_pretrained(save_dir)</span>
<span class="hljs-addition">+ model = INCModelForSequenceClassification.from_pretrained(save_dir)</span>`}}),ze=new U({}),Ie=new T({props:{code:"LSUyMGZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBUcmFpbmVyJTBBJTJCJTIwZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBJTkNUcmFpbmVyJTBBJTJCJTIwZnJvbSUyMG5ldXJhbF9jb21wcmVzc29yJTIwaW1wb3J0JTIwV2VpZ2h0UHJ1bmluZ0NvbmZpZyUwQSUwQSUyMyUyMFRoZSUyMGNvbmZpZ3VyYXRpb24lMjBkZXRhaWxpbmclMjB0aGUlMjBwcnVuaW5nJTIwcHJvY2VzcyUwQSUyQiUyMHBydW5pbmdfY29uZmlnJTIwJTNEJTIwV2VpZ2h0UHJ1bmluZ0NvbmZpZyglMEElMkIlMjAlMjAlMjAlMjBwcnVuaW5nX3R5cGUlM0QlMjJtYWduaXR1ZGUlMjIlMkMlMEElMkIlMjAlMjAlMjAlMjBzdGFydF9zdGVwJTNEMCUyQyUwQSUyQiUyMCUyMCUyMCUyMGVuZF9zdGVwJTNEMTUlMkMlMEElMkIlMjAlMjAlMjAlMjB0YXJnZXRfc3BhcnNpdHklM0QwLjIlMkMlMEElMkIlMjAlMjAlMjAlMjBwcnVuaW5nX3Njb3BlJTNEJTIybG9jYWwlMjIlMkMlMEElMkIlMjApJTBBJTBBLSUyMHRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyQiUyMHRyYWluZXIlMjAlM0QlMjBJTkNUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMkIlMjAlMjAlMjBwcnVuaW5nX2NvbmZpZyUzRHBydW5pbmdfY29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRFRyYWluaW5nQXJndW1lbnRzKHNhdmVfZGlyJTJDJTIwbnVtX3RyYWluX2Vwb2NocyUzRDEuMCUyQyUyMGRvX3RyYWluJTNEVHJ1ZSUyQyUyMGRvX2V2YWwlM0RGYWxzZSklMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRyYWluJTIyJTVELnNlbGVjdChyYW5nZSgzMDApKSUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRGRhdGFzZXQlNUIlMjJ2YWxpZGF0aW9uJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBJTIwJTIwJTIwJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyJTJDJTBBJTIwJTIwJTIwJTIwZGF0YV9jb2xsYXRvciUzRGRlZmF1bHRfZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0cmFpbl9yZXN1bHQlMjAlM0QlMjB0cmFpbmVyLnRyYWluKCklMEFtZXRyaWNzJTIwJTNEJTIwdHJhaW5lci5ldmFsdWF0ZSgpJTBBdHJhaW5lci5zYXZlX21vZGVsKCklMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKHNhdmVfZGlyKQ==",highlighted:`<span class="hljs-deletion">- from transformers import Trainer</span>
<span class="hljs-addition">+ from optimum.intel import INCTrainer</span>
<span class="hljs-addition">+ from neural_compressor import WeightPruningConfig</span>

# The configuration detailing the pruning process
<span class="hljs-addition">+ pruning_config = WeightPruningConfig(</span>
<span class="hljs-addition">+    pruning_type=&quot;magnitude&quot;,</span>
<span class="hljs-addition">+    start_step=0,</span>
<span class="hljs-addition">+    end_step=15,</span>
<span class="hljs-addition">+    target_sparsity=0.2,</span>
<span class="hljs-addition">+    pruning_scope=&quot;local&quot;,</span>
<span class="hljs-addition">+ )</span>

<span class="hljs-deletion">- trainer = Trainer(</span>
<span class="hljs-addition">+ trainer = INCTrainer(</span>
    model=model,
<span class="hljs-addition">+   pruning_config=pruning_config,</span>
    args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=False),
    train_dataset=dataset[&quot;train&quot;].select(range(300)),
    eval_dataset=dataset[&quot;validation&quot;],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

train_result = trainer.train()
metrics = trainer.evaluate()
trainer.save_model()

model = AutoModelForSequenceClassification.from_pretrained(save_dir)`}}),Re=new U({}),Ee=new T({props:{code:"LSUyMGZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBUcmFpbmVyJTBBJTJCJTIwZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBJTkNUcmFpbmVyJTBBJTJCJTIwZnJvbSUyMG5ldXJhbF9jb21wcmVzc29yJTIwaW1wb3J0JTIwRGlzdGlsbGF0aW9uQ29uZmlnJTBBJTBBJTJCJTIwdGVhY2hlcl9tb2RlbF9pZCUyMCUzRCUyMCUyMnRleHRhdHRhY2slMkZiZXJ0LWJhc2UtdW5jYXNlZC1TU1QtMiUyMiUwQSUyQiUyMHRlYWNoZXJfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCh0ZWFjaGVyX21vZGVsX2lkKSUwQSUyQiUyMGRpc3RpbGxhdGlvbl9jb25maWclMjAlM0QlMjBEaXN0aWxsYXRpb25Db25maWcodGVhY2hlcl9tb2RlbCUzRHRlYWNoZXJfbW9kZWwpJTBBJTBBLSUyMHRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyQiUyMHRyYWluZXIlMjAlM0QlMjBJTkNUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMkIlMjAlMjAlMjBkaXN0aWxsYXRpb25fY29uZmlnJTNEZGlzdGlsbGF0aW9uX2NvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0RUcmFpbmluZ0FyZ3VtZW50cyhzYXZlX2RpciUyQyUyMG51bV90cmFpbl9lcG9jaHMlM0QxLjAlMkMlMjBkb190cmFpbiUzRFRydWUlMkMlMjBkb19ldmFsJTNERmFsc2UpJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRGRhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RC5zZWxlY3QocmFuZ2UoMzAwKSklMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RkYXRhc2V0JTVCJTIydmFsaWRhdGlvbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGNvbXB1dGVfbWV0cmljcyUzRGNvbXB1dGVfbWV0cmljcyUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkZWZhdWx0X2RhdGFfY29sbGF0b3IlMkMlMEEpJTBBJTBBdHJhaW5fcmVzdWx0JTIwJTNEJTIwdHJhaW5lci50cmFpbigpJTBBbWV0cmljcyUyMCUzRCUyMHRyYWluZXIuZXZhbHVhdGUoKSUwQXRyYWluZXIuc2F2ZV9tb2RlbCgpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChzYXZlX2Rpcik=",highlighted:`<span class="hljs-deletion">- from transformers import Trainer</span>
<span class="hljs-addition">+ from optimum.intel import INCTrainer</span>
<span class="hljs-addition">+ from neural_compressor import DistillationConfig</span>

<span class="hljs-addition">+ teacher_model_id = &quot;textattack/bert-base-uncased-SST-2&quot;</span>
<span class="hljs-addition">+ teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_id)</span>
<span class="hljs-addition">+ distillation_config = DistillationConfig(teacher_model=teacher_model)</span>

<span class="hljs-deletion">- trainer = Trainer(</span>
<span class="hljs-addition">+ trainer = INCTrainer(</span>
    model=model,
<span class="hljs-addition">+   distillation_config=distillation_config,</span>
    args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=False),
    train_dataset=dataset[&quot;train&quot;].select(range(300)),
    eval_dataset=dataset[&quot;validation&quot;],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
)

train_result = trainer.train()
metrics = trainer.evaluate()
trainer.save_model()

model = AutoModelForSequenceClassification.from_pretrained(save_dir)`}}),Ce=new U({}),Ne=new T({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBJTkNNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIySW50ZWwlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1maW5ldHVuZWQtc3N0LTItZW5nbGlzaC1pbnQ4LWR5bmFtaWMlMjIlMEFtb2RlbCUyMCUzRCUyMElOQ01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSk=",highlighted:`<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> INCModelForSequenceClassification

model_name = <span class="hljs-string">&quot;Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-dynamic&quot;</span>
model = INCModelForSequenceClassification.from_pretrained(model_name)`}}),Ve=new U({}),Ye=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkKSUwQXBpcGVfY2xzJTIwJTNEJTIwcGlwZWxpbmUoJTIydGV4dC1jbGFzc2lmaWNhdGlvbiUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBdGV4dCUyMCUzRCUyMCUyMkhlJ3MlMjBhJTIwZHJlYWRmdWwlMjBtYWdpY2lhbi4lMjIlMEFvdXRwdXRzJTIwJTNEJTIwcGlwZV9jbHModGV4dCklMEElMEElNUIlN0InbGFiZWwnJTNBJTIwJ05FR0FUSVZFJyUyQyUyMCdzY29yZSclM0ElMjAwLjk4ODAyMTYxMjE2NzM1ODQlN0QlNUQ=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline

tokenizer = AutoTokenizer.from_pretrained(model_id)
pipe_cls = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=model, tokenizer=tokenizer)
text = <span class="hljs-string">&quot;He&#x27;s a dreadful magician.&quot;</span>
outputs = pipe_cls(text)

[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9880216121673584</span>}]`}}),{c(){v=l("meta"),It=d(),j=l("h1"),N=l("a"),De=l("span"),m(oe.$$.fragment),Na=d(),Ke=l("span"),ka=p("Optimization"),Rt=d(),$e=l("p"),Va=p("Optimum Intel can be used to apply popular compression techniques such as quantization, pruning and knowledge distillation."),_t=d(),g=l("h2"),k=l("a"),Le=l("span"),m(se.$$.fragment),Qa=d(),Oe=l("span"),Ya=p("Post-training optimization"),Ft=d(),V=l("p"),qa=p("Post-training compression techniques such as dynamic and static quantization can be easily applied on your model using our "),re=l("a"),et=l("code"),$a=p("INCQuantizer"),Sa=p(`.
Note that quantization is currently only supported for CPUs (only CPU backends are available), so we will not be utilizing GPUs / CUDA in the following examples.`),Et=d(),X=l("h3"),Q=l("a"),tt=l("span"),m(pe.$$.fragment),Aa=d(),at=l("span"),Ha=p("Dynamic quantization"),Ct=d(),Se=l("p"),xa=p("You can easily add dynamic quantization on your model by using the following command line:"),Bt=d(),m(ce.$$.fragment),Nt=d(),Ae=l("p"),Pa=p("When applying post-training quantization, an accuracy tolerance along with an adapted evaluation function can also be specified in order to find a quantized model meeting the specified constraints. This can be done for both dynamic and static quantization."),kt=d(),m(de.$$.fragment),Vt=d(),z=l("h3"),Y=l("a"),lt=l("span"),m(ue.$$.fragment),Da=d(),nt=l("span"),Ka=p("Static quantization"),Qt=d(),He=l("p"),La=p("In the same manner we can apply static quantization, for which we also need to generate the calibration dataset in order to perform the calibration step."),Yt=d(),m(me.$$.fragment),qt=d(),G=l("h3"),q=l("a"),it=l("span"),m(he.$$.fragment),Oa=d(),ot=l("span"),el=p("Specify Quantization Recipes"),$t=d(),$=l("p"),tl=p("The "),ye=l("a"),al=p("SmoothQuant"),ll=p(" methodology is available for post-training quantization. This methodology usually improves the accuracy of the model in comparison to other post-training static quantization methodologies. This is done by migrating the difficulty from activations to weights with a mathematically equivalent transformation."),St=d(),m(Me.$$.fragment),At=d(),Z=l("p"),nl=p("Please refer to INC "),fe=l("a"),il=p("documentation"),ol=p(" and the list of "),be=l("a"),sl=p("models"),rl=p(" quantized with the methodology for more details."),Ht=d(),I=l("h3"),S=l("a"),st=l("span"),m(we.$$.fragment),pl=d(),rt=l("span"),cl=p("Distributed Acuracy-aware Tuning"),xt=p(`

One challenge in model quantization is identifying the optimal configuration that balances accuracy and performance. Distributed tuning speeds up this time-consuming process by parallelizing it across multiple nodes, which accelerates the tuning process in linear scaling.
`),w=l("p"),dl=p("To utilize distributed tuning, please set the "),pt=l("code"),ul=p("quant_level"),ml=p(" to "),ct=l("code"),hl=p("1"),yl=p(" and run it with "),dt=l("code"),Ml=p("mpirun"),fl=p("."),Pt=d(),m(Je.$$.fragment),Dt=d(),m(Ue.$$.fragment),Kt=d(),W=l("p"),bl=p("Please refer to INC "),Te=l("a"),wl=p("documentation"),Jl=p(" and "),Ze=l("a"),Ul=p("text-classification"),Tl=p(" example for more details."),Lt=d(),R=l("h2"),A=l("a"),ut=l("span"),m(We.$$.fragment),Zl=d(),mt=l("span"),Wl=p("During training optimization"),Ot=d(),J=l("p"),vl=p("The "),ve=l("a"),ht=l("code"),jl=p("INCTrainer"),gl=p(` class provides an API to train your model while combining different compression techniques such as knowledge distillation, pruning and quantization.
The `),yt=l("code"),Xl=p("INCTrainer"),zl=p(" is very similar to the \u{1F917} Transformers "),je=l("a"),Mt=l("code"),Gl=p("Trainer"),Il=p(", which can be replaced with minimal changes in your code."),ea=d(),_=l("h3"),H=l("a"),ft=l("span"),m(ge.$$.fragment),Rl=d(),bt=l("span"),_l=p("Quantization"),ta=d(),x=l("p"),Fl=p("To apply quantization during training, you only need to create the appropriate configuration and pass it to the "),wt=l("code"),El=p("INCTrainer"),Cl=p("."),aa=d(),m(Xe.$$.fragment),la=d(),F=l("h3"),P=l("a"),Jt=l("span"),m(ze.$$.fragment),Bl=d(),Ut=l("span"),Nl=p("Pruning"),na=d(),D=l("p"),kl=p(`In the same manner, pruning can be applied by specifying the pruning configuration detailing the desired pruning process.
To know more about the different supported methodologies, you can refer to the Neural Compressor `),Ge=l("a"),Vl=p("documentation"),Ql=p(`.
At the moment, pruning is applied on both the linear and the convolutional layers, and not on other layers such as the embeddings. It\u2019s important to mention that the pruning sparsity defined in the configuration will be applied on these layers, and thus will not results in the global model sparsity.`),ia=d(),m(Ie.$$.fragment),oa=d(),E=l("h3"),K=l("a"),Tt=l("span"),m(Re.$$.fragment),Yl=d(),Zt=l("span"),ql=p("Knowledge distillation"),sa=d(),_e=l("p"),$l=p(`Knowledge distillation can also be applied in the same manner.
To know more about the different supported methodologies, you can refer to the Neural Compressor `),Fe=l("a"),Sl=p("documentation"),ra=d(),m(Ee.$$.fragment),pa=d(),C=l("h2"),L=l("a"),Wt=l("span"),m(Ce.$$.fragment),Al=d(),vt=l("span"),Hl=p("Loading a quantized model"),ca=d(),O=l("p"),xl=p("To load a quantized model hosted locally or on the \u{1F917} hub, you must instantiate you model using our "),Be=l("a"),jt=l("code"),Pl=p("INCModelForXxx"),Dl=p(" classes."),da=d(),m(Ne.$$.fragment),ua=d(),ee=l("p"),Kl=p("You can load many more quantized models hosted on the hub under the Intel organization "),ke=l("a"),gt=l("code"),Ll=p("here"),Ol=p("."),ma=d(),B=l("h2"),te=l("a"),Xt=l("span"),m(Ve.$$.fragment),en=d(),zt=l("span"),tn=p("Inference with Transformers pipeline"),ha=d(),ae=l("p"),an=p("The quantized model can then easily be used to run inference with the Transformers "),Qe=l("a"),ln=p("pipelines"),nn=p("."),ya=d(),m(Ye.$$.fragment),Ma=d(),le=l("p"),on=p("Check out the "),qe=l("a"),Gt=l("code"),sn=p("examples"),rn=p(" directory for more sophisticated usage."),this.h()},l(e){const o=vi('[data-svelte="svelte-1phssyn"]',document.head);v=n(o,"META",{name:!0,content:!0}),o.forEach(t),It=u(e),j=n(e,"H1",{class:!0});var ba=i(j);N=n(ba,"A",{id:!0,class:!0,href:!0});var dn=i(N);De=n(dn,"SPAN",{});var un=i(De);h(oe.$$.fragment,un),un.forEach(t),dn.forEach(t),Na=u(ba),Ke=n(ba,"SPAN",{});var mn=i(Ke);ka=c(mn,"Optimization"),mn.forEach(t),ba.forEach(t),Rt=u(e),$e=n(e,"P",{});var hn=i($e);Va=c(hn,"Optimum Intel can be used to apply popular compression techniques such as quantization, pruning and knowledge distillation."),hn.forEach(t),_t=u(e),g=n(e,"H2",{class:!0});var wa=i(g);k=n(wa,"A",{id:!0,class:!0,href:!0});var yn=i(k);Le=n(yn,"SPAN",{});var Mn=i(Le);h(se.$$.fragment,Mn),Mn.forEach(t),yn.forEach(t),Qa=u(wa),Oe=n(wa,"SPAN",{});var fn=i(Oe);Ya=c(fn,"Post-training optimization"),fn.forEach(t),wa.forEach(t),Ft=u(e),V=n(e,"P",{});var Ja=i(V);qa=c(Ja,"Post-training compression techniques such as dynamic and static quantization can be easily applied on your model using our "),re=n(Ja,"A",{href:!0,rel:!0});var bn=i(re);et=n(bn,"CODE",{});var wn=i(et);$a=c(wn,"INCQuantizer"),wn.forEach(t),bn.forEach(t),Sa=c(Ja,`.
Note that quantization is currently only supported for CPUs (only CPU backends are available), so we will not be utilizing GPUs / CUDA in the following examples.`),Ja.forEach(t),Et=u(e),X=n(e,"H3",{class:!0});var Ua=i(X);Q=n(Ua,"A",{id:!0,class:!0,href:!0});var Jn=i(Q);tt=n(Jn,"SPAN",{});var Un=i(tt);h(pe.$$.fragment,Un),Un.forEach(t),Jn.forEach(t),Aa=u(Ua),at=n(Ua,"SPAN",{});var Tn=i(at);Ha=c(Tn,"Dynamic quantization"),Tn.forEach(t),Ua.forEach(t),Ct=u(e),Se=n(e,"P",{});var Zn=i(Se);xa=c(Zn,"You can easily add dynamic quantization on your model by using the following command line:"),Zn.forEach(t),Bt=u(e),h(ce.$$.fragment,e),Nt=u(e),Ae=n(e,"P",{});var Wn=i(Ae);Pa=c(Wn,"When applying post-training quantization, an accuracy tolerance along with an adapted evaluation function can also be specified in order to find a quantized model meeting the specified constraints. This can be done for both dynamic and static quantization."),Wn.forEach(t),kt=u(e),h(de.$$.fragment,e),Vt=u(e),z=n(e,"H3",{class:!0});var Ta=i(z);Y=n(Ta,"A",{id:!0,class:!0,href:!0});var vn=i(Y);lt=n(vn,"SPAN",{});var jn=i(lt);h(ue.$$.fragment,jn),jn.forEach(t),vn.forEach(t),Da=u(Ta),nt=n(Ta,"SPAN",{});var gn=i(nt);Ka=c(gn,"Static quantization"),gn.forEach(t),Ta.forEach(t),Qt=u(e),He=n(e,"P",{});var Xn=i(He);La=c(Xn,"In the same manner we can apply static quantization, for which we also need to generate the calibration dataset in order to perform the calibration step."),Xn.forEach(t),Yt=u(e),h(me.$$.fragment,e),qt=u(e),G=n(e,"H3",{class:!0});var Za=i(G);q=n(Za,"A",{id:!0,class:!0,href:!0});var zn=i(q);it=n(zn,"SPAN",{});var Gn=i(it);h(he.$$.fragment,Gn),Gn.forEach(t),zn.forEach(t),Oa=u(Za),ot=n(Za,"SPAN",{});var In=i(ot);el=c(In,"Specify Quantization Recipes"),In.forEach(t),Za.forEach(t),$t=u(e),$=n(e,"P",{});var Wa=i($);tl=c(Wa,"The "),ye=n(Wa,"A",{href:!0,rel:!0});var Rn=i(ye);al=c(Rn,"SmoothQuant"),Rn.forEach(t),ll=c(Wa," methodology is available for post-training quantization. This methodology usually improves the accuracy of the model in comparison to other post-training static quantization methodologies. This is done by migrating the difficulty from activations to weights with a mathematically equivalent transformation."),Wa.forEach(t),St=u(e),h(Me.$$.fragment,e),At=u(e),Z=n(e,"P",{});var xe=i(Z);nl=c(xe,"Please refer to INC "),fe=n(xe,"A",{href:!0,rel:!0});var _n=i(fe);il=c(_n,"documentation"),_n.forEach(t),ol=c(xe," and the list of "),be=n(xe,"A",{href:!0,rel:!0});var Fn=i(be);sl=c(Fn,"models"),Fn.forEach(t),rl=c(xe," quantized with the methodology for more details."),xe.forEach(t),Ht=u(e),I=n(e,"H3",{class:!0});var va=i(I);S=n(va,"A",{id:!0,class:!0,href:!0});var En=i(S);st=n(En,"SPAN",{});var Cn=i(st);h(we.$$.fragment,Cn),Cn.forEach(t),En.forEach(t),pl=u(va),rt=n(va,"SPAN",{});var Bn=i(rt);cl=c(Bn,"Distributed Acuracy-aware Tuning"),Bn.forEach(t),va.forEach(t),xt=c(e,`

One challenge in model quantization is identifying the optimal configuration that balances accuracy and performance. Distributed tuning speeds up this time-consuming process by parallelizing it across multiple nodes, which accelerates the tuning process in linear scaling.
`),w=n(e,"P",{});var ne=i(w);dl=c(ne,"To utilize distributed tuning, please set the "),pt=n(ne,"CODE",{});var Nn=i(pt);ul=c(Nn,"quant_level"),Nn.forEach(t),ml=c(ne," to "),ct=n(ne,"CODE",{});var kn=i(ct);hl=c(kn,"1"),kn.forEach(t),yl=c(ne," and run it with "),dt=n(ne,"CODE",{});var Vn=i(dt);Ml=c(Vn,"mpirun"),Vn.forEach(t),fl=c(ne,"."),ne.forEach(t),Pt=u(e),h(Je.$$.fragment,e),Dt=u(e),h(Ue.$$.fragment,e),Kt=u(e),W=n(e,"P",{});var Pe=i(W);bl=c(Pe,"Please refer to INC "),Te=n(Pe,"A",{href:!0,rel:!0});var Qn=i(Te);wl=c(Qn,"documentation"),Qn.forEach(t),Jl=c(Pe," and "),Ze=n(Pe,"A",{href:!0,rel:!0});var Yn=i(Ze);Ul=c(Yn,"text-classification"),Yn.forEach(t),Tl=c(Pe," example for more details."),Pe.forEach(t),Lt=u(e),R=n(e,"H2",{class:!0});var ja=i(R);A=n(ja,"A",{id:!0,class:!0,href:!0});var qn=i(A);ut=n(qn,"SPAN",{});var $n=i(ut);h(We.$$.fragment,$n),$n.forEach(t),qn.forEach(t),Zl=u(ja),mt=n(ja,"SPAN",{});var Sn=i(mt);Wl=c(Sn,"During training optimization"),Sn.forEach(t),ja.forEach(t),Ot=u(e),J=n(e,"P",{});var ie=i(J);vl=c(ie,"The "),ve=n(ie,"A",{href:!0,rel:!0});var An=i(ve);ht=n(An,"CODE",{});var Hn=i(ht);jl=c(Hn,"INCTrainer"),Hn.forEach(t),An.forEach(t),gl=c(ie,` class provides an API to train your model while combining different compression techniques such as knowledge distillation, pruning and quantization.
The `),yt=n(ie,"CODE",{});var xn=i(yt);Xl=c(xn,"INCTrainer"),xn.forEach(t),zl=c(ie," is very similar to the \u{1F917} Transformers "),je=n(ie,"A",{href:!0,rel:!0});var Pn=i(je);Mt=n(Pn,"CODE",{});var Dn=i(Mt);Gl=c(Dn,"Trainer"),Dn.forEach(t),Pn.forEach(t),Il=c(ie,", which can be replaced with minimal changes in your code."),ie.forEach(t),ea=u(e),_=n(e,"H3",{class:!0});var ga=i(_);H=n(ga,"A",{id:!0,class:!0,href:!0});var Kn=i(H);ft=n(Kn,"SPAN",{});var Ln=i(ft);h(ge.$$.fragment,Ln),Ln.forEach(t),Kn.forEach(t),Rl=u(ga),bt=n(ga,"SPAN",{});var On=i(bt);_l=c(On,"Quantization"),On.forEach(t),ga.forEach(t),ta=u(e),x=n(e,"P",{});var Xa=i(x);Fl=c(Xa,"To apply quantization during training, you only need to create the appropriate configuration and pass it to the "),wt=n(Xa,"CODE",{});var ei=i(wt);El=c(ei,"INCTrainer"),ei.forEach(t),Cl=c(Xa,"."),Xa.forEach(t),aa=u(e),h(Xe.$$.fragment,e),la=u(e),F=n(e,"H3",{class:!0});var za=i(F);P=n(za,"A",{id:!0,class:!0,href:!0});var ti=i(P);Jt=n(ti,"SPAN",{});var ai=i(Jt);h(ze.$$.fragment,ai),ai.forEach(t),ti.forEach(t),Bl=u(za),Ut=n(za,"SPAN",{});var li=i(Ut);Nl=c(li,"Pruning"),li.forEach(t),za.forEach(t),na=u(e),D=n(e,"P",{});var Ga=i(D);kl=c(Ga,`In the same manner, pruning can be applied by specifying the pruning configuration detailing the desired pruning process.
To know more about the different supported methodologies, you can refer to the Neural Compressor `),Ge=n(Ga,"A",{href:!0,rel:!0});var ni=i(Ge);Vl=c(ni,"documentation"),ni.forEach(t),Ql=c(Ga,`.
At the moment, pruning is applied on both the linear and the convolutional layers, and not on other layers such as the embeddings. It\u2019s important to mention that the pruning sparsity defined in the configuration will be applied on these layers, and thus will not results in the global model sparsity.`),Ga.forEach(t),ia=u(e),h(Ie.$$.fragment,e),oa=u(e),E=n(e,"H3",{class:!0});var Ia=i(E);K=n(Ia,"A",{id:!0,class:!0,href:!0});var ii=i(K);Tt=n(ii,"SPAN",{});var oi=i(Tt);h(Re.$$.fragment,oi),oi.forEach(t),ii.forEach(t),Yl=u(Ia),Zt=n(Ia,"SPAN",{});var si=i(Zt);ql=c(si,"Knowledge distillation"),si.forEach(t),Ia.forEach(t),sa=u(e),_e=n(e,"P",{});var pn=i(_e);$l=c(pn,`Knowledge distillation can also be applied in the same manner.
To know more about the different supported methodologies, you can refer to the Neural Compressor `),Fe=n(pn,"A",{href:!0,rel:!0});var ri=i(Fe);Sl=c(ri,"documentation"),ri.forEach(t),pn.forEach(t),ra=u(e),h(Ee.$$.fragment,e),pa=u(e),C=n(e,"H2",{class:!0});var Ra=i(C);L=n(Ra,"A",{id:!0,class:!0,href:!0});var pi=i(L);Wt=n(pi,"SPAN",{});var ci=i(Wt);h(Ce.$$.fragment,ci),ci.forEach(t),pi.forEach(t),Al=u(Ra),vt=n(Ra,"SPAN",{});var di=i(vt);Hl=c(di,"Loading a quantized model"),di.forEach(t),Ra.forEach(t),ca=u(e),O=n(e,"P",{});var _a=i(O);xl=c(_a,"To load a quantized model hosted locally or on the \u{1F917} hub, you must instantiate you model using our "),Be=n(_a,"A",{href:!0,rel:!0});var ui=i(Be);jt=n(ui,"CODE",{});var mi=i(jt);Pl=c(mi,"INCModelForXxx"),mi.forEach(t),ui.forEach(t),Dl=c(_a," classes."),_a.forEach(t),da=u(e),h(Ne.$$.fragment,e),ua=u(e),ee=n(e,"P",{});var Fa=i(ee);Kl=c(Fa,"You can load many more quantized models hosted on the hub under the Intel organization "),ke=n(Fa,"A",{href:!0,rel:!0});var hi=i(ke);gt=n(hi,"CODE",{});var yi=i(gt);Ll=c(yi,"here"),yi.forEach(t),hi.forEach(t),Ol=c(Fa,"."),Fa.forEach(t),ma=u(e),B=n(e,"H2",{class:!0});var Ea=i(B);te=n(Ea,"A",{id:!0,class:!0,href:!0});var Mi=i(te);Xt=n(Mi,"SPAN",{});var fi=i(Xt);h(Ve.$$.fragment,fi),fi.forEach(t),Mi.forEach(t),en=u(Ea),zt=n(Ea,"SPAN",{});var bi=i(zt);tn=c(bi,"Inference with Transformers pipeline"),bi.forEach(t),Ea.forEach(t),ha=u(e),ae=n(e,"P",{});var Ca=i(ae);an=c(Ca,"The quantized model can then easily be used to run inference with the Transformers "),Qe=n(Ca,"A",{href:!0,rel:!0});var wi=i(Qe);ln=c(wi,"pipelines"),wi.forEach(t),nn=c(Ca,"."),Ca.forEach(t),ya=u(e),h(Ye.$$.fragment,e),Ma=u(e),le=n(e,"P",{});var Ba=i(le);on=c(Ba,"Check out the "),qe=n(Ba,"A",{href:!0,rel:!0});var Ji=i(qe);Gt=n(Ji,"CODE",{});var Ui=i(Gt);sn=c(Ui,"examples"),Ui.forEach(t),Ji.forEach(t),rn=c(Ba," directory for more sophisticated usage."),Ba.forEach(t),this.h()},h(){s(v,"name","hf:doc:metadata"),s(v,"content",JSON.stringify(zi)),s(N,"id","optimization"),s(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(N,"href","#optimization"),s(j,"class","relative group"),s(k,"id","posttraining-optimization"),s(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(k,"href","#posttraining-optimization"),s(g,"class","relative group"),s(re,"href","https://huggingface.co/docs/optimum/intel_optimization#optimum.intel.neural_compressor.IncQuantizer"),s(re,"rel","nofollow"),s(Q,"id","dynamic-quantization"),s(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(Q,"href","#dynamic-quantization"),s(X,"class","relative group"),s(Y,"id","static-quantization"),s(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(Y,"href","#static-quantization"),s(z,"class","relative group"),s(q,"id","specify-quantization-recipes"),s(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(q,"href","#specify-quantization-recipes"),s(G,"class","relative group"),s(ye,"href","https://arxiv.org/abs/2211.10438"),s(ye,"rel","nofollow"),s(fe,"href","https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md"),s(fe,"rel","nofollow"),s(be,"href","https://github.com/intel/neural-compressor/blob/master/docs/source/smooth_quant.md#validated-models"),s(be,"rel","nofollow"),s(S,"id","distributed-acuracyaware-tuning"),s(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(S,"href","#distributed-acuracyaware-tuning"),s(I,"class","relative group"),s(Te,"href","https://github.com/intel/neural-compressor/blob/master/docs/source/tuning_strategies.md#distributed-tuning"),s(Te,"rel","nofollow"),s(Ze,"href","https://github.com/huggingface/optimum-intel/tree/main/examples/neural_compressor/text-classification"),s(Ze,"rel","nofollow"),s(A,"id","during-training-optimization"),s(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(A,"href","#during-training-optimization"),s(R,"class","relative group"),s(ve,"href","https://huggingface.co/docs/optimum/main/intel/reference_inc#optimum.intel.INCTrainer"),s(ve,"rel","nofollow"),s(je,"href","https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer"),s(je,"rel","nofollow"),s(H,"id","quantization"),s(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(H,"href","#quantization"),s(_,"class","relative group"),s(P,"id","pruning"),s(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(P,"href","#pruning"),s(F,"class","relative group"),s(Ge,"href","https://github.com/intel/neural-compressor/tree/master/neural_compressor/compression/pruner#pruning-types"),s(Ge,"rel","nofollow"),s(K,"id","knowledge-distillation"),s(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(K,"href","#knowledge-distillation"),s(E,"class","relative group"),s(Fe,"href","https://github.com/intel/neural-compressor/blob/master/docs/source/distillation.md"),s(Fe,"rel","nofollow"),s(L,"id","loading-a-quantized-model"),s(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(L,"href","#loading-a-quantized-model"),s(C,"class","relative group"),s(Be,"href","https://huggingface.co/docs/optimum/main/intel/reference_inc#optimum.intel.neural_compressor.quantization.INCModel"),s(Be,"rel","nofollow"),s(ke,"href","https://huggingface.co/Intel"),s(ke,"rel","nofollow"),s(te,"id","inference-with-transformers-pipeline"),s(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(te,"href","#inference-with-transformers-pipeline"),s(B,"class","relative group"),s(Qe,"href","https://huggingface.co/docs/transformers/main/en/main_classes/pipelines"),s(Qe,"rel","nofollow"),s(qe,"href","https://github.com/huggingface/optimum-intel/tree/main/examples"),s(qe,"rel","nofollow")},m(e,o){a(document.head,v),r(e,It,o),r(e,j,o),a(j,N),a(N,De),y(oe,De,null),a(j,Na),a(j,Ke),a(Ke,ka),r(e,Rt,o),r(e,$e,o),a($e,Va),r(e,_t,o),r(e,g,o),a(g,k),a(k,Le),y(se,Le,null),a(g,Qa),a(g,Oe),a(Oe,Ya),r(e,Ft,o),r(e,V,o),a(V,qa),a(V,re),a(re,et),a(et,$a),a(V,Sa),r(e,Et,o),r(e,X,o),a(X,Q),a(Q,tt),y(pe,tt,null),a(X,Aa),a(X,at),a(at,Ha),r(e,Ct,o),r(e,Se,o),a(Se,xa),r(e,Bt,o),y(ce,e,o),r(e,Nt,o),r(e,Ae,o),a(Ae,Pa),r(e,kt,o),y(de,e,o),r(e,Vt,o),r(e,z,o),a(z,Y),a(Y,lt),y(ue,lt,null),a(z,Da),a(z,nt),a(nt,Ka),r(e,Qt,o),r(e,He,o),a(He,La),r(e,Yt,o),y(me,e,o),r(e,qt,o),r(e,G,o),a(G,q),a(q,it),y(he,it,null),a(G,Oa),a(G,ot),a(ot,el),r(e,$t,o),r(e,$,o),a($,tl),a($,ye),a(ye,al),a($,ll),r(e,St,o),y(Me,e,o),r(e,At,o),r(e,Z,o),a(Z,nl),a(Z,fe),a(fe,il),a(Z,ol),a(Z,be),a(be,sl),a(Z,rl),r(e,Ht,o),r(e,I,o),a(I,S),a(S,st),y(we,st,null),a(I,pl),a(I,rt),a(rt,cl),r(e,xt,o),r(e,w,o),a(w,dl),a(w,pt),a(pt,ul),a(w,ml),a(w,ct),a(ct,hl),a(w,yl),a(w,dt),a(dt,Ml),a(w,fl),r(e,Pt,o),y(Je,e,o),r(e,Dt,o),y(Ue,e,o),r(e,Kt,o),r(e,W,o),a(W,bl),a(W,Te),a(Te,wl),a(W,Jl),a(W,Ze),a(Ze,Ul),a(W,Tl),r(e,Lt,o),r(e,R,o),a(R,A),a(A,ut),y(We,ut,null),a(R,Zl),a(R,mt),a(mt,Wl),r(e,Ot,o),r(e,J,o),a(J,vl),a(J,ve),a(ve,ht),a(ht,jl),a(J,gl),a(J,yt),a(yt,Xl),a(J,zl),a(J,je),a(je,Mt),a(Mt,Gl),a(J,Il),r(e,ea,o),r(e,_,o),a(_,H),a(H,ft),y(ge,ft,null),a(_,Rl),a(_,bt),a(bt,_l),r(e,ta,o),r(e,x,o),a(x,Fl),a(x,wt),a(wt,El),a(x,Cl),r(e,aa,o),y(Xe,e,o),r(e,la,o),r(e,F,o),a(F,P),a(P,Jt),y(ze,Jt,null),a(F,Bl),a(F,Ut),a(Ut,Nl),r(e,na,o),r(e,D,o),a(D,kl),a(D,Ge),a(Ge,Vl),a(D,Ql),r(e,ia,o),y(Ie,e,o),r(e,oa,o),r(e,E,o),a(E,K),a(K,Tt),y(Re,Tt,null),a(E,Yl),a(E,Zt),a(Zt,ql),r(e,sa,o),r(e,_e,o),a(_e,$l),a(_e,Fe),a(Fe,Sl),r(e,ra,o),y(Ee,e,o),r(e,pa,o),r(e,C,o),a(C,L),a(L,Wt),y(Ce,Wt,null),a(C,Al),a(C,vt),a(vt,Hl),r(e,ca,o),r(e,O,o),a(O,xl),a(O,Be),a(Be,jt),a(jt,Pl),a(O,Dl),r(e,da,o),y(Ne,e,o),r(e,ua,o),r(e,ee,o),a(ee,Kl),a(ee,ke),a(ke,gt),a(gt,Ll),a(ee,Ol),r(e,ma,o),r(e,B,o),a(B,te),a(te,Xt),y(Ve,Xt,null),a(B,en),a(B,zt),a(zt,tn),r(e,ha,o),r(e,ae,o),a(ae,an),a(ae,Qe),a(Qe,ln),a(ae,nn),r(e,ya,o),y(Ye,e,o),r(e,Ma,o),r(e,le,o),a(le,on),a(le,qe),a(qe,Gt),a(Gt,sn),a(le,rn),fa=!0},p:ji,i(e){fa||(M(oe.$$.fragment,e),M(se.$$.fragment,e),M(pe.$$.fragment,e),M(ce.$$.fragment,e),M(de.$$.fragment,e),M(ue.$$.fragment,e),M(me.$$.fragment,e),M(he.$$.fragment,e),M(Me.$$.fragment,e),M(we.$$.fragment,e),M(Je.$$.fragment,e),M(Ue.$$.fragment,e),M(We.$$.fragment,e),M(ge.$$.fragment,e),M(Xe.$$.fragment,e),M(ze.$$.fragment,e),M(Ie.$$.fragment,e),M(Re.$$.fragment,e),M(Ee.$$.fragment,e),M(Ce.$$.fragment,e),M(Ne.$$.fragment,e),M(Ve.$$.fragment,e),M(Ye.$$.fragment,e),fa=!0)},o(e){f(oe.$$.fragment,e),f(se.$$.fragment,e),f(pe.$$.fragment,e),f(ce.$$.fragment,e),f(de.$$.fragment,e),f(ue.$$.fragment,e),f(me.$$.fragment,e),f(he.$$.fragment,e),f(Me.$$.fragment,e),f(we.$$.fragment,e),f(Je.$$.fragment,e),f(Ue.$$.fragment,e),f(We.$$.fragment,e),f(ge.$$.fragment,e),f(Xe.$$.fragment,e),f(ze.$$.fragment,e),f(Ie.$$.fragment,e),f(Re.$$.fragment,e),f(Ee.$$.fragment,e),f(Ce.$$.fragment,e),f(Ne.$$.fragment,e),f(Ve.$$.fragment,e),f(Ye.$$.fragment,e),fa=!1},d(e){t(v),e&&t(It),e&&t(j),b(oe),e&&t(Rt),e&&t($e),e&&t(_t),e&&t(g),b(se),e&&t(Ft),e&&t(V),e&&t(Et),e&&t(X),b(pe),e&&t(Ct),e&&t(Se),e&&t(Bt),b(ce,e),e&&t(Nt),e&&t(Ae),e&&t(kt),b(de,e),e&&t(Vt),e&&t(z),b(ue),e&&t(Qt),e&&t(He),e&&t(Yt),b(me,e),e&&t(qt),e&&t(G),b(he),e&&t($t),e&&t($),e&&t(St),b(Me,e),e&&t(At),e&&t(Z),e&&t(Ht),e&&t(I),b(we),e&&t(xt),e&&t(w),e&&t(Pt),b(Je,e),e&&t(Dt),b(Ue,e),e&&t(Kt),e&&t(W),e&&t(Lt),e&&t(R),b(We),e&&t(Ot),e&&t(J),e&&t(ea),e&&t(_),b(ge),e&&t(ta),e&&t(x),e&&t(aa),b(Xe,e),e&&t(la),e&&t(F),b(ze),e&&t(na),e&&t(D),e&&t(ia),b(Ie,e),e&&t(oa),e&&t(E),b(Re),e&&t(sa),e&&t(_e),e&&t(ra),b(Ee,e),e&&t(pa),e&&t(C),b(Ce),e&&t(ca),e&&t(O),e&&t(da),b(Ne,e),e&&t(ua),e&&t(ee),e&&t(ma),e&&t(B),b(Ve),e&&t(ha),e&&t(ae),e&&t(ya),b(Ye,e),e&&t(Ma),e&&t(le)}}}const zi={local:"optimization",sections:[{local:"posttraining-optimization",sections:[{local:"dynamic-quantization",title:"Dynamic quantization"},{local:"static-quantization",title:"Static quantization"},{local:"specify-quantization-recipes",title:"Specify Quantization Recipes"},{local:"distributed-acuracyaware-tuning",title:"Distributed Acuracy-aware Tuning"}],title:"Post-training optimization"},{local:"during-training-optimization",sections:[{local:"quantization",title:"Quantization"},{local:"pruning",title:"Pruning"},{local:"knowledge-distillation",title:"Knowledge distillation"}],title:"During training optimization"},{local:"loading-a-quantized-model",title:"Loading a quantized model"},{local:"inference-with-transformers-pipeline",title:"Inference with Transformers pipeline"}],title:"Optimization"};function Gi(cn){return gi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fi extends Ti{constructor(v){super();Zi(this,v,Gi,Xi,Wi,{})}}export{Fi as default,zi as metadata};
