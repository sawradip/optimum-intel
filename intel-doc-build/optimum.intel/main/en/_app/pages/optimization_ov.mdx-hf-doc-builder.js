import{S as Ya,i as Sa,s as Ha,e as i,k as c,w as y,t as a,M as xa,c as o,d as l,m as d,a as s,x as J,h as n,b as M,G as t,g as p,y as f,L as $a,q as w,o as U,B as j,v as Oa}from"../chunks/vendor-hf-doc-builder.js";import{I as we}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as xe}from"../chunks/CodeBlock-hf-doc-builder.js";function Da(Ll){let v,$e,C,z,Ue,H,Et,je,zt,Oe,T,Bt,Te,Gt,Rt,x,kt,Xt,De,I,B,be,$,_t,ge,qt,Pe,de,Ft,Le,O,Ke,G,Qt,ve,Nt,Yt,et,Z,R,Ce,D,St,Ie,Ht,tt,k,xt,Ze,$t,Ot,lt,W,X,We,P,Dt,Ve,Pt,at,me,Lt,nt,L,it,V,_,Ae,K,Kt,Ee,el,ot,b,tl,ze,ll,al,Be,nl,il,st,u,ol,Ge,sl,rl,ee,pl,Ml,te,cl,dl,rt,le,pt,he,A,ml,Re,hl,ul,ae,yl,Jl,Mt,m,fl,ke,wl,Ul,Xe,jl,Tl,_e,bl,gl,qe,vl,Cl,Fe,Il,Zl,Qe,Wl,Vl,Ne,Al,El,ct,ne,dt,q,zl,ie,Bl,Gl,mt,F,Rl,oe,kl,Xl,ht,Q,_l,se,ql,Fl,ut,N,Ql,re,Nl,Yl,yt,E,Y,Ye,pe,Sl,Se,Hl,Jt,g,xl,He,$l,Ol,Me,Dl,Pl,ft,ce,wt;return H=new we({}),$=new we({}),O=new xe({props:{code:"ZnJvbSUyMGZ1bmN0b29scyUyMGltcG9ydCUyMHBhcnRpYWwlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUyQyUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb3B0aW11bS5pbnRlbCUyMGltcG9ydCUyME9WQ29uZmlnJTJDJTIwT1ZRdWFudGl6ZXIlMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkLWZpbmV0dW5lZC1zc3QtMi1lbmdsaXNoJTIyJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEElMjMlMjBUaGUlMjBkaXJlY3RvcnklMjB3aGVyZSUyMHRoZSUyMHF1YW50aXplZCUyMG1vZGVsJTIwd2lsbCUyMGJlJTIwc2F2ZWQlMEFzYXZlX2RpciUyMCUzRCUyMCUyMnB0cV9tb2RlbCUyMiUwQSUwQWRlZiUyMHByZXByb2Nlc3NfZnVuY3Rpb24oZXhhbXBsZXMlMkMlMjB0b2tlbml6ZXIpJTNBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwdG9rZW5pemVyKGV4YW1wbGVzJTVCJTIyc2VudGVuY2UlMjIlNUQlMkMlMjBwYWRkaW5nJTNEJTIybWF4X2xlbmd0aCUyMiUyQyUyMG1heF9sZW5ndGglM0QxMjglMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSklMEElMEElMjMlMjBMb2FkJTIwdGhlJTIwZGVmYXVsdCUyMHF1YW50aXphdGlvbiUyMGNvbmZpZ3VyYXRpb24lMjBkZXRhaWxpbmclMjB0aGUlMjBxdWFudGl6YXRpb24lMjB3ZSUyMHdpc2glMjB0byUyMGFwcGx5JTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyME9WQ29uZmlnKCklMEElMjMlMjBJbnN0YW50aWF0ZSUyMG91ciUyME9WUXVhbnRpemVyJTIwdXNpbmclMjB0aGUlMjBkZXNpcmVkJTIwY29uZmlndXJhdGlvbiUwQXF1YW50aXplciUyMCUzRCUyME9WUXVhbnRpemVyLmZyb21fcHJldHJhaW5lZChtb2RlbCklMEElMjMlMjBDcmVhdGUlMjB0aGUlMjBjYWxpYnJhdGlvbiUyMGRhdGFzZXQlMjB1c2VkJTIwdG8lMjBwZXJmb3JtJTIwc3RhdGljJTIwcXVhbnRpemF0aW9uJTBBY2FsaWJyYXRpb25fZGF0YXNldCUyMCUzRCUyMHF1YW50aXplci5nZXRfY2FsaWJyYXRpb25fZGF0YXNldCglMEElMjAlMjAlMjAlMjAlMjJnbHVlJTIyJTJDJTBBJTIwJTIwJTIwJTIwZGF0YXNldF9jb25maWdfbmFtZSUzRCUyMnNzdDIlMjIlMkMlMEElMjAlMjAlMjAlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uJTNEcGFydGlhbChwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUyQyUwQSUyMCUyMCUyMCUyMG51bV9zYW1wbGVzJTNEMzAwJTJDJTBBJTIwJTIwJTIwJTIwZGF0YXNldF9zcGxpdCUzRCUyMnRyYWluJTIyJTJDJTBBKSUwQSUyMyUyMEFwcGx5JTIwc3RhdGljJTIwcXVhbnRpemF0aW9uJTIwYW5kJTIwZXhwb3J0JTIwdGhlJTIwcmVzdWx0aW5nJTIwcXVhbnRpemVkJTIwbW9kZWwlMjB0byUyME9wZW5WSU5PJTIwSVIlMjBmb3JtYXQlMEFxdWFudGl6ZXIucXVhbnRpemUoJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMEElMjAlMjAlMjAlMjBjYWxpYnJhdGlvbl9kYXRhc2V0JTNEY2FsaWJyYXRpb25fZGF0YXNldCUyQyUwQSUyMCUyMCUyMCUyMHNhdmVfZGlyZWN0b3J5JTNEc2F2ZV9kaXIlMkMlMEEpJTBBJTIzJTIwU2F2ZSUyMHRoZSUyMHRva2VuaXplciUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoc2F2ZV9kaXIp",highlighted:`<span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVConfig, OVQuantizer

model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
<span class="hljs-comment"># The directory where the quantized model will be saved</span>
save_dir = <span class="hljs-string">&quot;ptq_model&quot;</span>

<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples, tokenizer</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;sentence&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Load the default quantization configuration detailing the quantization we wish to apply</span>
quantization_config = OVConfig()
<span class="hljs-comment"># Instantiate our OVQuantizer using the desired configuration</span>
quantizer = OVQuantizer.from_pretrained(model)
<span class="hljs-comment"># Create the calibration dataset used to perform static quantization</span>
calibration_dataset = quantizer.get_calibration_dataset(
    <span class="hljs-string">&quot;glue&quot;</span>,
    dataset_config_name=<span class="hljs-string">&quot;sst2&quot;</span>,
    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),
    num_samples=<span class="hljs-number">300</span>,
    dataset_split=<span class="hljs-string">&quot;train&quot;</span>,
)
<span class="hljs-comment"># Apply static quantization and export the resulting quantized model to OpenVINO IR format</span>
quantizer.quantize(
    quantization_config=quantization_config,
    calibration_dataset=calibration_dataset,
    save_directory=save_dir,
)
<span class="hljs-comment"># Save the tokenizer</span>
tokenizer.save_pretrained(save_dir)`}}),D=new we({}),P=new we({}),L=new xe({props:{code:"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",highlighted:`import evaluate
import numpy as np
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TrainingArguments,
    default_data_collator,
)
from datasets import load_dataset
<span class="hljs-deletion">- from transformers import Trainer</span>
<span class="hljs-addition">+ from optimum.intel import OVConfig, OVTrainer, OVModelForSequenceClassification</span>

model_id = &quot;distilbert-base-uncased-finetuned-sst-2-english&quot;
model = AutoModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
# The directory where the quantized model will be saved
save_dir = &quot;qat_model&quot;
dataset = load_dataset(&quot;glue&quot;, &quot;sst2&quot;)
dataset = dataset.map(
    lambda examples: tokenizer(examples[&quot;sentence&quot;], padding=True), batched=True
)
metric = evaluate.load(&quot;glue&quot;, &quot;sst2&quot;)

def compute_metrics(eval_preds):
    preds = np.argmax(eval_preds.predictions, axis=1)
    return metric.compute(predictions=preds, references=eval_preds.label_ids)

# Load the default quantization configuration detailing the quantization we wish to apply
<span class="hljs-addition">+ ov_config = OVConfig()</span>

<span class="hljs-deletion">- trainer = Trainer(</span>
<span class="hljs-addition">+ trainer = OVTrainer(</span>
    model=model,
    args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=True),
    train_dataset=dataset[&quot;train&quot;].select(range(300)),
    eval_dataset=dataset[&quot;validation&quot;],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
<span class="hljs-addition">+   ov_config=ov_config,</span>
<span class="hljs-addition">+   task=&quot;text-classification&quot;,</span>
)

# Train the model while applying quantization
train_result = trainer.train()
metrics = trainer.evaluate()
# Export the quantized model to OpenVINO IR format and save it
trainer.save_model()

# Load the resulting quantized model
<span class="hljs-deletion">- model = AutoModelForSequenceClassification.from_pretrained(save_dir)</span>
<span class="hljs-addition">+ model = OVModelForSequenceClassification.from_pretrained(save_dir)</span>`}}),K=new we({}),le=new xe({props:{code:"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",highlighted:`compression_config = [
    {
        <span class="hljs-string">&quot;compression&quot;</span>:
        {
        <span class="hljs-string">&quot;algorithm&quot;</span>:  <span class="hljs-string">&quot;movement_sparsity&quot;</span>,
        <span class="hljs-string">&quot;params&quot;</span>: {
            <span class="hljs-string">&quot;warmup_start_epoch&quot;</span>:  <span class="hljs-number">1</span>,
            <span class="hljs-string">&quot;warmup_end_epoch&quot;</span>:    <span class="hljs-number">4</span>,
            <span class="hljs-string">&quot;importance_regularization_factor&quot;</span>:  <span class="hljs-number">0.01</span>,
            <span class="hljs-string">&quot;enable_structured_masking&quot;</span>:  <span class="hljs-literal">True</span>
        },
        <span class="hljs-string">&quot;sparse_structure_by_scopes&quot;</span>: [
            {<span class="hljs-string">&quot;mode&quot;</span>:  <span class="hljs-string">&quot;block&quot;</span>,   <span class="hljs-string">&quot;sparse_factors&quot;</span>: [<span class="hljs-number">32</span>, <span class="hljs-number">32</span>], <span class="hljs-string">&quot;target_scopes&quot;</span>: <span class="hljs-string">&quot;{re}.*BertAttention.*&quot;</span>},
            {<span class="hljs-string">&quot;mode&quot;</span>:  <span class="hljs-string">&quot;per_dim&quot;</span>, <span class="hljs-string">&quot;axis&quot;</span>:  <span class="hljs-number">0</span>,                 <span class="hljs-string">&quot;target_scopes&quot;</span>: <span class="hljs-string">&quot;{re}.*BertIntermediate.*&quot;</span>},
            {<span class="hljs-string">&quot;mode&quot;</span>:  <span class="hljs-string">&quot;per_dim&quot;</span>, <span class="hljs-string">&quot;axis&quot;</span>:  <span class="hljs-number">1</span>,                 <span class="hljs-string">&quot;target_scopes&quot;</span>: <span class="hljs-string">&quot;{re}.*BertOutput.*&quot;</span>},
        ],
        <span class="hljs-string">&quot;ignored_scopes&quot;</span>: [<span class="hljs-string">&quot;{re}.*NNCFEmbedding&quot;</span>, <span class="hljs-string">&quot;{re}.*pooler.*&quot;</span>, <span class="hljs-string">&quot;{re}.*LayerNorm.*&quot;</span>]
        }
    },
    {
        <span class="hljs-string">&quot;algorithm&quot;</span>: <span class="hljs-string">&quot;quantization&quot;</span>,
        <span class="hljs-string">&quot;weights&quot;</span>: {<span class="hljs-string">&quot;mode&quot;</span>: <span class="hljs-string">&quot;symmetric&quot;</span>}
        <span class="hljs-string">&quot;activations&quot;</span>: { <span class="hljs-string">&quot;mode&quot;</span>: <span class="hljs-string">&quot;symmetric&quot;</span>},
    }
]`}}),ne=new xe({props:{code:"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",highlighted:`<span class="hljs-deletion">- from transformers import Trainer, TrainingArguments</span>
<span class="hljs-addition">+ from optimum.intel import OVConfig, OVTrainer, OVTrainingArguments</span>

# Load teacher model
<span class="hljs-addition">+ teacher_model = AutoModelForSequenceClassification.from_pretrained(teacher_model_or_path)</span>

<span class="hljs-deletion">- ov_config = OVConfig()</span>
<span class="hljs-addition">+ ov_config = OVConfig(compression=compression_config)</span>

trainer = OVTrainer(
    model=model,
<span class="hljs-addition">+   teacher_model=teacher_model,</span>
<span class="hljs-deletion">-   args=TrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=True),</span>
<span class="hljs-addition">+   args=OVTrainingArguments(save_dir, num_train_epochs=1.0, do_train=True, do_eval=True, distillation_temperature=3, distillation_weight=0.9),</span>
    train_dataset=dataset[&quot;train&quot;].select(range(300)),
    eval_dataset=dataset[&quot;validation&quot;],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
<span class="hljs-addition">+   ov_config=ov_config,</span>
    task=&quot;text-classification&quot;,
)

# Train the model like usual, internally the training is applied with pruning, quantization and distillation
train_result = trainer.train()
metrics = trainer.evaluate()
# Export the quantized model to OpenVINO IR format and save it
trainer.save_model()`}}),pe=new we({}),ce=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyaGVsZW5haSUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkLWZpbmV0dW5lZC1zc3QtMi1lbmdsaXNoLW92LWludDglMjIlMEFvdl9tb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEFjbHNfcGlwZSUyMCUzRCUyMHBpcGVsaW5lKCUyMnRleHQtY2xhc3NpZmljYXRpb24lMjIlMkMlMjBtb2RlbCUzRG92X21vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUwQXRleHQlMjAlM0QlMjAlMjJIZSdzJTIwYSUyMGRyZWFkZnVsJTIwbWFnaWNpYW4uJTIyJTBBb3V0cHV0cyUyMCUzRCUyMGNsc19waXBlKHRleHQpJTBBJTBBJTVCJTdCJ2xhYmVsJyUzQSUyMCdORUdBVElWRSclMkMlMjAnc2NvcmUnJTNBJTIwMC45ODQwMTk1MTc4OTg1NTk2JTdEJTVE",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForSequenceClassification

model_id = <span class="hljs-string">&quot;helenai/distilbert-base-uncased-finetuned-sst-2-english-ov-int8&quot;</span>
ov_model = OVModelForSequenceClassification.from_pretrained(model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)
cls_pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=ov_model, tokenizer=tokenizer)
text = <span class="hljs-string">&quot;He&#x27;s a dreadful magician.&quot;</span>
outputs = cls_pipe(text)

[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9840195178985596</span>}]`}}),{c(){v=i("meta"),$e=c(),C=i("h1"),z=i("a"),Ue=i("span"),y(H.$$.fragment),Et=c(),je=i("span"),zt=a("Optimization"),Oe=c(),T=i("p"),Bt=a("\u{1F917} Optimum Intel provides an "),Te=i("code"),Gt=a("openvino"),Rt=a(" package that enables you to apply a variety of model compression methods such as quantization, pruning, on many models hosted on the \u{1F917} hub using the "),x=i("a"),kt=a("NNCF"),Xt=a(" framework."),De=c(),I=i("h2"),B=i("a"),be=i("span"),y($.$$.fragment),_t=c(),ge=i("span"),qt=a("Post-training optimization"),Pe=c(),de=i("p"),Ft=a(`Post-training static quantization introduces an additional calibration step where data is fed through the network in order to compute the activations quantization parameters.
Here is how to apply static quantization on a fine-tuned DistilBERT:`),Le=c(),y(O.$$.fragment),Ke=c(),G=i("p"),Qt=a("The "),ve=i("code"),Nt=a("quantize()"),Yt=a(" method applies post-training static quantization and export the resulting quantized model to the OpenVINO Intermediate Representation (IR). The resulting graph is represented with two files: an XML file describing the network topology and a binary file describing the weights. The resulting model can be run on any target Intel device."),et=c(),Z=i("h2"),R=i("a"),Ce=i("span"),y(D.$$.fragment),St=c(),Ie=i("span"),Ht=a("Training-time optimization"),tt=c(),k=i("p"),xt=a("Apart from optimizing a model after training like post-training quantization above, "),Ze=i("code"),$t=a("optimum.openvino"),Ot=a(" also provides optimization methods during training, namely Quantization-Aware Training (QAT) and Joint Pruning, Quantization and Distillation (JPQD)."),lt=c(),W=i("h3"),X=i("a"),We=i("span"),y(P.$$.fragment),Dt=c(),Ve=i("span"),Pt=a("Quantization-Aware Training (QAT)"),at=c(),me=i("p"),Lt=a("QAT simulates the effects of quantization during training, in order to alleviate its effects on the model\u2019s accuracy. It is recommended in the case where post-training quantization results in high accuracy degradation. Here is an example on how to fine-tune a DistilBERT on the sst-2 task while applying quantization aware training (QAT)."),nt=c(),y(L.$$.fragment),it=c(),V=i("h3"),_=i("a"),Ae=i("span"),y(K.$$.fragment),Kt=c(),Ee=i("span"),el=a("Joint Pruning, Quantization and Distillation (JPQD)"),ot=c(),b=i("p"),tl=a("Other than quantization, compression methods like pruning and distillation are common in further improving the task performance and efficiency. Structured pruning slims a model for lower computational demands while distillation leverages knowledge of a teacher, usually, larger model to improve model prediction. Combining these methods with quantization can result in optimized model with significant efficiency improvement while enjoying good task accuracy retention. In "),ze=i("code"),ll=a("optimum.openvino"),al=a(", "),Be=i("code"),nl=a("OVTrainer"),il=a(" provides the capability to jointly prune, quantize and distill a model during training. Following is an example on how to perform the optimization on BERT-base for the sst-2 task."),st=c(),u=i("p"),ol=a("First, we create a config dictionary to specify the target algorithms. As "),Ge=i("code"),sl=a("optimum.openvino"),rl=a(" relies on NNCF as backend, the config format follows NNCF specifications (see "),ee=i("a"),pl=a("here"),Ml=a("). In the example config below, we specify pruning and quantization in a list of compression with thier hyperparameters. The pruning method closely resembles the work of "),te=i("a"),cl=a("Lagunas et al., 2021, Block Pruning For Faster Transformers"),dl=a(" whereas the quantization refers to QAT. With this configuration, the model under optimization will be initialized with pruning and quantization operators at the beginning of the training."),rt=c(),y(le.$$.fragment),pt=c(),he=i("blockquote"),A=i("p"),ml=a("Known limitation: Current structured pruning with movement sparsity only supports "),Re=i("em"),hl=a("BERT, Wav2vec2 and Swin"),ul=a(" family of models. See "),ae=i("a"),yl=a("here"),Jl=a(" for more information."),Mt=c(),m=i("p"),fl=a("Once we have the config ready, we can start develop the training pipeline like the snippet below. Since we are customizing joint compression with config above, notice that "),ke=i("code"),wl=a("OVConfig"),Ul=a(" is initialized with config dictionary (JSON parsing to python dictionary is skipped for brevity). As for distillation, users are required to load the teacher model, it is just like a normal model loading with transformers API. "),Xe=i("code"),jl=a("OVTrainingArguments"),Tl=a(" extends transformers\u2019 "),_e=i("code"),bl=a("TrainingArguments"),gl=a(" with distillation hyperparameters, i.e. distillation weightage and temperature for ease of use. The snippet below shows how we load a teacher model and create training arguments with "),qe=i("code"),vl=a("OVTrainingArguments"),Cl=a(". Subsequently, the teacher model, with the instantiated "),Fe=i("code"),Il=a("OVConfig"),Zl=a(" and "),Qe=i("code"),Wl=a("OVTrainingArguments"),Vl=a(" are fed to "),Ne=i("code"),Al=a("OVTrainer"),El=a(". Voila! that is all we need, the rest of the pipeline is identical to native transformers training."),ct=c(),y(ne.$$.fragment),dt=c(),q=i("p"),zl=a("More on the description and how to configure movement sparsity, see NNCF documentation "),ie=i("a"),Bl=a("here"),Gl=a("."),mt=c(),F=i("p"),Rl=a("More on available algorithms in NNCF, see documentation "),oe=i("a"),kl=a("here"),Xl=a("."),ht=c(),Q=i("p"),_l=a("For complete JPQD scripts, please refer to examples provided "),se=i("a"),ql=a("here"),Fl=a("."),ut=c(),N=i("p"),Ql=a("Quantization-Aware Training (QAT) and knowledge distillation can also be combined in order to optimize Stable Diffusion models while maintaining accuracy. For more details, take a look at this "),re=i("a"),Nl=a("blog post"),Yl=a("."),yt=c(),E=i("h2"),Y=i("a"),Ye=i("span"),y(pe.$$.fragment),Sl=c(),Se=i("span"),Hl=a("Inference with Transformers pipeline"),Jt=c(),g=i("p"),xl=a("After applying quantization on our model, we can then easily load it with our "),He=i("code"),$l=a("OVModelFor<Task>"),Ol=a(" classes and perform inference with OpenVINO Runtime using the Transformers "),Me=i("a"),Dl=a("pipelines"),Pl=a("."),ft=c(),y(ce.$$.fragment),this.h()},l(e){const r=xa('[data-svelte="svelte-1phssyn"]',document.head);v=o(r,"META",{name:!0,content:!0}),r.forEach(l),$e=d(e),C=o(e,"H1",{class:!0});var Ut=s(C);z=o(Ut,"A",{id:!0,class:!0,href:!0});var Kl=s(z);Ue=o(Kl,"SPAN",{});var ea=s(Ue);J(H.$$.fragment,ea),ea.forEach(l),Kl.forEach(l),Et=d(Ut),je=o(Ut,"SPAN",{});var ta=s(je);zt=n(ta,"Optimization"),ta.forEach(l),Ut.forEach(l),Oe=d(e),T=o(e,"P",{});var ue=s(T);Bt=n(ue,"\u{1F917} Optimum Intel provides an "),Te=o(ue,"CODE",{});var la=s(Te);Gt=n(la,"openvino"),la.forEach(l),Rt=n(ue," package that enables you to apply a variety of model compression methods such as quantization, pruning, on many models hosted on the \u{1F917} hub using the "),x=o(ue,"A",{href:!0,rel:!0});var aa=s(x);kt=n(aa,"NNCF"),aa.forEach(l),Xt=n(ue," framework."),ue.forEach(l),De=d(e),I=o(e,"H2",{class:!0});var jt=s(I);B=o(jt,"A",{id:!0,class:!0,href:!0});var na=s(B);be=o(na,"SPAN",{});var ia=s(be);J($.$$.fragment,ia),ia.forEach(l),na.forEach(l),_t=d(jt),ge=o(jt,"SPAN",{});var oa=s(ge);qt=n(oa,"Post-training optimization"),oa.forEach(l),jt.forEach(l),Pe=d(e),de=o(e,"P",{});var sa=s(de);Ft=n(sa,`Post-training static quantization introduces an additional calibration step where data is fed through the network in order to compute the activations quantization parameters.
Here is how to apply static quantization on a fine-tuned DistilBERT:`),sa.forEach(l),Le=d(e),J(O.$$.fragment,e),Ke=d(e),G=o(e,"P",{});var Tt=s(G);Qt=n(Tt,"The "),ve=o(Tt,"CODE",{});var ra=s(ve);Nt=n(ra,"quantize()"),ra.forEach(l),Yt=n(Tt," method applies post-training static quantization and export the resulting quantized model to the OpenVINO Intermediate Representation (IR). The resulting graph is represented with two files: an XML file describing the network topology and a binary file describing the weights. The resulting model can be run on any target Intel device."),Tt.forEach(l),et=d(e),Z=o(e,"H2",{class:!0});var bt=s(Z);R=o(bt,"A",{id:!0,class:!0,href:!0});var pa=s(R);Ce=o(pa,"SPAN",{});var Ma=s(Ce);J(D.$$.fragment,Ma),Ma.forEach(l),pa.forEach(l),St=d(bt),Ie=o(bt,"SPAN",{});var ca=s(Ie);Ht=n(ca,"Training-time optimization"),ca.forEach(l),bt.forEach(l),tt=d(e),k=o(e,"P",{});var gt=s(k);xt=n(gt,"Apart from optimizing a model after training like post-training quantization above, "),Ze=o(gt,"CODE",{});var da=s(Ze);$t=n(da,"optimum.openvino"),da.forEach(l),Ot=n(gt," also provides optimization methods during training, namely Quantization-Aware Training (QAT) and Joint Pruning, Quantization and Distillation (JPQD)."),gt.forEach(l),lt=d(e),W=o(e,"H3",{class:!0});var vt=s(W);X=o(vt,"A",{id:!0,class:!0,href:!0});var ma=s(X);We=o(ma,"SPAN",{});var ha=s(We);J(P.$$.fragment,ha),ha.forEach(l),ma.forEach(l),Dt=d(vt),Ve=o(vt,"SPAN",{});var ua=s(Ve);Pt=n(ua,"Quantization-Aware Training (QAT)"),ua.forEach(l),vt.forEach(l),at=d(e),me=o(e,"P",{});var ya=s(me);Lt=n(ya,"QAT simulates the effects of quantization during training, in order to alleviate its effects on the model\u2019s accuracy. It is recommended in the case where post-training quantization results in high accuracy degradation. Here is an example on how to fine-tune a DistilBERT on the sst-2 task while applying quantization aware training (QAT)."),ya.forEach(l),nt=d(e),J(L.$$.fragment,e),it=d(e),V=o(e,"H3",{class:!0});var Ct=s(V);_=o(Ct,"A",{id:!0,class:!0,href:!0});var Ja=s(_);Ae=o(Ja,"SPAN",{});var fa=s(Ae);J(K.$$.fragment,fa),fa.forEach(l),Ja.forEach(l),Kt=d(Ct),Ee=o(Ct,"SPAN",{});var wa=s(Ee);el=n(wa,"Joint Pruning, Quantization and Distillation (JPQD)"),wa.forEach(l),Ct.forEach(l),ot=d(e),b=o(e,"P",{});var ye=s(b);tl=n(ye,"Other than quantization, compression methods like pruning and distillation are common in further improving the task performance and efficiency. Structured pruning slims a model for lower computational demands while distillation leverages knowledge of a teacher, usually, larger model to improve model prediction. Combining these methods with quantization can result in optimized model with significant efficiency improvement while enjoying good task accuracy retention. In "),ze=o(ye,"CODE",{});var Ua=s(ze);ll=n(Ua,"optimum.openvino"),Ua.forEach(l),al=n(ye,", "),Be=o(ye,"CODE",{});var ja=s(Be);nl=n(ja,"OVTrainer"),ja.forEach(l),il=n(ye," provides the capability to jointly prune, quantize and distill a model during training. Following is an example on how to perform the optimization on BERT-base for the sst-2 task."),ye.forEach(l),st=d(e),u=o(e,"P",{});var S=s(u);ol=n(S,"First, we create a config dictionary to specify the target algorithms. As "),Ge=o(S,"CODE",{});var Ta=s(Ge);sl=n(Ta,"optimum.openvino"),Ta.forEach(l),rl=n(S," relies on NNCF as backend, the config format follows NNCF specifications (see "),ee=o(S,"A",{href:!0,rel:!0});var ba=s(ee);pl=n(ba,"here"),ba.forEach(l),Ml=n(S,"). In the example config below, we specify pruning and quantization in a list of compression with thier hyperparameters. The pruning method closely resembles the work of "),te=o(S,"A",{href:!0,rel:!0});var ga=s(te);cl=n(ga,"Lagunas et al., 2021, Block Pruning For Faster Transformers"),ga.forEach(l),dl=n(S," whereas the quantization refers to QAT. With this configuration, the model under optimization will be initialized with pruning and quantization operators at the beginning of the training."),S.forEach(l),rt=d(e),J(le.$$.fragment,e),pt=d(e),he=o(e,"BLOCKQUOTE",{});var va=s(he);A=o(va,"P",{});var Je=s(A);ml=n(Je,"Known limitation: Current structured pruning with movement sparsity only supports "),Re=o(Je,"EM",{});var Ca=s(Re);hl=n(Ca,"BERT, Wav2vec2 and Swin"),Ca.forEach(l),ul=n(Je," family of models. See "),ae=o(Je,"A",{href:!0,rel:!0});var Ia=s(ae);yl=n(Ia,"here"),Ia.forEach(l),Jl=n(Je," for more information."),Je.forEach(l),va.forEach(l),Mt=d(e),m=o(e,"P",{});var h=s(m);fl=n(h,"Once we have the config ready, we can start develop the training pipeline like the snippet below. Since we are customizing joint compression with config above, notice that "),ke=o(h,"CODE",{});var Za=s(ke);wl=n(Za,"OVConfig"),Za.forEach(l),Ul=n(h," is initialized with config dictionary (JSON parsing to python dictionary is skipped for brevity). As for distillation, users are required to load the teacher model, it is just like a normal model loading with transformers API. "),Xe=o(h,"CODE",{});var Wa=s(Xe);jl=n(Wa,"OVTrainingArguments"),Wa.forEach(l),Tl=n(h," extends transformers\u2019 "),_e=o(h,"CODE",{});var Va=s(_e);bl=n(Va,"TrainingArguments"),Va.forEach(l),gl=n(h," with distillation hyperparameters, i.e. distillation weightage and temperature for ease of use. The snippet below shows how we load a teacher model and create training arguments with "),qe=o(h,"CODE",{});var Aa=s(qe);vl=n(Aa,"OVTrainingArguments"),Aa.forEach(l),Cl=n(h,". Subsequently, the teacher model, with the instantiated "),Fe=o(h,"CODE",{});var Ea=s(Fe);Il=n(Ea,"OVConfig"),Ea.forEach(l),Zl=n(h," and "),Qe=o(h,"CODE",{});var za=s(Qe);Wl=n(za,"OVTrainingArguments"),za.forEach(l),Vl=n(h," are fed to "),Ne=o(h,"CODE",{});var Ba=s(Ne);Al=n(Ba,"OVTrainer"),Ba.forEach(l),El=n(h,". Voila! that is all we need, the rest of the pipeline is identical to native transformers training."),h.forEach(l),ct=d(e),J(ne.$$.fragment,e),dt=d(e),q=o(e,"P",{});var It=s(q);zl=n(It,"More on the description and how to configure movement sparsity, see NNCF documentation "),ie=o(It,"A",{href:!0,rel:!0});var Ga=s(ie);Bl=n(Ga,"here"),Ga.forEach(l),Gl=n(It,"."),It.forEach(l),mt=d(e),F=o(e,"P",{});var Zt=s(F);Rl=n(Zt,"More on available algorithms in NNCF, see documentation "),oe=o(Zt,"A",{href:!0,rel:!0});var Ra=s(oe);kl=n(Ra,"here"),Ra.forEach(l),Xl=n(Zt,"."),Zt.forEach(l),ht=d(e),Q=o(e,"P",{});var Wt=s(Q);_l=n(Wt,"For complete JPQD scripts, please refer to examples provided "),se=o(Wt,"A",{href:!0,rel:!0});var ka=s(se);ql=n(ka,"here"),ka.forEach(l),Fl=n(Wt,"."),Wt.forEach(l),ut=d(e),N=o(e,"P",{});var Vt=s(N);Ql=n(Vt,"Quantization-Aware Training (QAT) and knowledge distillation can also be combined in order to optimize Stable Diffusion models while maintaining accuracy. For more details, take a look at this "),re=o(Vt,"A",{href:!0,rel:!0});var Xa=s(re);Nl=n(Xa,"blog post"),Xa.forEach(l),Yl=n(Vt,"."),Vt.forEach(l),yt=d(e),E=o(e,"H2",{class:!0});var At=s(E);Y=o(At,"A",{id:!0,class:!0,href:!0});var _a=s(Y);Ye=o(_a,"SPAN",{});var qa=s(Ye);J(pe.$$.fragment,qa),qa.forEach(l),_a.forEach(l),Sl=d(At),Se=o(At,"SPAN",{});var Fa=s(Se);Hl=n(Fa,"Inference with Transformers pipeline"),Fa.forEach(l),At.forEach(l),Jt=d(e),g=o(e,"P",{});var fe=s(g);xl=n(fe,"After applying quantization on our model, we can then easily load it with our "),He=o(fe,"CODE",{});var Qa=s(He);$l=n(Qa,"OVModelFor<Task>"),Qa.forEach(l),Ol=n(fe," classes and perform inference with OpenVINO Runtime using the Transformers "),Me=o(fe,"A",{href:!0,rel:!0});var Na=s(Me);Dl=n(Na,"pipelines"),Na.forEach(l),Pl=n(fe,"."),fe.forEach(l),ft=d(e),J(ce.$$.fragment,e),this.h()},h(){M(v,"name","hf:doc:metadata"),M(v,"content",JSON.stringify(Pa)),M(z,"id","optimization"),M(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(z,"href","#optimization"),M(C,"class","relative group"),M(x,"href","https://docs.openvino.ai/2022.1/docs_nncf_introduction.html"),M(x,"rel","nofollow"),M(B,"id","posttraining-optimization"),M(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(B,"href","#posttraining-optimization"),M(I,"class","relative group"),M(R,"id","trainingtime-optimization"),M(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(R,"href","#trainingtime-optimization"),M(Z,"class","relative group"),M(X,"id","quantizationaware-training-qat"),M(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(X,"href","#quantizationaware-training-qat"),M(W,"class","relative group"),M(_,"id","joint-pruning-quantization-and-distillation-jpqd"),M(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(_,"href","#joint-pruning-quantization-and-distillation-jpqd"),M(V,"class","relative group"),M(ee,"href","https://github.com/openvinotoolkit/nncf/tree/develop/docs/compression_algorithms"),M(ee,"rel","nofollow"),M(te,"href","https://arxiv.org/pdf/2109.04838.pdf"),M(te,"rel","nofollow"),M(ae,"href","https://github.com/openvinotoolkit/nncf/blob/develop/nncf/experimental/torch/sparsity/movement/MovementSparsity.md"),M(ae,"rel","nofollow"),M(ie,"href","https://github.com/openvinotoolkit/nncf/blob/develop/nncf/experimental/torch/sparsity/movement/MovementSparsity.md"),M(ie,"rel","nofollow"),M(oe,"href","https://github.com/openvinotoolkit/nncf/tree/develop/docs/compression_algorithms"),M(oe,"rel","nofollow"),M(se,"href","https://github.com/huggingface/optimum-intel/tree/main/examples/openvino"),M(se,"rel","nofollow"),M(re,"href","https://huggingface.co/blog/train-optimize-sd-intel"),M(re,"rel","nofollow"),M(Y,"id","inference-with-transformers-pipeline"),M(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),M(Y,"href","#inference-with-transformers-pipeline"),M(E,"class","relative group"),M(Me,"href","https://huggingface.co/docs/transformers/main/en/main_classes/pipelines"),M(Me,"rel","nofollow")},m(e,r){t(document.head,v),p(e,$e,r),p(e,C,r),t(C,z),t(z,Ue),f(H,Ue,null),t(C,Et),t(C,je),t(je,zt),p(e,Oe,r),p(e,T,r),t(T,Bt),t(T,Te),t(Te,Gt),t(T,Rt),t(T,x),t(x,kt),t(T,Xt),p(e,De,r),p(e,I,r),t(I,B),t(B,be),f($,be,null),t(I,_t),t(I,ge),t(ge,qt),p(e,Pe,r),p(e,de,r),t(de,Ft),p(e,Le,r),f(O,e,r),p(e,Ke,r),p(e,G,r),t(G,Qt),t(G,ve),t(ve,Nt),t(G,Yt),p(e,et,r),p(e,Z,r),t(Z,R),t(R,Ce),f(D,Ce,null),t(Z,St),t(Z,Ie),t(Ie,Ht),p(e,tt,r),p(e,k,r),t(k,xt),t(k,Ze),t(Ze,$t),t(k,Ot),p(e,lt,r),p(e,W,r),t(W,X),t(X,We),f(P,We,null),t(W,Dt),t(W,Ve),t(Ve,Pt),p(e,at,r),p(e,me,r),t(me,Lt),p(e,nt,r),f(L,e,r),p(e,it,r),p(e,V,r),t(V,_),t(_,Ae),f(K,Ae,null),t(V,Kt),t(V,Ee),t(Ee,el),p(e,ot,r),p(e,b,r),t(b,tl),t(b,ze),t(ze,ll),t(b,al),t(b,Be),t(Be,nl),t(b,il),p(e,st,r),p(e,u,r),t(u,ol),t(u,Ge),t(Ge,sl),t(u,rl),t(u,ee),t(ee,pl),t(u,Ml),t(u,te),t(te,cl),t(u,dl),p(e,rt,r),f(le,e,r),p(e,pt,r),p(e,he,r),t(he,A),t(A,ml),t(A,Re),t(Re,hl),t(A,ul),t(A,ae),t(ae,yl),t(A,Jl),p(e,Mt,r),p(e,m,r),t(m,fl),t(m,ke),t(ke,wl),t(m,Ul),t(m,Xe),t(Xe,jl),t(m,Tl),t(m,_e),t(_e,bl),t(m,gl),t(m,qe),t(qe,vl),t(m,Cl),t(m,Fe),t(Fe,Il),t(m,Zl),t(m,Qe),t(Qe,Wl),t(m,Vl),t(m,Ne),t(Ne,Al),t(m,El),p(e,ct,r),f(ne,e,r),p(e,dt,r),p(e,q,r),t(q,zl),t(q,ie),t(ie,Bl),t(q,Gl),p(e,mt,r),p(e,F,r),t(F,Rl),t(F,oe),t(oe,kl),t(F,Xl),p(e,ht,r),p(e,Q,r),t(Q,_l),t(Q,se),t(se,ql),t(Q,Fl),p(e,ut,r),p(e,N,r),t(N,Ql),t(N,re),t(re,Nl),t(N,Yl),p(e,yt,r),p(e,E,r),t(E,Y),t(Y,Ye),f(pe,Ye,null),t(E,Sl),t(E,Se),t(Se,Hl),p(e,Jt,r),p(e,g,r),t(g,xl),t(g,He),t(He,$l),t(g,Ol),t(g,Me),t(Me,Dl),t(g,Pl),p(e,ft,r),f(ce,e,r),wt=!0},p:$a,i(e){wt||(w(H.$$.fragment,e),w($.$$.fragment,e),w(O.$$.fragment,e),w(D.$$.fragment,e),w(P.$$.fragment,e),w(L.$$.fragment,e),w(K.$$.fragment,e),w(le.$$.fragment,e),w(ne.$$.fragment,e),w(pe.$$.fragment,e),w(ce.$$.fragment,e),wt=!0)},o(e){U(H.$$.fragment,e),U($.$$.fragment,e),U(O.$$.fragment,e),U(D.$$.fragment,e),U(P.$$.fragment,e),U(L.$$.fragment,e),U(K.$$.fragment,e),U(le.$$.fragment,e),U(ne.$$.fragment,e),U(pe.$$.fragment,e),U(ce.$$.fragment,e),wt=!1},d(e){l(v),e&&l($e),e&&l(C),j(H),e&&l(Oe),e&&l(T),e&&l(De),e&&l(I),j($),e&&l(Pe),e&&l(de),e&&l(Le),j(O,e),e&&l(Ke),e&&l(G),e&&l(et),e&&l(Z),j(D),e&&l(tt),e&&l(k),e&&l(lt),e&&l(W),j(P),e&&l(at),e&&l(me),e&&l(nt),j(L,e),e&&l(it),e&&l(V),j(K),e&&l(ot),e&&l(b),e&&l(st),e&&l(u),e&&l(rt),j(le,e),e&&l(pt),e&&l(he),e&&l(Mt),e&&l(m),e&&l(ct),j(ne,e),e&&l(dt),e&&l(q),e&&l(mt),e&&l(F),e&&l(ht),e&&l(Q),e&&l(ut),e&&l(N),e&&l(yt),e&&l(E),j(pe),e&&l(Jt),e&&l(g),e&&l(ft),j(ce,e)}}}const Pa={local:"optimization",sections:[{local:"posttraining-optimization",title:"Post-training optimization"},{local:"trainingtime-optimization",sections:[{local:"quantizationaware-training-qat",title:"Quantization-Aware Training (QAT) "},{local:"joint-pruning-quantization-and-distillation-jpqd",title:"Joint Pruning, Quantization and Distillation (JPQD)"}],title:"Training-time optimization"},{local:"inference-with-transformers-pipeline",title:"Inference with Transformers pipeline"}],title:"Optimization"};function La(Ll){return Oa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ln extends Ya{constructor(v){super();Sa(this,v,La,Da,Ha,{})}}export{ln as default,Pa as metadata};
