import{S as an,i as nn,s as rn,e as a,k as l,w as d,t as c,M as ln,c as n,d as t,m as s,a as r,x as u,h,b as i,G as o,g as p,y as f,L as sn,q as g,o as v,B as _,v as mn}from"../chunks/vendor-hf-doc-builder.js";import{D as b}from"../chunks/Docstring-hf-doc-builder.js";import{I as y}from"../chunks/IconCopyLink-hf-doc-builder.js";function pn(la){let I,vt,M,D,Qe,oe,io,He,lo,_t,z,Q,Oe,ae,so,Ve,mo,bt,C,ne,po,We,co,uo,H,re,ho,ie,fo,Ue,go,vo,_o,O,le,bo,se,$o,Be,Co,No,$t,E,V,Re,me,yo,Ge,wo,Ct,$,pe,Io,Je,Mo,zo,W,de,Eo,je,qo,ko,U,ce,Fo,Ke,To,xo,B,ue,Po,he,So,Xe,Lo,Ao,Nt,q,R,Ye,fe,Do,Ze,Qo,yt,k,ge,Ho,G,ve,Oo,et,Vo,wt,F,J,tt,_e,Wo,ot,Uo,It,be,$e,Mt,T,j,at,Ce,Bo,nt,Ro,zt,Ne,ye,Et,x,K,rt,we,Go,it,Jo,qt,Ie,Me,kt,P,X,lt,ze,jo,st,Ko,Ft,Ee,qe,Tt,S,Y,mt,ke,Xo,pt,Yo,xt,Fe,Te,Pt,L,Z,dt,xe,Zo,ct,ea,St,N,Pe,ta,ut,oa,aa,ht,na,Lt,A,ee,ft,Se,ra,gt,ia,At,Le,Ae,Dt;return oe=new y({}),ae=new y({}),ne=new b({props:{name:"class optimum.intel.INCQuantizer",anchor:"optimum.intel.INCQuantizer",parameters:[{name:"model",val:": Module"},{name:"eval_fn",val:": typing.Union[typing.Callable[[transformers.modeling_utils.PreTrainedModel], int], NoneType] = None"},{name:"calibration_fn",val:": typing.Union[typing.Callable[[transformers.modeling_utils.PreTrainedModel], int], NoneType] = None"},{name:"task",val:": typing.Optional[str] = None"},{name:"seed",val:": int = 42"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L95"}}),re=new b({props:{name:"get_calibration_dataset",anchor:"optimum.intel.INCQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": str = 'train'"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"use_auth_token",val:": bool = False"}],parametersDescription:[{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
in generic formats and optionally a dataset script, if it requires some code to read the data files.`,name:"dataset_name"},{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, defaults to <code>&quot;train&quot;</code>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.intel.INCQuantizer.get_calibration_dataset.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the token generated when running <code>transformers-cli login</code>.`,name:"use_auth_token"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L314",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration step.</p>
`}}),le=new b({props:{name:"quantize",anchor:"optimum.intel.INCQuantizer.quantize",parameters:[{name:"quantization_config",val:": PostTrainingQuantConfig"},{name:"save_directory",val:": typing.Union[str, pathlib.Path]"},{name:"calibration_dataset",val:": Dataset = None"},{name:"batch_size",val:": int = 8"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"remove_unused_columns",val:": bool = True"},{name:"file_name",val:": str = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.INCQuantizer.quantize.quantization_config",description:`<strong>quantization_config</strong> (<code>PostTrainingQuantConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.intel.INCQuantizer.quantize.save_directory",description:`<strong>save_directory</strong> (<code>Union[str, Path]</code>) &#x2014;
The directory where the quantized model should be saved.`,name:"save_directory"},{anchor:"optimum.intel.INCQuantizer.quantize.calibration_dataset",description:`<strong>calibration_dataset</strong> (<code>datasets.Dataset</code>, defaults to <code>None</code>) &#x2014;
The dataset to use for the calibration step, needed for post-training static quantization.`,name:"calibration_dataset"},{anchor:"optimum.intel.INCQuantizer.quantize.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 8) &#x2014;
The number of calibration samples to load per batch.`,name:"batch_size"},{anchor:"optimum.intel.INCQuantizer.quantize.data_collator",description:`<strong>data_collator</strong> (<code>DataCollator</code>, defaults to <code>None</code>) &#x2014;
The function to use to form a batch from a list of elements of the calibration dataset.`,name:"data_collator"},{anchor:"optimum.intel.INCQuantizer.quantize.remove_unused_columns",description:`<strong>remove_unused_columns</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether or not to remove the columns unused by the model forward method.`,name:"remove_unused_columns"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L136"}}),me=new y({}),pe=new b({props:{name:"class optimum.intel.INCTrainer",anchor:"optimum.intel.INCTrainer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None"},{name:"args",val:": TrainingArguments = None"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"train_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"model_init",val:": typing.Callable[[], transformers.modeling_utils.PreTrainedModel] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"callbacks",val:": typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None"},{name:"optimizers",val:": typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)"},{name:"preprocess_logits_for_metrics",val:": typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None"},{name:"quantization_config",val:": typing.Optional[neural_compressor.conf.pythonic_config._BaseQuantizationConfig] = None"},{name:"pruning_config",val:": typing.Optional[neural_compressor.conf.pythonic_config._BaseQuantizationConfig] = None"},{name:"distillation_config",val:": typing.Optional[neural_compressor.conf.pythonic_config._BaseQuantizationConfig] = None"},{name:"task",val:": typing.Optional[str] = None"},{name:"save_onnx_model",val:": bool = False"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/trainer.py#L88"}}),de=new b({props:{name:"compute_distillation_loss",anchor:"optimum.intel.INCTrainer.compute_distillation_loss",parameters:[{name:"student_outputs",val:""},{name:"teacher_outputs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/trainer.py#L772"}}),ce=new b({props:{name:"compute_loss",anchor:"optimum.intel.INCTrainer.compute_loss",parameters:[{name:"model",val:""},{name:"inputs",val:""},{name:"return_outputs",val:" = False"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/trainer.py#L696"}}),ue=new b({props:{name:"save_model",anchor:"optimum.intel.INCTrainer.save_model",parameters:[{name:"output_dir",val:": typing.Optional[str] = None"},{name:"_internal_call",val:": bool = False"},{name:"save_onnx_model",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/trainer.py#L546"}}),fe=new y({}),ge=new b({props:{name:"class optimum.intel.neural_compressor.INCModel",anchor:"optimum.intel.neural_compressor.INCModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L458"}}),ve=new b({props:{name:"from_pretrained",anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained",parameters:[{name:"model_name_or_path",val:": str"},{name:"q_model_name",val:": typing.Optional[str] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.model_name_or_path",description:`<strong>model_name_or_path</strong> (<em>str</em>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.q_model_name",description:`<strong>q_model_name</strong> (<em>str</em>, <em>optional</em>) &#x2014;
Name of the state dictionary located in model_name_or_path used to load the quantized model. If
state_dict is specified, the latter will not be used.`,name:"q_model_name"},{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<em>str</em>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded configuration should be cached if the standard cache should
not be used.`,name:"cache_dir"},{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<em>bool</em>, <em>optional</em>, defaults to <em>False</em>) &#x2014;
Whether or not to force to (re-)download the configuration files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<em>bool</em>, <em>optional</em>, defaults to <em>False</em>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.revision(str,",description:`<strong>revision(<em>str</em>,</strong> <em>optional</em>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"optimum.intel.neural_compressor.INCModel.from_pretrained.state_dict_path",description:`<strong>state_dict_path</strong> (<em>str</em>, <em>optional</em>) &#x2014;
The path to the state dictionary of the quantized model.`,name:"state_dict_path"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L467",returnDescription:`
<p>Quantized model.</p>
`,returnType:`
<p>q_model</p>
`}}),_e=new y({}),$e=new b({props:{name:"class optimum.intel.INCModelForSequenceClassification",anchor:"optimum.intel.INCModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L621"}}),Ce=new y({}),ye=new b({props:{name:"class optimum.intel.INCModelForQuestionAnswering",anchor:"optimum.intel.INCModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L617"}}),we=new y({}),Me=new b({props:{name:"class optimum.intel.INCModelForTokenClassification",anchor:"optimum.intel.INCModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L625"}}),ze=new y({}),qe=new b({props:{name:"class optimum.intel.INCModelForMultipleChoice",anchor:"optimum.intel.INCModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L629"}}),ke=new y({}),Te=new b({props:{name:"class optimum.intel.INCModelForMaskedLM",anchor:"optimum.intel.INCModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L637"}}),xe=new y({}),Pe=new b({props:{name:"class optimum.intel.INCModelForCausalLM",anchor:"optimum.intel.INCModelForCausalLM",parameters:[{name:"model",val:""},{name:"config",val:": PretrainedConfig = None"},{name:"model_save_dir",val:": typing.Union[str, pathlib.Path, tempfile.TemporaryDirectory, NoneType] = None"},{name:"use_cache",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.INCModelForCausalLM.model",description:"<strong>model</strong> (<code>PyTorch model</code>) &#x2014; is the main class used to run inference.",name:"model"},{anchor:"optimum.intel.INCModelForCausalLM.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.`,name:"config"},{anchor:"optimum.intel.INCModelForCausalLM.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;cpu&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/modeling_decoder.py#L38"}}),Se=new y({}),Ae=new b({props:{name:"class optimum.intel.INCModelForSeq2SeqLM",anchor:"optimum.intel.INCModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/neural_compressor/quantization.py#L633"}}),{c(){I=a("meta"),vt=l(),M=a("h1"),D=a("a"),Qe=a("span"),d(oe.$$.fragment),io=l(),He=a("span"),lo=c("Reference"),_t=l(),z=a("h2"),Q=a("a"),Oe=a("span"),d(ae.$$.fragment),so=l(),Ve=a("span"),mo=c("INCQuantizer"),bt=l(),C=a("div"),d(ne.$$.fragment),po=l(),We=a("p"),co=c("Handle the Neural Compressor quantization process."),uo=l(),H=a("div"),d(re.$$.fragment),ho=l(),ie=a("p"),fo=c("Create the calibration "),Ue=a("code"),go=c("datasets.Dataset"),vo=c(" to use for the post-training static quantization calibration step."),_o=l(),O=a("div"),d(le.$$.fragment),bo=l(),se=a("p"),$o=c("Quantize a model given the optimization specifications defined in "),Be=a("code"),Co=c("quantization_config"),No=c("."),$t=l(),E=a("h2"),V=a("a"),Re=a("span"),d(me.$$.fragment),yo=l(),Ge=a("span"),wo=c("INCTrainer"),Ct=l(),$=a("div"),d(pe.$$.fragment),Io=l(),Je=a("p"),Mo=c("INCTrainer enables Intel Neural Compression quantization aware training, pruning and distillation."),zo=l(),W=a("div"),d(de.$$.fragment),Eo=l(),je=a("p"),qo=c("How the distillation loss is computed given the student and teacher outputs."),ko=l(),U=a("div"),d(ce.$$.fragment),Fo=l(),Ke=a("p"),To=c("How the loss is computed by Trainer. By default, all models return the loss in the first element."),xo=l(),B=a("div"),d(ue.$$.fragment),Po=l(),he=a("p"),So=c("Will save the model, so you can reload it using "),Xe=a("code"),Lo=c("from_pretrained()"),Ao=c(`.
Will only save from the main process.`),Nt=l(),q=a("h2"),R=a("a"),Ye=a("span"),d(fe.$$.fragment),Do=l(),Ze=a("span"),Qo=c("INCModel"),yt=l(),k=a("div"),d(ge.$$.fragment),Ho=l(),G=a("div"),d(ve.$$.fragment),Oo=l(),et=a("p"),Vo=c("Instantiate a quantized pytorch model from a given Intel Neural Compressor configuration file."),wt=l(),F=a("h2"),J=a("a"),tt=a("span"),d(_e.$$.fragment),Wo=l(),ot=a("span"),Uo=c("INCModelForSequenceClassification"),It=l(),be=a("div"),d($e.$$.fragment),Mt=l(),T=a("h2"),j=a("a"),at=a("span"),d(Ce.$$.fragment),Bo=l(),nt=a("span"),Ro=c("INCModelForQuestionAnswering"),zt=l(),Ne=a("div"),d(ye.$$.fragment),Et=l(),x=a("h2"),K=a("a"),rt=a("span"),d(we.$$.fragment),Go=l(),it=a("span"),Jo=c("INCModelForTokenClassification"),qt=l(),Ie=a("div"),d(Me.$$.fragment),kt=l(),P=a("h2"),X=a("a"),lt=a("span"),d(ze.$$.fragment),jo=l(),st=a("span"),Ko=c("INCModelForMultipleChoice"),Ft=l(),Ee=a("div"),d(qe.$$.fragment),Tt=l(),S=a("h2"),Y=a("a"),mt=a("span"),d(ke.$$.fragment),Xo=l(),pt=a("span"),Yo=c("INCModelForMaskedLM"),xt=l(),Fe=a("div"),d(Te.$$.fragment),Pt=l(),L=a("h2"),Z=a("a"),dt=a("span"),d(xe.$$.fragment),Zo=l(),ct=a("span"),ea=c("INCModelForCausalLM"),St=l(),N=a("div"),d(Pe.$$.fragment),ta=l(),ut=a("p"),oa=c(`Neural-compressor Model with a causal language modeling head on top (linear layer with weights tied to the input
embeddings).`),aa=l(),ht=a("p"),na=c(`This model check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Lt=l(),A=a("h2"),ee=a("a"),ft=a("span"),d(Se.$$.fragment),ra=l(),gt=a("span"),ia=c("INCModelForSeq2SeqLM"),At=l(),Le=a("div"),d(Ae.$$.fragment),this.h()},l(e){const m=ln('[data-svelte="svelte-1phssyn"]',document.head);I=n(m,"META",{name:!0,content:!0}),m.forEach(t),vt=s(e),M=n(e,"H1",{class:!0});var Qt=r(M);D=n(Qt,"A",{id:!0,class:!0,href:!0});var sa=r(D);Qe=n(sa,"SPAN",{});var ma=r(Qe);u(oe.$$.fragment,ma),ma.forEach(t),sa.forEach(t),io=s(Qt),He=n(Qt,"SPAN",{});var pa=r(He);lo=h(pa,"Reference"),pa.forEach(t),Qt.forEach(t),_t=s(e),z=n(e,"H2",{class:!0});var Ht=r(z);Q=n(Ht,"A",{id:!0,class:!0,href:!0});var da=r(Q);Oe=n(da,"SPAN",{});var ca=r(Oe);u(ae.$$.fragment,ca),ca.forEach(t),da.forEach(t),so=s(Ht),Ve=n(Ht,"SPAN",{});var ua=r(Ve);mo=h(ua,"INCQuantizer"),ua.forEach(t),Ht.forEach(t),bt=s(e),C=n(e,"DIV",{class:!0});var te=r(C);u(ne.$$.fragment,te),po=s(te),We=n(te,"P",{});var ha=r(We);co=h(ha,"Handle the Neural Compressor quantization process."),ha.forEach(t),uo=s(te),H=n(te,"DIV",{class:!0});var Ot=r(H);u(re.$$.fragment,Ot),ho=s(Ot),ie=n(Ot,"P",{});var Vt=r(ie);fo=h(Vt,"Create the calibration "),Ue=n(Vt,"CODE",{});var fa=r(Ue);go=h(fa,"datasets.Dataset"),fa.forEach(t),vo=h(Vt," to use for the post-training static quantization calibration step."),Vt.forEach(t),Ot.forEach(t),_o=s(te),O=n(te,"DIV",{class:!0});var Wt=r(O);u(le.$$.fragment,Wt),bo=s(Wt),se=n(Wt,"P",{});var Ut=r(se);$o=h(Ut,"Quantize a model given the optimization specifications defined in "),Be=n(Ut,"CODE",{});var ga=r(Be);Co=h(ga,"quantization_config"),ga.forEach(t),No=h(Ut,"."),Ut.forEach(t),Wt.forEach(t),te.forEach(t),$t=s(e),E=n(e,"H2",{class:!0});var Bt=r(E);V=n(Bt,"A",{id:!0,class:!0,href:!0});var va=r(V);Re=n(va,"SPAN",{});var _a=r(Re);u(me.$$.fragment,_a),_a.forEach(t),va.forEach(t),yo=s(Bt),Ge=n(Bt,"SPAN",{});var ba=r(Ge);wo=h(ba,"INCTrainer"),ba.forEach(t),Bt.forEach(t),Ct=s(e),$=n(e,"DIV",{class:!0});var w=r($);u(pe.$$.fragment,w),Io=s(w),Je=n(w,"P",{});var $a=r(Je);Mo=h($a,"INCTrainer enables Intel Neural Compression quantization aware training, pruning and distillation."),$a.forEach(t),zo=s(w),W=n(w,"DIV",{class:!0});var Rt=r(W);u(de.$$.fragment,Rt),Eo=s(Rt),je=n(Rt,"P",{});var Ca=r(je);qo=h(Ca,"How the distillation loss is computed given the student and teacher outputs."),Ca.forEach(t),Rt.forEach(t),ko=s(w),U=n(w,"DIV",{class:!0});var Gt=r(U);u(ce.$$.fragment,Gt),Fo=s(Gt),Ke=n(Gt,"P",{});var Na=r(Ke);To=h(Na,"How the loss is computed by Trainer. By default, all models return the loss in the first element."),Na.forEach(t),Gt.forEach(t),xo=s(w),B=n(w,"DIV",{class:!0});var Jt=r(B);u(ue.$$.fragment,Jt),Po=s(Jt),he=n(Jt,"P",{});var jt=r(he);So=h(jt,"Will save the model, so you can reload it using "),Xe=n(jt,"CODE",{});var ya=r(Xe);Lo=h(ya,"from_pretrained()"),ya.forEach(t),Ao=h(jt,`.
Will only save from the main process.`),jt.forEach(t),Jt.forEach(t),w.forEach(t),Nt=s(e),q=n(e,"H2",{class:!0});var Kt=r(q);R=n(Kt,"A",{id:!0,class:!0,href:!0});var wa=r(R);Ye=n(wa,"SPAN",{});var Ia=r(Ye);u(fe.$$.fragment,Ia),Ia.forEach(t),wa.forEach(t),Do=s(Kt),Ze=n(Kt,"SPAN",{});var Ma=r(Ze);Qo=h(Ma,"INCModel"),Ma.forEach(t),Kt.forEach(t),yt=s(e),k=n(e,"DIV",{class:!0});var Xt=r(k);u(ge.$$.fragment,Xt),Ho=s(Xt),G=n(Xt,"DIV",{class:!0});var Yt=r(G);u(ve.$$.fragment,Yt),Oo=s(Yt),et=n(Yt,"P",{});var za=r(et);Vo=h(za,"Instantiate a quantized pytorch model from a given Intel Neural Compressor configuration file."),za.forEach(t),Yt.forEach(t),Xt.forEach(t),wt=s(e),F=n(e,"H2",{class:!0});var Zt=r(F);J=n(Zt,"A",{id:!0,class:!0,href:!0});var Ea=r(J);tt=n(Ea,"SPAN",{});var qa=r(tt);u(_e.$$.fragment,qa),qa.forEach(t),Ea.forEach(t),Wo=s(Zt),ot=n(Zt,"SPAN",{});var ka=r(ot);Uo=h(ka,"INCModelForSequenceClassification"),ka.forEach(t),Zt.forEach(t),It=s(e),be=n(e,"DIV",{class:!0});var Fa=r(be);u($e.$$.fragment,Fa),Fa.forEach(t),Mt=s(e),T=n(e,"H2",{class:!0});var eo=r(T);j=n(eo,"A",{id:!0,class:!0,href:!0});var Ta=r(j);at=n(Ta,"SPAN",{});var xa=r(at);u(Ce.$$.fragment,xa),xa.forEach(t),Ta.forEach(t),Bo=s(eo),nt=n(eo,"SPAN",{});var Pa=r(nt);Ro=h(Pa,"INCModelForQuestionAnswering"),Pa.forEach(t),eo.forEach(t),zt=s(e),Ne=n(e,"DIV",{class:!0});var Sa=r(Ne);u(ye.$$.fragment,Sa),Sa.forEach(t),Et=s(e),x=n(e,"H2",{class:!0});var to=r(x);K=n(to,"A",{id:!0,class:!0,href:!0});var La=r(K);rt=n(La,"SPAN",{});var Aa=r(rt);u(we.$$.fragment,Aa),Aa.forEach(t),La.forEach(t),Go=s(to),it=n(to,"SPAN",{});var Da=r(it);Jo=h(Da,"INCModelForTokenClassification"),Da.forEach(t),to.forEach(t),qt=s(e),Ie=n(e,"DIV",{class:!0});var Qa=r(Ie);u(Me.$$.fragment,Qa),Qa.forEach(t),kt=s(e),P=n(e,"H2",{class:!0});var oo=r(P);X=n(oo,"A",{id:!0,class:!0,href:!0});var Ha=r(X);lt=n(Ha,"SPAN",{});var Oa=r(lt);u(ze.$$.fragment,Oa),Oa.forEach(t),Ha.forEach(t),jo=s(oo),st=n(oo,"SPAN",{});var Va=r(st);Ko=h(Va,"INCModelForMultipleChoice"),Va.forEach(t),oo.forEach(t),Ft=s(e),Ee=n(e,"DIV",{class:!0});var Wa=r(Ee);u(qe.$$.fragment,Wa),Wa.forEach(t),Tt=s(e),S=n(e,"H2",{class:!0});var ao=r(S);Y=n(ao,"A",{id:!0,class:!0,href:!0});var Ua=r(Y);mt=n(Ua,"SPAN",{});var Ba=r(mt);u(ke.$$.fragment,Ba),Ba.forEach(t),Ua.forEach(t),Xo=s(ao),pt=n(ao,"SPAN",{});var Ra=r(pt);Yo=h(Ra,"INCModelForMaskedLM"),Ra.forEach(t),ao.forEach(t),xt=s(e),Fe=n(e,"DIV",{class:!0});var Ga=r(Fe);u(Te.$$.fragment,Ga),Ga.forEach(t),Pt=s(e),L=n(e,"H2",{class:!0});var no=r(L);Z=n(no,"A",{id:!0,class:!0,href:!0});var Ja=r(Z);dt=n(Ja,"SPAN",{});var ja=r(dt);u(xe.$$.fragment,ja),ja.forEach(t),Ja.forEach(t),Zo=s(no),ct=n(no,"SPAN",{});var Ka=r(ct);ea=h(Ka,"INCModelForCausalLM"),Ka.forEach(t),no.forEach(t),St=s(e),N=n(e,"DIV",{class:!0});var De=r(N);u(Pe.$$.fragment,De),ta=s(De),ut=n(De,"P",{});var Xa=r(ut);oa=h(Xa,`Neural-compressor Model with a causal language modeling head on top (linear layer with weights tied to the input
embeddings).`),Xa.forEach(t),aa=s(De),ht=n(De,"P",{});var Ya=r(ht);na=h(Ya,`This model check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Ya.forEach(t),De.forEach(t),Lt=s(e),A=n(e,"H2",{class:!0});var ro=r(A);ee=n(ro,"A",{id:!0,class:!0,href:!0});var Za=r(ee);ft=n(Za,"SPAN",{});var en=r(ft);u(Se.$$.fragment,en),en.forEach(t),Za.forEach(t),ra=s(ro),gt=n(ro,"SPAN",{});var tn=r(gt);ia=h(tn,"INCModelForSeq2SeqLM"),tn.forEach(t),ro.forEach(t),At=s(e),Le=n(e,"DIV",{class:!0});var on=r(Le);u(Ae.$$.fragment,on),on.forEach(t),this.h()},h(){i(I,"name","hf:doc:metadata"),i(I,"content",JSON.stringify(dn)),i(D,"id","reference"),i(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(D,"href","#reference"),i(M,"class","relative group"),i(Q,"id","optimum.intel.INCQuantizer"),i(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Q,"href","#optimum.intel.INCQuantizer"),i(z,"class","relative group"),i(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(V,"id","optimum.intel.INCTrainer"),i(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(V,"href","#optimum.intel.INCTrainer"),i(E,"class","relative group"),i(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(R,"id","optimum.intel.neural_compressor.INCModel"),i(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(R,"href","#optimum.intel.neural_compressor.INCModel"),i(q,"class","relative group"),i(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(J,"id","optimum.intel.INCModelForSequenceClassification"),i(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(J,"href","#optimum.intel.INCModelForSequenceClassification"),i(F,"class","relative group"),i(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(j,"id","optimum.intel.INCModelForQuestionAnswering"),i(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(j,"href","#optimum.intel.INCModelForQuestionAnswering"),i(T,"class","relative group"),i(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(K,"id","optimum.intel.INCModelForTokenClassification"),i(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(K,"href","#optimum.intel.INCModelForTokenClassification"),i(x,"class","relative group"),i(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(X,"id","optimum.intel.INCModelForMultipleChoice"),i(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(X,"href","#optimum.intel.INCModelForMultipleChoice"),i(P,"class","relative group"),i(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Y,"id","optimum.intel.INCModelForMaskedLM"),i(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Y,"href","#optimum.intel.INCModelForMaskedLM"),i(S,"class","relative group"),i(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Z,"id","optimum.intel.INCModelForCausalLM"),i(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Z,"href","#optimum.intel.INCModelForCausalLM"),i(L,"class","relative group"),i(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ee,"id","optimum.intel.INCModelForSeq2SeqLM"),i(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ee,"href","#optimum.intel.INCModelForSeq2SeqLM"),i(A,"class","relative group"),i(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,m){o(document.head,I),p(e,vt,m),p(e,M,m),o(M,D),o(D,Qe),f(oe,Qe,null),o(M,io),o(M,He),o(He,lo),p(e,_t,m),p(e,z,m),o(z,Q),o(Q,Oe),f(ae,Oe,null),o(z,so),o(z,Ve),o(Ve,mo),p(e,bt,m),p(e,C,m),f(ne,C,null),o(C,po),o(C,We),o(We,co),o(C,uo),o(C,H),f(re,H,null),o(H,ho),o(H,ie),o(ie,fo),o(ie,Ue),o(Ue,go),o(ie,vo),o(C,_o),o(C,O),f(le,O,null),o(O,bo),o(O,se),o(se,$o),o(se,Be),o(Be,Co),o(se,No),p(e,$t,m),p(e,E,m),o(E,V),o(V,Re),f(me,Re,null),o(E,yo),o(E,Ge),o(Ge,wo),p(e,Ct,m),p(e,$,m),f(pe,$,null),o($,Io),o($,Je),o(Je,Mo),o($,zo),o($,W),f(de,W,null),o(W,Eo),o(W,je),o(je,qo),o($,ko),o($,U),f(ce,U,null),o(U,Fo),o(U,Ke),o(Ke,To),o($,xo),o($,B),f(ue,B,null),o(B,Po),o(B,he),o(he,So),o(he,Xe),o(Xe,Lo),o(he,Ao),p(e,Nt,m),p(e,q,m),o(q,R),o(R,Ye),f(fe,Ye,null),o(q,Do),o(q,Ze),o(Ze,Qo),p(e,yt,m),p(e,k,m),f(ge,k,null),o(k,Ho),o(k,G),f(ve,G,null),o(G,Oo),o(G,et),o(et,Vo),p(e,wt,m),p(e,F,m),o(F,J),o(J,tt),f(_e,tt,null),o(F,Wo),o(F,ot),o(ot,Uo),p(e,It,m),p(e,be,m),f($e,be,null),p(e,Mt,m),p(e,T,m),o(T,j),o(j,at),f(Ce,at,null),o(T,Bo),o(T,nt),o(nt,Ro),p(e,zt,m),p(e,Ne,m),f(ye,Ne,null),p(e,Et,m),p(e,x,m),o(x,K),o(K,rt),f(we,rt,null),o(x,Go),o(x,it),o(it,Jo),p(e,qt,m),p(e,Ie,m),f(Me,Ie,null),p(e,kt,m),p(e,P,m),o(P,X),o(X,lt),f(ze,lt,null),o(P,jo),o(P,st),o(st,Ko),p(e,Ft,m),p(e,Ee,m),f(qe,Ee,null),p(e,Tt,m),p(e,S,m),o(S,Y),o(Y,mt),f(ke,mt,null),o(S,Xo),o(S,pt),o(pt,Yo),p(e,xt,m),p(e,Fe,m),f(Te,Fe,null),p(e,Pt,m),p(e,L,m),o(L,Z),o(Z,dt),f(xe,dt,null),o(L,Zo),o(L,ct),o(ct,ea),p(e,St,m),p(e,N,m),f(Pe,N,null),o(N,ta),o(N,ut),o(ut,oa),o(N,aa),o(N,ht),o(ht,na),p(e,Lt,m),p(e,A,m),o(A,ee),o(ee,ft),f(Se,ft,null),o(A,ra),o(A,gt),o(gt,ia),p(e,At,m),p(e,Le,m),f(Ae,Le,null),Dt=!0},p:sn,i(e){Dt||(g(oe.$$.fragment,e),g(ae.$$.fragment,e),g(ne.$$.fragment,e),g(re.$$.fragment,e),g(le.$$.fragment,e),g(me.$$.fragment,e),g(pe.$$.fragment,e),g(de.$$.fragment,e),g(ce.$$.fragment,e),g(ue.$$.fragment,e),g(fe.$$.fragment,e),g(ge.$$.fragment,e),g(ve.$$.fragment,e),g(_e.$$.fragment,e),g($e.$$.fragment,e),g(Ce.$$.fragment,e),g(ye.$$.fragment,e),g(we.$$.fragment,e),g(Me.$$.fragment,e),g(ze.$$.fragment,e),g(qe.$$.fragment,e),g(ke.$$.fragment,e),g(Te.$$.fragment,e),g(xe.$$.fragment,e),g(Pe.$$.fragment,e),g(Se.$$.fragment,e),g(Ae.$$.fragment,e),Dt=!0)},o(e){v(oe.$$.fragment,e),v(ae.$$.fragment,e),v(ne.$$.fragment,e),v(re.$$.fragment,e),v(le.$$.fragment,e),v(me.$$.fragment,e),v(pe.$$.fragment,e),v(de.$$.fragment,e),v(ce.$$.fragment,e),v(ue.$$.fragment,e),v(fe.$$.fragment,e),v(ge.$$.fragment,e),v(ve.$$.fragment,e),v(_e.$$.fragment,e),v($e.$$.fragment,e),v(Ce.$$.fragment,e),v(ye.$$.fragment,e),v(we.$$.fragment,e),v(Me.$$.fragment,e),v(ze.$$.fragment,e),v(qe.$$.fragment,e),v(ke.$$.fragment,e),v(Te.$$.fragment,e),v(xe.$$.fragment,e),v(Pe.$$.fragment,e),v(Se.$$.fragment,e),v(Ae.$$.fragment,e),Dt=!1},d(e){t(I),e&&t(vt),e&&t(M),_(oe),e&&t(_t),e&&t(z),_(ae),e&&t(bt),e&&t(C),_(ne),_(re),_(le),e&&t($t),e&&t(E),_(me),e&&t(Ct),e&&t($),_(pe),_(de),_(ce),_(ue),e&&t(Nt),e&&t(q),_(fe),e&&t(yt),e&&t(k),_(ge),_(ve),e&&t(wt),e&&t(F),_(_e),e&&t(It),e&&t(be),_($e),e&&t(Mt),e&&t(T),_(Ce),e&&t(zt),e&&t(Ne),_(ye),e&&t(Et),e&&t(x),_(we),e&&t(qt),e&&t(Ie),_(Me),e&&t(kt),e&&t(P),_(ze),e&&t(Ft),e&&t(Ee),_(qe),e&&t(Tt),e&&t(S),_(ke),e&&t(xt),e&&t(Fe),_(Te),e&&t(Pt),e&&t(L),_(xe),e&&t(St),e&&t(N),_(Pe),e&&t(Lt),e&&t(A),_(Se),e&&t(At),e&&t(Le),_(Ae)}}}const dn={local:"reference",sections:[{local:"optimum.intel.INCQuantizer",title:"INCQuantizer"},{local:"optimum.intel.INCTrainer",title:"INCTrainer"},{local:"optimum.intel.neural_compressor.INCModel",title:"INCModel"},{local:"optimum.intel.INCModelForSequenceClassification",title:"INCModelForSequenceClassification"},{local:"optimum.intel.INCModelForQuestionAnswering",title:"INCModelForQuestionAnswering"},{local:"optimum.intel.INCModelForTokenClassification",title:"INCModelForTokenClassification"},{local:"optimum.intel.INCModelForMultipleChoice",title:"INCModelForMultipleChoice"},{local:"optimum.intel.INCModelForMaskedLM",title:"INCModelForMaskedLM"},{local:"optimum.intel.INCModelForCausalLM",title:"INCModelForCausalLM"},{local:"optimum.intel.INCModelForSeq2SeqLM",title:"INCModelForSeq2SeqLM"}],title:"Reference"};function cn(la){return mn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gn extends an{constructor(I){super();nn(this,I,cn,pn,rn,{})}}export{gn as default,dn as metadata};
