import{S as hc,i as uc,s as fc,F as Np,e as l,c as i,a as r,d as o,b,g as v,H as Bp,I as zp,J as Sp,q as $,o as k,w as V,k as g,x as T,m as M,G as e,y as F,P as Qp,B as C,v as Ap,Z as Yp,_ as Lp,t as d,M as Dp,h as c,L as W}from"../chunks/vendor-hf-doc-builder.js";import{D as O}from"../chunks/Docstring-hf-doc-builder.js";import{C as I}from"../chunks/CodeBlock-hf-doc-builder.js";import{I as q}from"../chunks/IconCopyLink-hf-doc-builder.js";function Pp(J){let n,f;const p=J[3].default,m=Np(p,J,J[2],null);return{c(){n=l("div"),m&&m.c(),this.h()},l(u){n=i(u,"DIV",{class:!0});var s=r(n);m&&m.l(s),s.forEach(o),this.h()},h(){b(n,"class","course-tip "+(J[0]==="orange"?"course-tip-orange":"")+" bg-gradient-to-br dark:bg-gradient-to-r before:border-"+J[0]+"-500 dark:before:border-"+J[0]+"-800 from-"+J[0]+"-50 dark:from-gray-900 to-white dark:to-gray-950 border border-"+J[0]+"-50 text-"+J[0]+"-700 dark:text-gray-400")},m(u,s){v(u,n,s),m&&m.m(n,null),f=!0},p(u,[s]){m&&m.p&&(!f||s&4)&&Bp(m,p,u,u[2],f?Sp(p,u[2],s,null):zp(u[2]),null)},i(u){f||($(m,u),f=!0)},o(u){k(m,u),f=!1},d(u){u&&o(n),m&&m.d(u)}}}function Hp(J,n,f){let{$$slots:p={},$$scope:m}=n,{warning:u=!1}=n;const s=u?"orange":"green";return J.$$set=a=>{"warning"in a&&f(1,u=a.warning),"$$scope"in a&&f(2,m=a.$$scope)},[s,u,m,p]}class de extends hc{constructor(n){super();uc(this,n,Hp,Pp,fc,{warning:1})}}const{window:Kp}=Yp;function em(J){let n,f,p,m,u,s,a,w,h;m=new q({props:{classNames:"text-smd"}});const _=J[4].default,j=Np(_,J,J[3],null);return{c(){n=l("div"),f=l("a"),p=l("span"),V(m.$$.fragment),s=g(),j&&j.c(),this.h()},l(Z){n=i(Z,"DIV",{class:!0});var U=r(n);f=i(U,"A",{id:!0,class:!0,href:!0});var fe=r(f);p=i(fe,"SPAN",{});var nn=r(p);T(m.$$.fragment,nn),nn.forEach(o),fe.forEach(o),s=M(U),j&&j.l(U),U.forEach(o),this.h()},h(){b(f,"id",J[0]),b(f,"class","header-link block pr-0.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(f,"href",u=`#${J[0]}`),b(n,"class","relative group rounded-md")},m(Z,U){v(Z,n,U),e(n,f),e(f,p),F(m,p,null),e(n,s),j&&j.m(n,null),J[5](n),a=!0,w||(h=Qp(Kp,"hashchange",J[2]),w=!0)},p(Z,[U]){(!a||U&1)&&b(f,"id",Z[0]),(!a||U&1&&u!==(u=`#${Z[0]}`))&&b(f,"href",u),j&&j.p&&(!a||U&8)&&Bp(j,_,Z,Z[3],a?Sp(_,Z[3],U,null):zp(Z[3]),null)},i(Z){a||($(m.$$.fragment,Z),$(j,Z),a=!0)},o(Z){k(m.$$.fragment,Z),k(j,Z),a=!1},d(Z){Z&&o(n),C(m),j&&j.d(Z),J[5](null),w=!1,h()}}}const qp="bg-yellow-50 dark:bg-[#494a3d]";function tm(J,n,f){let{$$slots:p={},$$scope:m}=n,{anchor:u}=n,s;function a(){const{hash:h}=window.location,_=h.substring(1);s&&s.classList.remove(...qp.split(" ")),_===u&&s.classList.add(...qp.split(" "))}Ap(()=>{a()});function w(h){Lp[h?"unshift":"push"](()=>{s=h,f(1,s)})}return J.$$set=h=>{"anchor"in h&&f(0,u=h.anchor),"$$scope"in h&&f(3,m=h.$$scope)},[u,s,a,m,p,w]}class E extends hc{constructor(n){super();uc(this,n,tm,em,fc,{anchor:0})}}function om(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function nm(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvckZlYXR1cmVFeHRyYWN0aW9uJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyc2VudGVuY2UtdHJhbnNmb3JtZXJzJTJGYWxsLU1pbmlMTS1MNi12MiUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JGZWF0dXJlRXh0cmFjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyc2VudGVuY2UtdHJhbnNmb3JtZXJzJTJGYWxsLU1pbmlMTS1MNi12MiUyMiUyQyUyMGV4cG9ydCUzRFRydWUpJTBBcGlwZSUyMCUzRCUyMHBpcGVsaW5lKCUyMmZlYXR1cmUtZXh0cmFjdGlvbiUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBb3V0cHV0cyUyMCUzRCUyMHBpcGUoJTIyTXklMjBOYW1lJTIwaXMlMjBQZXRlciUyMGFuZCUyMEklMjBsaXZlJTIwaW4lMjBOZXclMjBZb3JrLiUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForFeatureExtraction

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForFeatureExtraction.from_pretrained(<span class="hljs-string">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;feature-extraction&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(<span class="hljs-string">&quot;My Name is Peter and I live in New York.&quot;</span>)`}}),{c(){n=l("p"),f=d("Example of feature extraction using "),p=l("code"),m=d("transformers.pipelines"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of feature extraction using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipelines"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function sm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function am(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvck1hc2tlZExNJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIycm9iZXJ0YS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwT1ZNb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJyb2JlcnRhLWJhc2UlMjIlMkMlMjBleHBvcnQlM0RUcnVlKSUwQW1hc2tfdG9rZW4lMjAlM0QlMjB0b2tlbml6ZXIubWFza190b2tlbiUwQXBpcGUlMjAlM0QlMjBwaXBlbGluZSglMjJmaWxsLW1hc2slMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUwQW91dHB1dHMlMjAlM0QlMjBwaXBlKCUyMlRoZSUyMGdvYWwlMjBvZiUyMGxpZmUlMjBpcyUyMiUyMCUyQiUyMG1hc2tfdG9rZW4p",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token = tokenizer.mask_token
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(<span class="hljs-string">&quot;The goal of life is&quot;</span> + mask_token)`}}),{c(){n=l("p"),f=d("Example of masked language modeling using "),p=l("code"),m=d("transformers.pipelines"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of masked language modeling using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipelines"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function lm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function im(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydC1iYXNlLWNhc2VkLWRpc3RpbGxlZC1zcXVhZCUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydC1iYXNlLWNhc2VkLWRpc3RpbGxlZC1zcXVhZCUyMiUyQyUyMGV4cG9ydCUzRFRydWUpJTBBcGlwZSUyMCUzRCUyMHBpcGVsaW5lKCUyMnF1ZXN0aW9uLWFuc3dlcmluZyUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBcXVlc3Rpb24lMkMlMjB0ZXh0JTIwJTNEJTIwJTIyV2hvJTIwd2FzJTIwSmltJTIwSGVuc29uJTNGJTIyJTJDJTIwJTIySmltJTIwSGVuc29uJTIwd2FzJTIwYSUyMG5pY2UlMjBwdXBwZXQlMjIlMEFvdXRwdXRzJTIwJTNEJTIwcGlwZShxdWVzdGlvbiUyQyUyMHRleHQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(question, text)`}}),{c(){n=l("p"),f=d("Example of question answering using "),p=l("code"),m=d("transformers.pipeline"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of question answering using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipeline"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function rm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function dm(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1maW5ldHVuZWQtc3N0LTItZW5nbGlzaCUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1maW5ldHVuZWQtc3N0LTItZW5nbGlzaCUyMiUyQyUyMGV4cG9ydCUzRFRydWUpJTBBcGlwZSUyMCUzRCUyMHBpcGVsaW5lKCUyMnRleHQtY2xhc3NpZmljYXRpb24lMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUwQW91dHB1dHMlMjAlM0QlMjBwaXBlKCUyMkhlbGxvJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;text-classification&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>)`}}),{c(){n=l("p"),f=d("Example of sequence classification using "),p=l("code"),m=d("transformers.pipeline"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of sequence classification using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipeline"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function cm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function pm(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24lMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkc2xpbSUyRmJlcnQtYmFzZS1ORVIlMjIpJTBBbW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZHNsaW0lMkZiZXJ0LWJhc2UtTkVSJTIyJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEFwaXBlJTIwJTNEJTIwcGlwZWxpbmUoJTIydG9rZW4tY2xhc3NpZmljYXRpb24lMjIlMkMlMjBtb2RlbCUzRG1vZGVsJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKSUwQW91dHB1dHMlMjAlM0QlMjBwaXBlKCUyMk15JTIwTmFtZSUyMGlzJTIwUGV0ZXIlMjBhbmQlMjBJJTIwbGl2ZSUyMGluJTIwTmV3JTIwWW9yay4lMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dslim/bert-base-NER&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;dslim/bert-base-NER&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(<span class="hljs-string">&quot;My Name is Peter and I live in New York.&quot;</span>)`}}),{c(){n=l("p"),f=d("Example of token classification using "),p=l("code"),m=d("transformers.pipelines"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of token classification using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipelines"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function mm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function hm(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTJDJTIwcGlwZWxpbmUlMEFmcm9tJTIwb3B0aW11bS5pbnRlbCUyMGltcG9ydCUyME9WTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBcHJlcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKCUyMnN1cGVyYiUyRmh1YmVydC1iYXNlLXN1cGVyYi1lciUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJzdXBlcmIlMkZodWJlcnQtYmFzZS1zdXBlcmItZXIlMjIlMkMlMjBleHBvcnQlM0RUcnVlKSUwQXBpcGUlMjAlM0QlMjBwaXBlbGluZSglMjJhdWRpby1jbGFzc2lmaWNhdGlvbiUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjBmZWF0dXJlX2V4dHJhY3RvciUzRHByZXByb2Nlc3NvciklMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMnN1cGVyYiUyMiUyQyUyMCUyMmtzJTIyJTJDJTIwc3BsaXQlM0QlMjJ0ZXN0JTIyKSUwQWF1ZGlvX2ZpbGUlMjAlM0QlMjBkYXRhc2V0JTVCMyU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMEFvdXRwdXRzJTIwJTNEJTIwcGlwZShhdWRpb19maWxlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>preprocessor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;superb/hubert-base-superb-er&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;superb/hubert-base-superb-er&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;audio-classification&quot;</span>, model=model, feature_extractor=preprocessor)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, <span class="hljs-string">&quot;ks&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_file = dataset[<span class="hljs-number">3</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(audio_file)`}}),{c(){n=l("p"),f=d("Example of audio classification using "),p=l("code"),m=d("transformers.pipelines"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of audio classification using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipelines"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function um(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function fm(J){let n,f,p,m,u;return m=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVk1vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBaW1wb3J0JTIwdG9yY2glMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmhmLWludGVybmFsLXRlc3RpbmclMkZsaWJyaXNwZWVjaF9hc3JfZGVtbyUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LnNvcnQoJTIyaWQlMjIpJTBBc2FtcGxpbmdfcmF0ZSUyMCUzRCUyMGRhdGFzZXQuZmVhdHVyZXMlNUIlMjJhdWRpbyUyMiU1RC5zYW1wbGluZ19yYXRlJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyYW50b24tbCUyRndhdjJ2ZWMyLWJhc2Utc3VwZXJiLXNkJTIyKSUwQW1vZGVsJTIwJTNEJTIwJTIwT1ZNb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyYW50b24tbCUyRndhdjJ2ZWMyLWJhc2Utc3VwZXJiLXNkJTIyJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEElMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwc2FtcGxpbmdfcmF0ZSUzRHNhbXBsaW5nX3JhdGUpJTBBJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQXByb2JhYmlsaXRpZXMlMjAlM0QlMjB0b3JjaC5zaWdtb2lkKHRvcmNoLmFzX3RlbnNvcihsb2dpdHMpJTVCMCU1RCklMEFsYWJlbHMlMjAlM0QlMjAocHJvYmFiaWxpdGllcyUyMCUzRSUyMDAuNSkubG9uZygpJTBBbGFiZWxzJTVCMCU1RC50b2xpc3QoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForAudioFrameClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;anton-l/wav2vec2-base-superb-sd&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model =  OVModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;anton-l/wav2vec2-base-superb-sd&quot;</span>, export=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, sampling_rate=sampling_rate)
<span class="hljs-meta">&gt;&gt;&gt; </span>   logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = torch.sigmoid(torch.as_tensor(logits)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = (probabilities &gt; <span class="hljs-number">0.5</span>).long()
<span class="hljs-meta">&gt;&gt;&gt; </span>labels[<span class="hljs-number">0</span>].tolist()`}}),{c(){n=l("p"),f=d("Example of audio frame classification:"),p=g(),V(m.$$.fragment)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Example of audio frame classification:"),a.forEach(o),p=M(s),T(m.$$.fragment,s)},m(s,a){v(s,n,a),e(n,f),v(s,p,a),F(m,s,a),u=!0},p:W,i(s){u||($(m.$$.fragment,s),u=!0)},o(s){k(m.$$.fragment,s),u=!1},d(s){s&&o(n),s&&o(p),C(m,s)}}}function gm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function Mm(J){let n,f,p,m,u;return m=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVk1vZGVsRm9yQ1RDJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2RlbW8lMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zb3J0KCUyMmlkJTIyKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmh1YmVydC1sYXJnZS1sczk2MC1mdCUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JDVEMuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGaHViZXJ0LWxhcmdlLWxzOTYwLWZ0JTIyJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEElMEElMjMlMjBhdWRpbyUyMGZpbGUlMjBpcyUyMGRlY29kZWQlMjBvbiUyMHRoZSUyMGZseSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQXByZWRpY3RlZF9pZHMlMjAlM0QlMjBucC5hcmdtYXgobG9naXRzJTJDJTIwYXhpcyUzRC0xKSUwQSUwQXRyYW5zY3JpcHRpb24lMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKHByZWRpY3RlZF9pZHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/hubert-large-ls960-ft&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForCTC.from_pretrained(<span class="hljs-string">&quot;facebook/hubert-large-ls960-ft&quot;</span>, export=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = np.argmax(logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)`}}),{c(){n=l("p"),f=d("Example of CTC:"),p=g(),V(m.$$.fragment)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Example of CTC:"),a.forEach(o),p=M(s),T(m.$$.fragment,s)},m(s,a){v(s,n,a),e(n,f),v(s,p,a),F(m,s,a),u=!0},p:W,i(s){u||($(m.$$.fragment,s),u=!0)},o(s){k(m.$$.fragment,s),u=!1},d(s){s&&o(n),s&&o(p),C(m,s)}}}function ym(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function bm(J){let n,f,p,m,u;return m=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBZnJvbSUyMG9wdGltdW0uaW50ZWwlMjBpbXBvcnQlMjBPVk1vZGVsRm9yQXVkaW9YVmVjdG9yJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBaW1wb3J0JTIwdG9yY2glMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmhmLWludGVybmFsLXRlc3RpbmclMkZsaWJyaXNwZWVjaF9hc3JfZGVtbyUyMiUyQyUyMCUyMmNsZWFuJTIyJTJDJTIwc3BsaXQlM0QlMjJ2YWxpZGF0aW9uJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LnNvcnQoJTIyaWQlMjIpJTBBc2FtcGxpbmdfcmF0ZSUyMCUzRCUyMGRhdGFzZXQuZmVhdHVyZXMlNUIlMjJhdWRpbyUyMiU1RC5zYW1wbGluZ19yYXRlJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyYW50b24tbCUyRndhdjJ2ZWMyLWJhc2Utc3VwZXJiLXN2JTIyKSUwQW1vZGVsJTIwJTNEJTIwT1ZNb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyYW50b24tbCUyRndhdjJ2ZWMyLWJhc2Utc3VwZXJiLXN2JTIyJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEElMEElMjMlMjBhdWRpbyUyMGZpbGUlMjBpcyUyMGRlY29kZWQlMjBvbiUyMHRoZSUyMGZseSUwQWlucHV0cyUyMCUzRCUyMGZlYXR1cmVfZXh0cmFjdG9yKCUwQSUyMCUyMCUyMCUyMCU1QmQlNUIlMjJhcnJheSUyMiU1RCUyMGZvciUyMGQlMjBpbiUyMGRhdGFzZXQlNUIlM0EyJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUwQSklMEElMjAlMjAlMjAlMjBlbWJlZGRpbmdzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmVtYmVkZGluZ3MlMEElMEFlbWJlZGRpbmdzJTIwJTNEJTIwdG9yY2gubm4uZnVuY3Rpb25hbC5ub3JtYWxpemUoZW1iZWRkaW5ncyUyQyUyMGRpbSUzRC0xKS5jcHUoKSUwQSUwQWNvc2luZV9zaW0lMjAlM0QlMjB0b3JjaC5ubi5Db3NpbmVTaW1pbGFyaXR5KGRpbSUzRC0xKSUwQXNpbWlsYXJpdHklMjAlM0QlMjBjb3NpbmVfc2ltKGVtYmVkZGluZ3MlNUIwJTVEJTJDJTIwZW1iZWRkaW5ncyU1QjElNUQpJTBBdGhyZXNob2xkJTIwJTNEJTIwMC43JTBBaWYlMjBzaW1pbGFyaXR5JTIwJTNDJTIwdGhyZXNob2xkJTNBJTBBJTIwJTIwJTIwJTIwcHJpbnQoJTIyU3BlYWtlcnMlMjBhcmUlMjBub3QlMjB0aGUlMjBzYW1lISUyMiklMEFyb3VuZChzaW1pbGFyaXR5Lml0ZW0oKSUyQyUyMDIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForAudioXVector
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;anton-l/wav2vec2-base-superb-sv&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;anton-l/wav2vec2-base-superb-sv&quot;</span>, export=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(
<span class="hljs-meta">... </span>    [d[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> dataset[:<span class="hljs-number">2</span>][<span class="hljs-string">&quot;audio&quot;</span>]], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>    embeddings = model(**inputs).embeddings

<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = torch.nn.functional.normalize(embeddings, dim=-<span class="hljs-number">1</span>).cpu()

<span class="hljs-meta">&gt;&gt;&gt; </span>cosine_sim = torch.nn.CosineSimilarity(dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>similarity = cosine_sim(embeddings[<span class="hljs-number">0</span>], embeddings[<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>threshold = <span class="hljs-number">0.7</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> similarity &lt; threshold:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Speakers are not the same!&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(similarity.item(), <span class="hljs-number">2</span>)`}}),{c(){n=l("p"),f=d("Example of Audio XVector:"),p=g(),V(m.$$.fragment)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Example of Audio XVector:"),a.forEach(o),p=M(s),T(m.$$.fragment,s)},m(s,a){v(s,n,a),e(n,f),v(s,p,a),F(m,s,a),u=!0},p:W,i(s){u||($(m.$$.fragment,s),u=!0)},o(s){k(m.$$.fragment,s),u=!1},d(s){s&&o(n),s&&o(p),C(m,s)}}}function vm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function wm(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTJDJTIwcGlwZWxpbmUlMEFmcm9tJTIwb3B0aW11bS5pbnRlbCUyMGltcG9ydCUyME9WTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBcHJlcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnZpdC1iYXNlLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwT1ZNb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnZpdC1iYXNlLXBhdGNoMTYtMjI0JTIyJTJDJTIwZXhwb3J0JTNEVHJ1ZSklMEFtb2RlbC5yZXNoYXBlKGJhdGNoX3NpemUlM0QxJTJDJTIwc2VxdWVuY2VfbGVuZ3RoJTNEMyUyQyUyMGhlaWdodCUzRDIyNCUyQyUyMHdpZHRoJTNEMjI0KSUwQXBpcGUlMjAlM0QlMjBwaXBlbGluZSglMjJpbWFnZS1jbGFzc2lmaWNhdGlvbiUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjBmZWF0dXJlX2V4dHJhY3RvciUzRHByZXByb2Nlc3NvciklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBb3V0cHV0cyUyMCUzRCUyMHBpcGUodXJsKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>preprocessor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.reshape(batch_size=<span class="hljs-number">1</span>, sequence_length=<span class="hljs-number">3</span>, height=<span class="hljs-number">224</span>, width=<span class="hljs-number">224</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;image-classification&quot;</span>, model=model, feature_extractor=preprocessor)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(url)`}}),{c(){n=l("p"),f=d("Example of image classification using "),p=l("code"),m=d("transformers.pipelines"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example of image classification using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipelines"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function _m(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function $m(J){let n,f,p,m,u;return m=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb3B0aW11bS5pbnRlbCUyMGltcG9ydCUyME9WTW9kZWxGb3JDYXVzYWxMTSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmdwdDIlMjIpJTBBbW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdwdDIlMjIlMkMlMjBleHBvcnQlM0RUcnVlKSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJJJTIwbG92ZSUyMHRoaXMlMjBzdG9yeSUyMGJlY2F1c2UlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQWdlbl90b2tlbnMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyUyQyUyMGRvX3NhbXBsZSUzRFRydWUlMkMlMjB0ZW1wZXJhdHVyZSUzRDAuOSUyQyUyMG1pbl9sZW5ndGglM0QyMCUyQyUyMG1heF9sZW5ndGglM0QyMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbl90b2tlbnMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;I love this story because&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(**inputs, do_sample=<span class="hljs-literal">True</span>, temperature=<span class="hljs-number">0.9</span>, min_length=<span class="hljs-number">20</span>, max_length=<span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(gen_tokens)`}}),{c(){n=l("p"),f=d("Example of text generation:"),p=g(),V(m.$$.fragment)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Example of text generation:"),a.forEach(o),p=M(s),T(m.$$.fragment,s)},m(s,a){v(s,n,a),e(n,f),v(s,p,a),F(m,s,a),u=!0},p:W,i(s){u||($(m.$$.fragment,s),u=!0)},o(s){k(m.$$.fragment,s),u=!1},d(s){s&&o(n),s&&o(p),C(m,s)}}}function km(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvckNhdXNhbExNJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZ3B0MiUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ3B0MiUyMiUyQyUyMGV4cG9ydCUzRFRydWUpJTBBZ2VuX3BpcGVsaW5lJTIwJTNEJTIwcGlwZWxpbmUoJTIydGV4dC1nZW5lcmF0aW9uJTIyJTJDJTIwbW9kZWwlM0Rtb2RlbCUyQyUyMHRva2VuaXplciUzRHRva2VuaXplciklMEF0ZXh0JTIwJTNEJTIwJTIySSUyMGxvdmUlMjB0aGlzJTIwc3RvcnklMjBiZWNhdXNlJTIyJTBBZ2VuJTIwJTNEJTIwZ2VuX3BpcGVsaW5lKHRleHQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_pipeline = pipeline(<span class="hljs-string">&quot;text-generation&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;I love this story because&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gen = gen_pipeline(text)`}}),{c(){n=l("p"),f=d("Example using "),p=l("code"),m=d("transformers.pipelines"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipelines"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function Vm(J){let n,f,p,m,u;return{c(){n=l("p"),f=d("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=l("code"),m=d("Module"),u=d(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=i(a,"CODE",{});var w=r(p);m=c(w,"Module"),w.forEach(o),u=c(a,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),a.forEach(o)},m(s,a){v(s,n,a),e(n,f),e(n,p),e(p,m),e(n,u)},d(s){s&&o(n)}}}function Tm(J){let n,f,p,m,u;return m=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb3B0aW11bS5pbnRlbCUyMGltcG9ydCUyME9WTW9kZWxGb3JTZXEyU2VxTE0lMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJlY2hhcmxhaXglMkZ0NS1zbWFsbC1vcGVudmlubyUyMiklMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmVjaGFybGFpeCUyRnQ1LXNtYWxsLW9wZW52aW5vJTIyKSUwQXRleHQlMjAlM0QlMjAlMjJIZSUyMG5ldmVyJTIwd2VudCUyMG91dCUyMHdpdGhvdXQlMjBhJTIwYm9vayUyMHVuZGVyJTIwaGlzJTIwYXJtJTJDJTIwYW5kJTIwaGUlMjBvZnRlbiUyMGNhbWUlMjBiYWNrJTIwd2l0aCUyMHR3by4lMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIodGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBZ2VuX3Rva2VucyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQW91dHB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbl90b2tlbnMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;echarlaix/t5-small-openvino&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;echarlaix/t5-small-openvino&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;He never went out without a book under his arm, and he often came back with two.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = tokenizer.batch_decode(gen_tokens)`}}),{c(){n=l("p"),f=d("Example of text generation:"),p=g(),V(m.$$.fragment)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Example of text generation:"),a.forEach(o),p=M(s),T(m.$$.fragment,s)},m(s,a){v(s,n,a),e(n,f),v(s,p,a),F(m,s,a),u=!0},p:W,i(s){u||($(m.$$.fragment,s),u=!0)},o(s){k(m.$$.fragment,s),u=!1},d(s){s&&o(n),s&&o(p),C(m,s)}}}function Fm(J){let n,f,p,m,u,s,a,w;return a=new I({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLmludGVsJTIwaW1wb3J0JTIwT1ZNb2RlbEZvclNlcTJTZXFMTSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmVjaGFybGFpeCUyRnQ1LXNtYWxsLW9wZW52aW5vJTIyKSUwQW1vZGVsJTIwJTNEJTIwT1ZNb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTIyZWNoYXJsYWl4JTJGdDUtc21hbGwtb3BlbnZpbm8lMjIpJTBBcGlwZSUyMCUzRCUyMHBpcGVsaW5lKCUyMnRyYW5zbGF0aW9uX2VuX3RvX2ZyJTIyJTJDJTIwbW9kZWwlM0Rtb2RlbCUyQyUyMHRva2VuaXplciUzRHRva2VuaXplciklMEF0ZXh0JTIwJTNEJTIwJTIySGUlMjBuZXZlciUyMHdlbnQlMjBvdXQlMjB3aXRob3V0JTIwYSUyMGJvb2slMjB1bmRlciUyMGhpcyUyMGFybSUyQyUyMGFuZCUyMGhlJTIwb2Z0ZW4lMjBjYW1lJTIwYmFjayUyMHdpdGglMjB0d28uJTIyJTBBb3V0cHV0cyUyMCUzRCUyMHBpcGUodGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel <span class="hljs-keyword">import</span> OVModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;echarlaix/t5-small-openvino&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;echarlaix/t5-small-openvino&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;translation_en_to_fr&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;He never went out without a book under his arm, and he often came back with two.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = pipe(text)`}}),{c(){n=l("p"),f=d("Example using "),p=l("code"),m=d("transformers.pipeline"),u=d(":"),s=g(),V(a.$$.fragment)},l(h){n=i(h,"P",{});var _=r(n);f=c(_,"Example using "),p=i(_,"CODE",{});var j=r(p);m=c(j,"transformers.pipeline"),j.forEach(o),u=c(_,":"),_.forEach(o),s=M(h),T(a.$$.fragment,h)},m(h,_){v(h,n,_),e(n,f),e(n,p),e(p,m),e(n,u),v(h,s,_),F(a,h,_),w=!0},p:W,i(h){w||($(a.$$.fragment,h),w=!0)},o(h){k(a.$$.fragment,h),w=!1},d(h){h&&o(n),h&&o(s),C(a,h)}}}function Cm(J){let n,f,p,m,u;return m=new I({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwub3BlbnZpbm8lMjBpbXBvcnQlMjBPVlF1YW50aXplciUyQyUyME9WTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEFtb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1maW5ldHVuZWQtc3N0LTItZW5nbGlzaCUyMiUyQyUyMGV4cG9ydCUzRFRydWUpJTBBJTIzJTIwb3IlMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkLWZpbmV0dW5lZC1zc3QtMi1lbmdsaXNoJTIyKSUwQXF1YW50aXplciUyMCUzRCUyME9WUXVhbnRpemVyLmZyb21fcHJldHJhaW5lZChtb2RlbCUyQyUyMHRhc2slM0QlMjJ0ZXh0LWNsYXNzaWZpY2F0aW9uJTIyKSUwQXF1YW50aXplci5xdWFudGl6ZShjYWxpYnJhdGlvbl9kYXRhc2V0JTNEY2FsaWJyYXRpb25fZGF0YXNldCUyQyUyMHNhdmVfZGlyZWN0b3J5JTNEJTIyLiUyRnF1YW50aXplZF9tb2RlbCUyMiklMEFvcHRpbWl6ZWRfbW9kZWwlMjAlM0QlMjBPVk1vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnF1YW50aXplZF9tb2RlbCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel.openvino <span class="hljs-keyword">import</span> OVQuantizer, OVModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OVModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = OVQuantizer.from_pretrained(model, task=<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.quantize(calibration_dataset=calibration_dataset, save_directory=<span class="hljs-string">&quot;./quantized_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimized_model = OVModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./quantized_model&quot;</span>)`}}),{c(){n=l("p"),f=d("Examples:"),p=g(),V(m.$$.fragment)},l(s){n=i(s,"P",{});var a=r(n);f=c(a,"Examples:"),a.forEach(o),p=M(s),T(m.$$.fragment,s)},m(s,a){v(s,n,a),e(n,f),v(s,p,a),F(m,s,a),u=!0},p:W,i(s){u||($(m.$$.fragment,s),u=!0)},o(s){k(m.$$.fragment,s),u=!1},d(s){s&&o(n),s&&o(p),C(m,s)}}}function Jm(J){let n,f;return n=new I({props:{code:"ZnJvbSUyMG9wdGltdW0uaW50ZWwub3BlbnZpbm8lMjBpbXBvcnQlMjBPVlF1YW50aXplciUyQyUyME9WTW9kZWxGb3JDYXVzYWxMTSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmRhdGFicmlja3MlMkZkb2xseS12Mi0zYiUyMiklMEFxdWFudGl6ZXIlMjAlM0QlMjBPVlF1YW50aXplci5mcm9tX3ByZXRyYWluZWQobW9kZWwlMkMlMjB0YXNrJTNEJTIydGV4dC1nZW5lcmF0aW9uJTIyKSUwQXF1YW50aXplci5xdWFudGl6ZShzYXZlX2RpcmVjdG9yeSUzRCUyMi4lMkZxdWFudGl6ZWRfbW9kZWwlMjIlMkMlMjB3ZWlnaHRzX29ubHklM0RUcnVlKSUwQW9wdGltaXplZF9tb2RlbCUyMCUzRCUyME9WTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnF1YW50aXplZF9tb2RlbCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.intel.openvino <span class="hljs-keyword">import</span> OVQuantizer, OVModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;databricks/dolly-v2-3b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = OVQuantizer.from_pretrained(model, task=<span class="hljs-string">&quot;text-generation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.quantize(save_directory=<span class="hljs-string">&quot;./quantized_model&quot;</span>, weights_only=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimized_model = OVModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;./quantized_model&quot;</span>)`}}),{c(){V(n.$$.fragment)},l(p){T(n.$$.fragment,p)},m(p,m){F(n,p,m),f=!0},p:W,i(p){f||($(n.$$.fragment,p),f=!0)},o(p){k(n.$$.fragment,p),f=!1},d(p){C(n,p)}}}function jm(J){let n,f,p,m,u,s,a,w,h,_,j,Z,U,fe,nn,Cn,tl,sa,z,Pt,ol,Jn,nl,sl,Ht,al,jn,ll,il,rl,H,Kt,dl,_e,cl,sn,pl,ml,Zn,hl,ul,fl,Ye,gl,Le,aa,$e,De,On,eo,Ml,Un,yl,la,S,to,bl,En,vl,wl,oo,_l,Wn,$l,kl,Vl,K,no,Tl,ke,Fl,an,Cl,Jl,In,jl,Zl,Ol,Pe,Ul,He,ia,Ve,Ke,xn,so,El,Gn,Wl,ra,A,ao,Il,Xn,xl,Gl,lo,Xl,Rn,Rl,ql,Nl,ee,io,Bl,Te,zl,ln,Sl,Al,qn,Ql,Yl,Ll,et,Dl,tt,da,Fe,ot,Nn,ro,Pl,Bn,Hl,ca,Q,co,Kl,zn,ei,ti,po,oi,Sn,ni,si,ai,te,mo,li,Ce,ii,rn,ri,di,An,ci,pi,mi,nt,hi,st,pa,Je,at,Qn,ho,ui,Yn,fi,ma,Y,uo,gi,Ln,Mi,yi,fo,bi,Dn,vi,wi,_i,oe,go,$i,je,ki,dn,Vi,Ti,Pn,Fi,Ci,Ji,lt,ji,it,ha,Ze,rt,Hn,Mo,Zi,Kn,Oi,ua,L,yo,Ui,es,Ei,Wi,bo,Ii,ts,xi,Gi,Xi,ne,vo,Ri,Oe,qi,cn,Ni,Bi,os,zi,Si,Ai,dt,Qi,ct,fa,Ue,pt,ns,wo,Yi,ss,Li,ga,x,_o,Di,as,Pi,Hi,$o,Ki,ls,er,tr,or,is,nr,sr,se,ko,ar,Ee,lr,pn,ir,rr,rs,dr,cr,pr,mt,mr,ht,Ma,We,ut,ds,Vo,hr,cs,ur,ya,G,To,fr,ps,gr,Mr,Fo,yr,ms,br,vr,wr,hs,_r,$r,ae,Co,kr,Ie,Vr,mn,Tr,Fr,us,Cr,Jr,jr,ft,Zr,gt,ba,xe,Mt,fs,Jo,Or,gs,Ur,va,X,jo,Er,Ms,Wr,Ir,Zo,xr,ys,Gr,Xr,Rr,bs,qr,Nr,le,Oo,Br,Ge,zr,hn,Sr,Ar,vs,Qr,Yr,Lr,yt,Dr,bt,wa,Xe,vt,ws,Uo,Pr,_s,Hr,_a,D,Eo,Kr,$s,ed,td,Wo,od,ks,nd,sd,ad,ie,Io,ld,Re,id,un,rd,dd,Vs,cd,pd,md,wt,hd,_t,$a,qe,$t,Ts,xo,ud,Fs,fd,ka,R,Go,gd,Cs,Md,yd,Xo,bd,Js,vd,wd,_d,kt,Ro,$d,qo,kd,js,Vd,Td,Fd,N,No,Cd,Ne,Jd,fn,jd,Zd,Zs,Od,Ud,Ed,Vt,Wd,Tt,Id,Ft,Va,Be,Ct,Os,Bo,xd,Us,Gd,Ta,ce,zo,Xd,Es,Rd,qd,B,So,Nd,ze,Bd,gn,zd,Sd,Ws,Ad,Qd,Yd,Jt,Ld,jt,Dd,Zt,Fa,Se,Ot,Is,Ao,Pd,xs,Hd,Ca,P,Qo,Kd,Gs,ec,tc,Ut,Yo,oc,Lo,nc,Xs,sc,ac,lc,re,Do,ic,Po,rc,Rs,dc,cc,pc,Et,mc,Wt,Ja;return s=new q({}),fe=new q({}),Pt=new O({props:{name:"class optimum.intel.OVModelForFeatureExtraction",anchor:"optimum.intel.OVModelForFeatureExtraction",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForFeatureExtraction.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForFeatureExtraction.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForFeatureExtraction.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForFeatureExtraction.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForFeatureExtraction.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForFeatureExtraction.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L356"}}),Kt=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForFeatureExtraction.forward",parameters:[{name:"input_ids",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"token_type_ids",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForFeatureExtraction.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForFeatureExtraction.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForFeatureExtraction.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L363"}}),Ye=new de({props:{$$slots:{default:[om]},$$scope:{ctx:J}}}),Le=new E({props:{anchor:"optimum.intel.OVModelForFeatureExtraction.forward.example",$$slots:{default:[nm]},$$scope:{ctx:J}}}),eo=new q({}),to=new O({props:{name:"class optimum.intel.OVModelForMaskedLM",anchor:"optimum.intel.OVModelForMaskedLM",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForMaskedLM.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForMaskedLM.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForMaskedLM.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForMaskedLM.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForMaskedLM.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForMaskedLM.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L426"}}),no=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"token_type_ids",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L433"}}),Pe=new de({props:{$$slots:{default:[sm]},$$scope:{ctx:J}}}),He=new E({props:{anchor:"optimum.intel.OVModelForMaskedLM.forward.example",$$slots:{default:[am]},$$scope:{ctx:J}}}),so=new q({}),ao=new O({props:{name:"class optimum.intel.OVModelForQuestionAnswering",anchor:"optimum.intel.OVModelForQuestionAnswering",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForQuestionAnswering.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForQuestionAnswering.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForQuestionAnswering.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForQuestionAnswering.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForQuestionAnswering.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForQuestionAnswering.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L221"}}),io=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"token_type_ids",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L228"}}),et=new de({props:{$$slots:{default:[lm]},$$scope:{ctx:J}}}),tt=new E({props:{anchor:"optimum.intel.OVModelForQuestionAnswering.forward.example",$$slots:{default:[im]},$$scope:{ctx:J}}}),ro=new q({}),co=new O({props:{name:"class optimum.intel.OVModelForSequenceClassification",anchor:"optimum.intel.OVModelForSequenceClassification",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForSequenceClassification.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForSequenceClassification.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForSequenceClassification.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForSequenceClassification.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForSequenceClassification.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForSequenceClassification.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L155"}}),mo=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"token_type_ids",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L162"}}),nt=new de({props:{$$slots:{default:[rm]},$$scope:{ctx:J}}}),st=new E({props:{anchor:"optimum.intel.OVModelForSequenceClassification.forward.example",$$slots:{default:[dm]},$$scope:{ctx:J}}}),ho=new q({}),uo=new O({props:{name:"class optimum.intel.OVModelForTokenClassification",anchor:"optimum.intel.OVModelForTokenClassification",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForTokenClassification.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForTokenClassification.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForTokenClassification.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForTokenClassification.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForTokenClassification.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForTokenClassification.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L291"}}),go=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"token_type_ids",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L298"}}),lt=new de({props:{$$slots:{default:[cm]},$$scope:{ctx:J}}}),it=new E({props:{anchor:"optimum.intel.OVModelForTokenClassification.forward.example",$$slots:{default:[pm]},$$scope:{ctx:J}}}),Mo=new q({}),yo=new O({props:{name:"class optimum.intel.OVModelForAudioClassification",anchor:"optimum.intel.OVModelForAudioClassification",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForAudioClassification.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForAudioClassification.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForAudioClassification.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForAudioClassification.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForAudioClassification.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForAudioClassification.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L552"}}),vo=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForAudioClassification.forward",parameters:[{name:"input_values",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForAudioClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForAudioClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForAudioClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L559"}}),dt=new de({props:{$$slots:{default:[mm]},$$scope:{ctx:J}}}),ct=new E({props:{anchor:"optimum.intel.OVModelForAudioClassification.forward.example",$$slots:{default:[hm]},$$scope:{ctx:J}}}),wo=new q({}),_o=new O({props:{name:"class optimum.intel.OVModelForAudioFrameClassification",anchor:"optimum.intel.OVModelForAudioFrameClassification",parameters:[{name:"model",val:": Model"},{name:"config",val:": PretrainedConfig = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForAudioFrameClassification.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForAudioFrameClassification.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForAudioFrameClassification.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForAudioFrameClassification.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForAudioFrameClassification.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForAudioFrameClassification.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L783"}}),ko=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForAudioFrameClassification.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForAudioFrameClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform..
Input values can be obtained from audio file loaded into an array using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autofeatureextractor" rel="nofollow"><code>AutoFeatureExtractor</code></a>.`,name:"input_values"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L791"}}),mt=new de({props:{$$slots:{default:[um]},$$scope:{ctx:J}}}),ht=new E({props:{anchor:"optimum.intel.OVModelForAudioFrameClassification.forward.example",$$slots:{default:[fm]},$$scope:{ctx:J}}}),Vo=new q({}),To=new O({props:{name:"class optimum.intel.OVModelForCTC",anchor:"optimum.intel.OVModelForCTC",parameters:[{name:"model",val:": Model"},{name:"config",val:": PretrainedConfig = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForCTC.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForCTC.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForCTC.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForCTC.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForCTC.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForCTC.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L625"}}),Co=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForCTC.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Union[torch.Tensor, numpy.ndarray, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForCTC.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform..
Input values can be obtained from audio file loaded into an array using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autofeatureextractor" rel="nofollow"><code>AutoFeatureExtractor</code></a>.`,name:"input_values"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L633"}}),ft=new de({props:{$$slots:{default:[gm]},$$scope:{ctx:J}}}),gt=new E({props:{anchor:"optimum.intel.OVModelForCTC.forward.example",$$slots:{default:[Mm]},$$scope:{ctx:J}}}),Jo=new q({}),jo=new O({props:{name:"class optimum.intel.OVModelForAudioXVector",anchor:"optimum.intel.OVModelForAudioXVector",parameters:[{name:"model",val:": Model"},{name:"config",val:": PretrainedConfig = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForAudioXVector.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForAudioXVector.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForAudioXVector.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForAudioXVector.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForAudioXVector.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForAudioXVector.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L706"}}),Oo=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForAudioXVector.forward",parameters:[{name:"input_values",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForAudioXVector.forward.input_values",description:`<strong>input_values</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform..
Input values can be obtained from audio file loaded into an array using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autofeatureextractor" rel="nofollow"><code>AutoFeatureExtractor</code></a>.`,name:"input_values"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L714"}}),yt=new de({props:{$$slots:{default:[ym]},$$scope:{ctx:J}}}),bt=new E({props:{anchor:"optimum.intel.OVModelForAudioXVector.forward.example",$$slots:{default:[bm]},$$scope:{ctx:J}}}),Uo=new q({}),Eo=new O({props:{name:"class optimum.intel.OVModelForImageClassification",anchor:"optimum.intel.OVModelForImageClassification",parameters:[{name:"model",val:" = None"},{name:"config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForImageClassification.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForImageClassification.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForImageClassification.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForImageClassification.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForImageClassification.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForImageClassification.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L493"}}),Io=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Union[torch.Tensor, numpy.ndarray]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.Tensor</code>) &#x2014;
Pixel values corresponding to the images in the current batch.
Pixel values can be obtained from encoded images using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autofeatureextractor" rel="nofollow"><code>AutoFeatureExtractor</code></a>.`,name:"pixel_values"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling.py#L500"}}),wt=new de({props:{$$slots:{default:[vm]},$$scope:{ctx:J}}}),_t=new E({props:{anchor:"optimum.intel.OVModelForImageClassification.forward.example",$$slots:{default:[wm]},$$scope:{ctx:J}}}),xo=new q({}),Go=new O({props:{name:"class optimum.intel.OVModelForCausalLM",anchor:"optimum.intel.OVModelForCausalLM",parameters:[{name:"model",val:": Model"},{name:"config",val:": PretrainedConfig = None"},{name:"device",val:": str = 'CPU'"},{name:"dynamic_shapes",val:": bool = True"},{name:"ov_config",val:": typing.Union[typing.Dict[str, str], NoneType] = None"},{name:"model_save_dir",val:": typing.Union[str, pathlib.Path, tempfile.TemporaryDirectory, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForCausalLM.model",description:"<strong>model</strong> (<code>openvino.runtime.Model</code>) &#x2014; is the main class used to run OpenVINO Runtime inference.",name:"model"},{anchor:"optimum.intel.OVModelForCausalLM.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014; <a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is the Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the configuration.
Check out the <code>~intel.openvino.modeling.OVBaseModel.from_pretrained</code> method to load the model weights.`,name:"config"},{anchor:"optimum.intel.OVModelForCausalLM.device",description:`<strong>device</strong> (<code>str</code>, defaults to <code>&quot;CPU&quot;</code>) &#x2014;
The device type for which the model will be optimized for. The resulting compiled model will contains nodes specific to this device.`,name:"device"},{anchor:"optimum.intel.OVModelForCausalLM.dynamic_shapes",description:`<strong>dynamic_shapes</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
All the model&#x2019;s dimension will be set to dynamic when set to <code>True</code>. Should be set to <code>False</code> for the model to not be dynamically reshaped by default.`,name:"dynamic_shapes"},{anchor:"optimum.intel.OVModelForCausalLM.ov_config",description:`<strong>ov_config</strong> (<code>Optional[Dict]</code>, defaults to <code>None</code>) &#x2014;
The dictionnary containing the informations related to the model compilation.`,name:"ov_config"},{anchor:"optimum.intel.OVModelForCausalLM.compile",description:`<strong>compile</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Disable the model compilation during the loading step when set to <code>False</code>.
Can be useful to avoid unnecessary compilation, in the case where the model needs to be statically reshaped, the device modified or if FP16 conversion is enabled.`,name:"compile"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling_decoder.py#L305"}}),Ro=new O({props:{name:"can_generate",anchor:"optimum.intel.OVModelForCausalLM.can_generate",parameters:[],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling_decoder.py#L481"}}),No=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForCausalLM.forward",parameters:[{name:"input_ids",val:": LongTensor"},{name:"attention_mask",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.Tensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="https://huggingface.co/docs/transformers/autoclass_tutorial#autotokenizer" rel="nofollow"><code>AutoTokenizer</code></a>.
<a href="https://huggingface.co/docs/transformers/glossary#input-ids" rel="nofollow">What are input IDs?</a>`,name:"input_ids"},{anchor:"optimum.intel.OVModelForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code>), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#attention-mask" rel="nofollow">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.Tensor</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>sentence A</strong>,</li>
<li>0 for tokens that are <strong>sentence B</strong>.
<a href="https://huggingface.co/docs/transformers/glossary#token-type-ids" rel="nofollow">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling_decoder.py#L309"}}),Vt=new de({props:{$$slots:{default:[_m]},$$scope:{ctx:J}}}),Tt=new E({props:{anchor:"optimum.intel.OVModelForCausalLM.forward.example",$$slots:{default:[$m]},$$scope:{ctx:J}}}),Ft=new E({props:{anchor:"optimum.intel.OVModelForCausalLM.forward.example-2",$$slots:{default:[km]},$$scope:{ctx:J}}}),Bo=new q({}),zo=new O({props:{name:"class optimum.intel.OVModelForSeq2SeqLM",anchor:"optimum.intel.OVModelForSeq2SeqLM",parameters:[{name:"encoder",val:": Model"},{name:"decoder",val:": Model"},{name:"decoder_with_past",val:": Model = None"},{name:"config",val:": PretrainedConfig = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForSeq2SeqLM.encoder",description:`<strong>encoder</strong> (<code>openvino.runtime.Model</code>) &#x2014;
The OpenVINO Runtime model associated to the encoder.`,name:"encoder"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.decoder",description:`<strong>decoder</strong> (<code>openvino.runtime.Model</code>) &#x2014;
The OpenVINO Runtime model associated to the decoder.`,name:"decoder"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.decoder_with_past",description:`<strong>decoder_with_past</strong> (<code>openvino.runtime.Model</code>) &#x2014;
The OpenVINO Runtime model associated  to the decoder with past key values.`,name:"decoder_with_past"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.config",description:`<strong>config</strong> (<code>transformers.PretrainedConfig</code>) &#x2014;
<a href="https://huggingface.co/docs/transformers/main_classes/configuration#transformers.PretrainedConfig" rel="nofollow">PretrainedConfig</a>
is an instance of the configuration associated to the model. Initializing with a config file does
not load the weights associated with the model, only the configuration.`,name:"config"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling_seq2seq.py#L135"}}),So=new O({props:{name:"forward",anchor:"optimum.intel.OVModelForSeq2SeqLM.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code>) &#x2014;
Indices of input sequence tokens in the vocabulary of shape <code>(batch_size, encoder_sequence_length)</code>.`,name:"input_ids"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code>) &#x2014;
Mask to avoid performing attention on padding token indices, of shape
<code>(batch_size, encoder_sequence_length)</code>. Mask values selected in <code>[0, 1]</code>.`,name:"attention_mask"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary of shape <code>(batch_size, decoder_sequence_length)</code>.`,name:"decoder_input_ids"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
The encoder <code>last_hidden_state</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.`,name:"encoder_outputs"},{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor), *optional*)</code> &#x2014;
Contains the precomputed key and value hidden states of the attention blocks used to speed up decoding.
The tuple is of length <code>config.n_layers</code> with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, decoder_sequence_length, embed_size_per_head)</code> and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.`,name:"past_key_values"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/modeling_seq2seq.py#L194"}}),Jt=new de({props:{$$slots:{default:[Vm]},$$scope:{ctx:J}}}),jt=new E({props:{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.example",$$slots:{default:[Tm]},$$scope:{ctx:J}}}),Zt=new E({props:{anchor:"optimum.intel.OVModelForSeq2SeqLM.forward.example-2",$$slots:{default:[Fm]},$$scope:{ctx:J}}}),Ao=new q({}),Qo=new O({props:{name:"class optimum.intel.OVQuantizer",anchor:"optimum.intel.OVQuantizer",parameters:[{name:"model",val:": PreTrainedModel"},{name:"task",val:": typing.Optional[str] = None"},{name:"seed",val:": int = 42"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/quantization.py#L68"}}),Yo=new O({props:{name:"get_calibration_dataset",anchor:"optimum.intel.OVQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": str = 'train'"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"use_auth_token",val:": bool = False"},{name:"cache_dir",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
in generic formats and optionally a dataset script, if it requires some code to read the data files.`,name:"dataset_name"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, defaults to <code>&quot;train&quot;</code>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the token generated when running <code>transformers-cli login</code>.`,name:"use_auth_token"},{anchor:"optimum.intel.OVQuantizer.get_calibration_dataset.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Caching directory for a calibration dataset.`,name:"cache_dir"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/quantization.py#L428",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration step.</p>
`}}),Do=new O({props:{name:"quantize",anchor:"optimum.intel.OVQuantizer.quantize",parameters:[{name:"calibration_dataset",val:": Dataset = None"},{name:"save_directory",val:": typing.Union[str, pathlib.Path] = None"},{name:"quantization_config",val:": OVConfig = None"},{name:"file_name",val:": typing.Optional[str] = None"},{name:"batch_size",val:": int = 1"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"remove_unused_columns",val:": bool = True"},{name:"weights_only",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.intel.OVQuantizer.quantize.calibration_dataset",description:`<strong>calibration_dataset</strong> (<code>datasets.Dataset</code>) &#x2014;
The dataset to use for the calibration step.`,name:"calibration_dataset"},{anchor:"optimum.intel.OVQuantizer.quantize.save_directory",description:`<strong>save_directory</strong> (<code>Union[str, Path]</code>) &#x2014;
The directory where the quantized model should be saved.`,name:"save_directory"},{anchor:"optimum.intel.OVQuantizer.quantize.quantization_config",description:`<strong>quantization_config</strong> (<code>OVConfig</code>, <em>optional</em>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.intel.OVQuantizer.quantize.file_name",description:`<strong>file_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The model file name to use when saving the model. Overwrites the default file name <code>&quot;model.onnx&quot;</code>.`,name:"file_name"},{anchor:"optimum.intel.OVQuantizer.quantize.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 8) &#x2014;
The number of calibration samples to load per batch.`,name:"batch_size"},{anchor:"optimum.intel.OVQuantizer.quantize.data_collator",description:`<strong>data_collator</strong> (<code>DataCollator</code>, <em>optional</em>) &#x2014;
The function to use to form a batch from a list of elements of the calibration dataset.`,name:"data_collator"},{anchor:"optimum.intel.OVQuantizer.quantize.remove_unused_columns",description:`<strong>remove_unused_columns</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether or not to remove the columns unused by the model forward method.`,name:"remove_unused_columns"},{anchor:"optimum.intel.OVQuantizer.quantize.weights_only",description:`<strong>weights_only</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Compress weights to integer precision (8-bit by default) while keeping activations
floating-point. Fits best for LLM footprint reduction and performance acceleration.`,name:"weights_only"}],source:"https://github.com/huggingface/optimum.intel/blob/main/optimum/intel/openvino/quantization.py#L106"}}),Et=new E({props:{anchor:"optimum.intel.OVQuantizer.quantize.example",$$slots:{default:[Cm]},$$scope:{ctx:J}}}),Wt=new E({props:{anchor:"optimum.intel.OVQuantizer.quantize.example-2",$$slots:{default:[Jm]},$$scope:{ctx:J}}}),{c(){n=l("meta"),f=g(),p=l("h1"),m=l("a"),u=l("span"),V(s.$$.fragment),a=g(),w=l("span"),h=d("Reference"),_=g(),j=l("h2"),Z=l("a"),U=l("span"),V(fe.$$.fragment),nn=g(),Cn=l("span"),tl=d("OVModelForFeatureExtraction"),sa=g(),z=l("div"),V(Pt.$$.fragment),ol=g(),Jn=l("p"),nl=d("OpenVINO Model with a BaseModelOutput for feature extraction tasks."),sl=g(),Ht=l("p"),al=d("This model inherits from "),jn=l("code"),ll=d("optimum.intel.openvino.modeling.OVBaseModel"),il=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),rl=g(),H=l("div"),V(Kt.$$.fragment),dl=g(),_e=l("p"),cl=d("The "),sn=l("a"),pl=d("OVModelForFeatureExtraction"),ml=d(" forward method, overrides the "),Zn=l("code"),hl=d("__call__"),ul=d(" special method."),fl=g(),V(Ye.$$.fragment),gl=g(),V(Le.$$.fragment),aa=g(),$e=l("h2"),De=l("a"),On=l("span"),V(eo.$$.fragment),Ml=g(),Un=l("span"),yl=d("OVModelForMaskedLM"),la=g(),S=l("div"),V(to.$$.fragment),bl=g(),En=l("p"),vl=d("OpenVINO Model with a MaskedLMOutput for masked language modeling tasks."),wl=g(),oo=l("p"),_l=d("This model inherits from "),Wn=l("code"),$l=d("optimum.intel.openvino.modeling.OVBaseModel"),kl=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Vl=g(),K=l("div"),V(no.$$.fragment),Tl=g(),ke=l("p"),Fl=d("The "),an=l("a"),Cl=d("OVModelForMaskedLM"),Jl=d(" forward method, overrides the "),In=l("code"),jl=d("__call__"),Zl=d(" special method."),Ol=g(),V(Pe.$$.fragment),Ul=g(),V(He.$$.fragment),ia=g(),Ve=l("h2"),Ke=l("a"),xn=l("span"),V(so.$$.fragment),El=g(),Gn=l("span"),Wl=d("OVModelForQuestionAnswering"),ra=g(),A=l("div"),V(ao.$$.fragment),Il=g(),Xn=l("p"),xl=d("OpenVINO Model with a QuestionAnsweringModelOutput for extractive question-answering tasks."),Gl=g(),lo=l("p"),Xl=d("This model inherits from "),Rn=l("code"),Rl=d("optimum.intel.openvino.modeling.OVBaseModel"),ql=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Nl=g(),ee=l("div"),V(io.$$.fragment),Bl=g(),Te=l("p"),zl=d("The "),ln=l("a"),Sl=d("OVModelForQuestionAnswering"),Al=d(" forward method, overrides the "),qn=l("code"),Ql=d("__call__"),Yl=d(" special method."),Ll=g(),V(et.$$.fragment),Dl=g(),V(tt.$$.fragment),da=g(),Fe=l("h2"),ot=l("a"),Nn=l("span"),V(ro.$$.fragment),Pl=g(),Bn=l("span"),Hl=d("OVModelForSequenceClassification"),ca=g(),Q=l("div"),V(co.$$.fragment),Kl=g(),zn=l("p"),ei=d("OpenVINO Model with a SequenceClassifierOutput for sequence classification tasks."),ti=g(),po=l("p"),oi=d("This model inherits from "),Sn=l("code"),ni=d("optimum.intel.openvino.modeling.OVBaseModel"),si=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),ai=g(),te=l("div"),V(mo.$$.fragment),li=g(),Ce=l("p"),ii=d("The "),rn=l("a"),ri=d("OVModelForSequenceClassification"),di=d(" forward method, overrides the "),An=l("code"),ci=d("__call__"),pi=d(" special method."),mi=g(),V(nt.$$.fragment),hi=g(),V(st.$$.fragment),pa=g(),Je=l("h2"),at=l("a"),Qn=l("span"),V(ho.$$.fragment),ui=g(),Yn=l("span"),fi=d("OVModelForTokenClassification"),ma=g(),Y=l("div"),V(uo.$$.fragment),gi=g(),Ln=l("p"),Mi=d("OpenVINO Model with a TokenClassifierOutput for token classification tasks."),yi=g(),fo=l("p"),bi=d("This model inherits from "),Dn=l("code"),vi=d("optimum.intel.openvino.modeling.OVBaseModel"),wi=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),_i=g(),oe=l("div"),V(go.$$.fragment),$i=g(),je=l("p"),ki=d("The "),dn=l("a"),Vi=d("OVModelForTokenClassification"),Ti=d(" forward method, overrides the "),Pn=l("code"),Fi=d("__call__"),Ci=d(" special method."),Ji=g(),V(lt.$$.fragment),ji=g(),V(it.$$.fragment),ha=g(),Ze=l("h2"),rt=l("a"),Hn=l("span"),V(Mo.$$.fragment),Zi=g(),Kn=l("span"),Oi=d("OVModelForAudioClassification"),ua=g(),L=l("div"),V(yo.$$.fragment),Ui=g(),es=l("p"),Ei=d("OpenVINO Model with a SequenceClassifierOutput for audio classification tasks."),Wi=g(),bo=l("p"),Ii=d("This model inherits from "),ts=l("code"),xi=d("optimum.intel.openvino.modeling.OVBaseModel"),Gi=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Xi=g(),ne=l("div"),V(vo.$$.fragment),Ri=g(),Oe=l("p"),qi=d("The "),cn=l("a"),Ni=d("OVModelForAudioClassification"),Bi=d(" forward method, overrides the "),os=l("code"),zi=d("__call__"),Si=d(" special method."),Ai=g(),V(dt.$$.fragment),Qi=g(),V(ct.$$.fragment),fa=g(),Ue=l("h2"),pt=l("a"),ns=l("span"),V(wo.$$.fragment),Yi=g(),ss=l("span"),Li=d("OVModelForAudioFrameClassification"),ga=g(),x=l("div"),V(_o.$$.fragment),Di=g(),as=l("p"),Pi=d("OpenVINO Model for with a frame classification head on top for tasks like Speaker Diarization."),Hi=g(),$o=l("p"),Ki=d("This model inherits from "),ls=l("code"),er=d("optimum.intel.openvino.modeling.OVBaseModel"),tr=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),or=g(),is=l("p"),nr=d("Audio Frame Classification model for OpenVINO."),sr=g(),se=l("div"),V(ko.$$.fragment),ar=g(),Ee=l("p"),lr=d("The "),pn=l("a"),ir=d("OVModelForAudioFrameClassification"),rr=d(" forward method, overrides the "),rs=l("code"),dr=d("__call__"),cr=d(" special method."),pr=g(),V(mt.$$.fragment),mr=g(),V(ht.$$.fragment),Ma=g(),We=l("h2"),ut=l("a"),ds=l("span"),V(Vo.$$.fragment),hr=g(),cs=l("span"),ur=d("OVModelForCTC"),ya=g(),G=l("div"),V(To.$$.fragment),fr=g(),ps=l("p"),gr=d("Onnx Model with a language modeling head on top for Connectionist Temporal Classification (CTC)."),Mr=g(),Fo=l("p"),yr=d("This model inherits from "),ms=l("code"),br=d("optimum.intel.openvino.modeling.OVBaseModel"),vr=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),wr=g(),hs=l("p"),_r=d("CTC model for OpenVINO."),$r=g(),ae=l("div"),V(Co.$$.fragment),kr=g(),Ie=l("p"),Vr=d("The "),mn=l("a"),Tr=d("OVModelForCTC"),Fr=d(" forward method, overrides the "),us=l("code"),Cr=d("__call__"),Jr=d(" special method."),jr=g(),V(ft.$$.fragment),Zr=g(),V(gt.$$.fragment),ba=g(),xe=l("h2"),Mt=l("a"),fs=l("span"),V(Jo.$$.fragment),Or=g(),gs=l("span"),Ur=d("OVModelForAudioXVector"),va=g(),X=l("div"),V(jo.$$.fragment),Er=g(),Ms=l("p"),Wr=d("Onnx Model with an XVector feature extraction head on top for tasks like Speaker Verification."),Ir=g(),Zo=l("p"),xr=d("This model inherits from "),ys=l("code"),Gr=d("optimum.intel.openvino.modeling.OVBaseModel"),Xr=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Rr=g(),bs=l("p"),qr=d("Audio XVector model for OpenVINO."),Nr=g(),le=l("div"),V(Oo.$$.fragment),Br=g(),Ge=l("p"),zr=d("The "),hn=l("a"),Sr=d("OVModelForAudioXVector"),Ar=d(" forward method, overrides the "),vs=l("code"),Qr=d("__call__"),Yr=d(" special method."),Lr=g(),V(yt.$$.fragment),Dr=g(),V(bt.$$.fragment),wa=g(),Xe=l("h2"),vt=l("a"),ws=l("span"),V(Uo.$$.fragment),Pr=g(),_s=l("span"),Hr=d("OVModelForImageClassification"),_a=g(),D=l("div"),V(Eo.$$.fragment),Kr=g(),$s=l("p"),ed=d("OpenVINO Model with a ImageClassifierOutput for image classification tasks."),td=g(),Wo=l("p"),od=d("This model inherits from "),ks=l("code"),nd=d("optimum.intel.openvino.modeling.OVBaseModel"),sd=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),ad=g(),ie=l("div"),V(Io.$$.fragment),ld=g(),Re=l("p"),id=d("The "),un=l("a"),rd=d("OVModelForImageClassification"),dd=d(" forward method, overrides the "),Vs=l("code"),cd=d("__call__"),pd=d(" special method."),md=g(),V(wt.$$.fragment),hd=g(),V(_t.$$.fragment),$a=g(),qe=l("h2"),$t=l("a"),Ts=l("span"),V(xo.$$.fragment),ud=g(),Fs=l("span"),fd=d("OVModelForCausalLM"),ka=g(),R=l("div"),V(Go.$$.fragment),gd=g(),Cs=l("p"),Md=d(`OpenVINO Model with a causal language modeling head on top (linear layer with weights tied to the input
embeddings).`),yd=g(),Xo=l("p"),bd=d("This model inherits from "),Js=l("code"),vd=d("optimum.intel.openvino.modeling.OVBaseModel"),wd=d(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),_d=g(),kt=l("div"),V(Ro.$$.fragment),$d=g(),qo=l("p"),kd=d("Returns True to validate the check that the model using "),js=l("code"),Vd=d("GenerationMixin.generate()"),Td=d(" can indeed generate."),Fd=g(),N=l("div"),V(No.$$.fragment),Cd=g(),Ne=l("p"),Jd=d("The "),fn=l("a"),jd=d("OVModelForCausalLM"),Zd=d(" forward method, overrides the "),Zs=l("code"),Od=d("__call__"),Ud=d(" special method."),Ed=g(),V(Vt.$$.fragment),Wd=g(),V(Tt.$$.fragment),Id=g(),V(Ft.$$.fragment),Va=g(),Be=l("h2"),Ct=l("a"),Os=l("span"),V(Bo.$$.fragment),xd=g(),Us=l("span"),Gd=d("OVModelForSeq2SeqLM"),Ta=g(),ce=l("div"),V(zo.$$.fragment),Xd=g(),Es=l("p"),Rd=d("Sequence-to-sequence model with a language modeling head for OpenVINO inference."),qd=g(),B=l("div"),V(So.$$.fragment),Nd=g(),ze=l("p"),Bd=d("The "),gn=l("a"),zd=d("OVModelForSeq2SeqLM"),Sd=d(" forward method, overrides the "),Ws=l("code"),Ad=d("__call__"),Qd=d(" special method."),Yd=g(),V(Jt.$$.fragment),Ld=g(),V(jt.$$.fragment),Dd=g(),V(Zt.$$.fragment),Fa=g(),Se=l("h2"),Ot=l("a"),Is=l("span"),V(Ao.$$.fragment),Pd=g(),xs=l("span"),Hd=d("OVQuantizer"),Ca=g(),P=l("div"),V(Qo.$$.fragment),Kd=g(),Gs=l("p"),ec=d("Handle the NNCF quantization process."),tc=g(),Ut=l("div"),V(Yo.$$.fragment),oc=g(),Lo=l("p"),nc=d("Create the calibration "),Xs=l("code"),sc=d("datasets.Dataset"),ac=d(" to use for the post-training static quantization calibration step."),lc=g(),re=l("div"),V(Do.$$.fragment),ic=g(),Po=l("p"),rc=d("Quantize a model given the optimization specifications defined in "),Rs=l("code"),dc=d("quantization_config"),cc=d("."),pc=g(),V(Et.$$.fragment),mc=g(),V(Wt.$$.fragment),this.h()},l(t){const y=Dp('[data-svelte="svelte-1phssyn"]',document.head);n=i(y,"META",{name:!0,content:!0}),y.forEach(o),f=M(t),p=i(t,"H1",{class:!0});var Ho=r(p);m=i(Ho,"A",{id:!0,class:!0,href:!0});var qs=r(m);u=i(qs,"SPAN",{});var Ns=r(u);T(s.$$.fragment,Ns),Ns.forEach(o),qs.forEach(o),a=M(Ho),w=i(Ho,"SPAN",{});var Bs=r(w);h=c(Bs,"Reference"),Bs.forEach(o),Ho.forEach(o),_=M(t),j=i(t,"H2",{class:!0});var Ko=r(j);Z=i(Ko,"A",{id:!0,class:!0,href:!0});var zs=r(Z);U=i(zs,"SPAN",{});var Ss=r(U);T(fe.$$.fragment,Ss),Ss.forEach(o),zs.forEach(o),nn=M(Ko),Cn=i(Ko,"SPAN",{});var As=r(Cn);tl=c(As,"OVModelForFeatureExtraction"),As.forEach(o),Ko.forEach(o),sa=M(t),z=i(t,"DIV",{class:!0});var pe=r(z);T(Pt.$$.fragment,pe),ol=M(pe),Jn=i(pe,"P",{});var Qs=r(Jn);nl=c(Qs,"OpenVINO Model with a BaseModelOutput for feature extraction tasks."),Qs.forEach(o),sl=M(pe),Ht=i(pe,"P",{});var en=r(Ht);al=c(en,"This model inherits from "),jn=i(en,"CODE",{});var Ys=r(jn);ll=c(Ys,"optimum.intel.openvino.modeling.OVBaseModel"),Ys.forEach(o),il=c(en,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),en.forEach(o),rl=M(pe),H=i(pe,"DIV",{class:!0});var me=r(H);T(Kt.$$.fragment,me),dl=M(me),_e=i(me,"P",{});var Ae=r(_e);cl=c(Ae,"The "),sn=i(Ae,"A",{href:!0});var Ls=r(sn);pl=c(Ls,"OVModelForFeatureExtraction"),Ls.forEach(o),ml=c(Ae," forward method, overrides the "),Zn=i(Ae,"CODE",{});var Ds=r(Zn);hl=c(Ds,"__call__"),Ds.forEach(o),ul=c(Ae," special method."),Ae.forEach(o),fl=M(me),T(Ye.$$.fragment,me),gl=M(me),T(Le.$$.fragment,me),me.forEach(o),pe.forEach(o),aa=M(t),$e=i(t,"H2",{class:!0});var tn=r($e);De=i(tn,"A",{id:!0,class:!0,href:!0});var Ps=r(De);On=i(Ps,"SPAN",{});var Hs=r(On);T(eo.$$.fragment,Hs),Hs.forEach(o),Ps.forEach(o),Ml=M(tn),Un=i(tn,"SPAN",{});var Ks=r(Un);yl=c(Ks,"OVModelForMaskedLM"),Ks.forEach(o),tn.forEach(o),la=M(t),S=i(t,"DIV",{class:!0});var he=r(S);T(to.$$.fragment,he),bl=M(he),En=i(he,"P",{});var ea=r(En);vl=c(ea,"OpenVINO Model with a MaskedLMOutput for masked language modeling tasks."),ea.forEach(o),wl=M(he),oo=i(he,"P",{});var on=r(oo);_l=c(on,"This model inherits from "),Wn=i(on,"CODE",{});var ta=r(Wn);$l=c(ta,"optimum.intel.openvino.modeling.OVBaseModel"),ta.forEach(o),kl=c(on,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),on.forEach(o),Vl=M(he),K=i(he,"DIV",{class:!0});var ue=r(K);T(no.$$.fragment,ue),Tl=M(ue),ke=i(ue,"P",{});var Qe=r(ke);Fl=c(Qe,"The "),an=i(Qe,"A",{href:!0});var oa=r(an);Cl=c(oa,"OVModelForMaskedLM"),oa.forEach(o),Jl=c(Qe," forward method, overrides the "),In=i(Qe,"CODE",{});var na=r(In);jl=c(na,"__call__"),na.forEach(o),Zl=c(Qe," special method."),Qe.forEach(o),Ol=M(ue),T(Pe.$$.fragment,ue),Ul=M(ue),T(He.$$.fragment,ue),ue.forEach(o),he.forEach(o),ia=M(t),Ve=i(t,"H2",{class:!0});var ja=r(Ve);Ke=i(ja,"A",{id:!0,class:!0,href:!0});var gc=r(Ke);xn=i(gc,"SPAN",{});var Mc=r(xn);T(so.$$.fragment,Mc),Mc.forEach(o),gc.forEach(o),El=M(ja),Gn=i(ja,"SPAN",{});var yc=r(Gn);Wl=c(yc,"OVModelForQuestionAnswering"),yc.forEach(o),ja.forEach(o),ra=M(t),A=i(t,"DIV",{class:!0});var It=r(A);T(ao.$$.fragment,It),Il=M(It),Xn=i(It,"P",{});var bc=r(Xn);xl=c(bc,"OpenVINO Model with a QuestionAnsweringModelOutput for extractive question-answering tasks."),bc.forEach(o),Gl=M(It),lo=i(It,"P",{});var Za=r(lo);Xl=c(Za,"This model inherits from "),Rn=i(Za,"CODE",{});var vc=r(Rn);Rl=c(vc,"optimum.intel.openvino.modeling.OVBaseModel"),vc.forEach(o),ql=c(Za,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Za.forEach(o),Nl=M(It),ee=i(It,"DIV",{class:!0});var xt=r(ee);T(io.$$.fragment,xt),Bl=M(xt),Te=i(xt,"P",{});var Mn=r(Te);zl=c(Mn,"The "),ln=i(Mn,"A",{href:!0});var wc=r(ln);Sl=c(wc,"OVModelForQuestionAnswering"),wc.forEach(o),Al=c(Mn," forward method, overrides the "),qn=i(Mn,"CODE",{});var _c=r(qn);Ql=c(_c,"__call__"),_c.forEach(o),Yl=c(Mn," special method."),Mn.forEach(o),Ll=M(xt),T(et.$$.fragment,xt),Dl=M(xt),T(tt.$$.fragment,xt),xt.forEach(o),It.forEach(o),da=M(t),Fe=i(t,"H2",{class:!0});var Oa=r(Fe);ot=i(Oa,"A",{id:!0,class:!0,href:!0});var $c=r(ot);Nn=i($c,"SPAN",{});var kc=r(Nn);T(ro.$$.fragment,kc),kc.forEach(o),$c.forEach(o),Pl=M(Oa),Bn=i(Oa,"SPAN",{});var Vc=r(Bn);Hl=c(Vc,"OVModelForSequenceClassification"),Vc.forEach(o),Oa.forEach(o),ca=M(t),Q=i(t,"DIV",{class:!0});var Gt=r(Q);T(co.$$.fragment,Gt),Kl=M(Gt),zn=i(Gt,"P",{});var Tc=r(zn);ei=c(Tc,"OpenVINO Model with a SequenceClassifierOutput for sequence classification tasks."),Tc.forEach(o),ti=M(Gt),po=i(Gt,"P",{});var Ua=r(po);oi=c(Ua,"This model inherits from "),Sn=i(Ua,"CODE",{});var Fc=r(Sn);ni=c(Fc,"optimum.intel.openvino.modeling.OVBaseModel"),Fc.forEach(o),si=c(Ua,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Ua.forEach(o),ai=M(Gt),te=i(Gt,"DIV",{class:!0});var Xt=r(te);T(mo.$$.fragment,Xt),li=M(Xt),Ce=i(Xt,"P",{});var yn=r(Ce);ii=c(yn,"The "),rn=i(yn,"A",{href:!0});var Cc=r(rn);ri=c(Cc,"OVModelForSequenceClassification"),Cc.forEach(o),di=c(yn," forward method, overrides the "),An=i(yn,"CODE",{});var Jc=r(An);ci=c(Jc,"__call__"),Jc.forEach(o),pi=c(yn," special method."),yn.forEach(o),mi=M(Xt),T(nt.$$.fragment,Xt),hi=M(Xt),T(st.$$.fragment,Xt),Xt.forEach(o),Gt.forEach(o),pa=M(t),Je=i(t,"H2",{class:!0});var Ea=r(Je);at=i(Ea,"A",{id:!0,class:!0,href:!0});var jc=r(at);Qn=i(jc,"SPAN",{});var Zc=r(Qn);T(ho.$$.fragment,Zc),Zc.forEach(o),jc.forEach(o),ui=M(Ea),Yn=i(Ea,"SPAN",{});var Oc=r(Yn);fi=c(Oc,"OVModelForTokenClassification"),Oc.forEach(o),Ea.forEach(o),ma=M(t),Y=i(t,"DIV",{class:!0});var Rt=r(Y);T(uo.$$.fragment,Rt),gi=M(Rt),Ln=i(Rt,"P",{});var Uc=r(Ln);Mi=c(Uc,"OpenVINO Model with a TokenClassifierOutput for token classification tasks."),Uc.forEach(o),yi=M(Rt),fo=i(Rt,"P",{});var Wa=r(fo);bi=c(Wa,"This model inherits from "),Dn=i(Wa,"CODE",{});var Ec=r(Dn);vi=c(Ec,"optimum.intel.openvino.modeling.OVBaseModel"),Ec.forEach(o),wi=c(Wa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Wa.forEach(o),_i=M(Rt),oe=i(Rt,"DIV",{class:!0});var qt=r(oe);T(go.$$.fragment,qt),$i=M(qt),je=i(qt,"P",{});var bn=r(je);ki=c(bn,"The "),dn=i(bn,"A",{href:!0});var Wc=r(dn);Vi=c(Wc,"OVModelForTokenClassification"),Wc.forEach(o),Ti=c(bn," forward method, overrides the "),Pn=i(bn,"CODE",{});var Ic=r(Pn);Fi=c(Ic,"__call__"),Ic.forEach(o),Ci=c(bn," special method."),bn.forEach(o),Ji=M(qt),T(lt.$$.fragment,qt),ji=M(qt),T(it.$$.fragment,qt),qt.forEach(o),Rt.forEach(o),ha=M(t),Ze=i(t,"H2",{class:!0});var Ia=r(Ze);rt=i(Ia,"A",{id:!0,class:!0,href:!0});var xc=r(rt);Hn=i(xc,"SPAN",{});var Gc=r(Hn);T(Mo.$$.fragment,Gc),Gc.forEach(o),xc.forEach(o),Zi=M(Ia),Kn=i(Ia,"SPAN",{});var Xc=r(Kn);Oi=c(Xc,"OVModelForAudioClassification"),Xc.forEach(o),Ia.forEach(o),ua=M(t),L=i(t,"DIV",{class:!0});var Nt=r(L);T(yo.$$.fragment,Nt),Ui=M(Nt),es=i(Nt,"P",{});var Rc=r(es);Ei=c(Rc,"OpenVINO Model with a SequenceClassifierOutput for audio classification tasks."),Rc.forEach(o),Wi=M(Nt),bo=i(Nt,"P",{});var xa=r(bo);Ii=c(xa,"This model inherits from "),ts=i(xa,"CODE",{});var qc=r(ts);xi=c(qc,"optimum.intel.openvino.modeling.OVBaseModel"),qc.forEach(o),Gi=c(xa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),xa.forEach(o),Xi=M(Nt),ne=i(Nt,"DIV",{class:!0});var Bt=r(ne);T(vo.$$.fragment,Bt),Ri=M(Bt),Oe=i(Bt,"P",{});var vn=r(Oe);qi=c(vn,"The "),cn=i(vn,"A",{href:!0});var Nc=r(cn);Ni=c(Nc,"OVModelForAudioClassification"),Nc.forEach(o),Bi=c(vn," forward method, overrides the "),os=i(vn,"CODE",{});var Bc=r(os);zi=c(Bc,"__call__"),Bc.forEach(o),Si=c(vn," special method."),vn.forEach(o),Ai=M(Bt),T(dt.$$.fragment,Bt),Qi=M(Bt),T(ct.$$.fragment,Bt),Bt.forEach(o),Nt.forEach(o),fa=M(t),Ue=i(t,"H2",{class:!0});var Ga=r(Ue);pt=i(Ga,"A",{id:!0,class:!0,href:!0});var zc=r(pt);ns=i(zc,"SPAN",{});var Sc=r(ns);T(wo.$$.fragment,Sc),Sc.forEach(o),zc.forEach(o),Yi=M(Ga),ss=i(Ga,"SPAN",{});var Ac=r(ss);Li=c(Ac,"OVModelForAudioFrameClassification"),Ac.forEach(o),Ga.forEach(o),ga=M(t),x=i(t,"DIV",{class:!0});var ge=r(x);T(_o.$$.fragment,ge),Di=M(ge),as=i(ge,"P",{});var Qc=r(as);Pi=c(Qc,"OpenVINO Model for with a frame classification head on top for tasks like Speaker Diarization."),Qc.forEach(o),Hi=M(ge),$o=i(ge,"P",{});var Xa=r($o);Ki=c(Xa,"This model inherits from "),ls=i(Xa,"CODE",{});var Yc=r(ls);er=c(Yc,"optimum.intel.openvino.modeling.OVBaseModel"),Yc.forEach(o),tr=c(Xa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Xa.forEach(o),or=M(ge),is=i(ge,"P",{});var Lc=r(is);nr=c(Lc,"Audio Frame Classification model for OpenVINO."),Lc.forEach(o),sr=M(ge),se=i(ge,"DIV",{class:!0});var zt=r(se);T(ko.$$.fragment,zt),ar=M(zt),Ee=i(zt,"P",{});var wn=r(Ee);lr=c(wn,"The "),pn=i(wn,"A",{href:!0});var Dc=r(pn);ir=c(Dc,"OVModelForAudioFrameClassification"),Dc.forEach(o),rr=c(wn," forward method, overrides the "),rs=i(wn,"CODE",{});var Pc=r(rs);dr=c(Pc,"__call__"),Pc.forEach(o),cr=c(wn," special method."),wn.forEach(o),pr=M(zt),T(mt.$$.fragment,zt),mr=M(zt),T(ht.$$.fragment,zt),zt.forEach(o),ge.forEach(o),Ma=M(t),We=i(t,"H2",{class:!0});var Ra=r(We);ut=i(Ra,"A",{id:!0,class:!0,href:!0});var Hc=r(ut);ds=i(Hc,"SPAN",{});var Kc=r(ds);T(Vo.$$.fragment,Kc),Kc.forEach(o),Hc.forEach(o),hr=M(Ra),cs=i(Ra,"SPAN",{});var ep=r(cs);ur=c(ep,"OVModelForCTC"),ep.forEach(o),Ra.forEach(o),ya=M(t),G=i(t,"DIV",{class:!0});var Me=r(G);T(To.$$.fragment,Me),fr=M(Me),ps=i(Me,"P",{});var tp=r(ps);gr=c(tp,"Onnx Model with a language modeling head on top for Connectionist Temporal Classification (CTC)."),tp.forEach(o),Mr=M(Me),Fo=i(Me,"P",{});var qa=r(Fo);yr=c(qa,"This model inherits from "),ms=i(qa,"CODE",{});var op=r(ms);br=c(op,"optimum.intel.openvino.modeling.OVBaseModel"),op.forEach(o),vr=c(qa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),qa.forEach(o),wr=M(Me),hs=i(Me,"P",{});var np=r(hs);_r=c(np,"CTC model for OpenVINO."),np.forEach(o),$r=M(Me),ae=i(Me,"DIV",{class:!0});var St=r(ae);T(Co.$$.fragment,St),kr=M(St),Ie=i(St,"P",{});var _n=r(Ie);Vr=c(_n,"The "),mn=i(_n,"A",{href:!0});var sp=r(mn);Tr=c(sp,"OVModelForCTC"),sp.forEach(o),Fr=c(_n," forward method, overrides the "),us=i(_n,"CODE",{});var ap=r(us);Cr=c(ap,"__call__"),ap.forEach(o),Jr=c(_n," special method."),_n.forEach(o),jr=M(St),T(ft.$$.fragment,St),Zr=M(St),T(gt.$$.fragment,St),St.forEach(o),Me.forEach(o),ba=M(t),xe=i(t,"H2",{class:!0});var Na=r(xe);Mt=i(Na,"A",{id:!0,class:!0,href:!0});var lp=r(Mt);fs=i(lp,"SPAN",{});var ip=r(fs);T(Jo.$$.fragment,ip),ip.forEach(o),lp.forEach(o),Or=M(Na),gs=i(Na,"SPAN",{});var rp=r(gs);Ur=c(rp,"OVModelForAudioXVector"),rp.forEach(o),Na.forEach(o),va=M(t),X=i(t,"DIV",{class:!0});var ye=r(X);T(jo.$$.fragment,ye),Er=M(ye),Ms=i(ye,"P",{});var dp=r(Ms);Wr=c(dp,"Onnx Model with an XVector feature extraction head on top for tasks like Speaker Verification."),dp.forEach(o),Ir=M(ye),Zo=i(ye,"P",{});var Ba=r(Zo);xr=c(Ba,"This model inherits from "),ys=i(Ba,"CODE",{});var cp=r(ys);Gr=c(cp,"optimum.intel.openvino.modeling.OVBaseModel"),cp.forEach(o),Xr=c(Ba,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Ba.forEach(o),Rr=M(ye),bs=i(ye,"P",{});var pp=r(bs);qr=c(pp,"Audio XVector model for OpenVINO."),pp.forEach(o),Nr=M(ye),le=i(ye,"DIV",{class:!0});var At=r(le);T(Oo.$$.fragment,At),Br=M(At),Ge=i(At,"P",{});var $n=r(Ge);zr=c($n,"The "),hn=i($n,"A",{href:!0});var mp=r(hn);Sr=c(mp,"OVModelForAudioXVector"),mp.forEach(o),Ar=c($n," forward method, overrides the "),vs=i($n,"CODE",{});var hp=r(vs);Qr=c(hp,"__call__"),hp.forEach(o),Yr=c($n," special method."),$n.forEach(o),Lr=M(At),T(yt.$$.fragment,At),Dr=M(At),T(bt.$$.fragment,At),At.forEach(o),ye.forEach(o),wa=M(t),Xe=i(t,"H2",{class:!0});var za=r(Xe);vt=i(za,"A",{id:!0,class:!0,href:!0});var up=r(vt);ws=i(up,"SPAN",{});var fp=r(ws);T(Uo.$$.fragment,fp),fp.forEach(o),up.forEach(o),Pr=M(za),_s=i(za,"SPAN",{});var gp=r(_s);Hr=c(gp,"OVModelForImageClassification"),gp.forEach(o),za.forEach(o),_a=M(t),D=i(t,"DIV",{class:!0});var Qt=r(D);T(Eo.$$.fragment,Qt),Kr=M(Qt),$s=i(Qt,"P",{});var Mp=r($s);ed=c(Mp,"OpenVINO Model with a ImageClassifierOutput for image classification tasks."),Mp.forEach(o),td=M(Qt),Wo=i(Qt,"P",{});var Sa=r(Wo);od=c(Sa,"This model inherits from "),ks=i(Sa,"CODE",{});var yp=r(ks);nd=c(yp,"optimum.intel.openvino.modeling.OVBaseModel"),yp.forEach(o),sd=c(Sa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Sa.forEach(o),ad=M(Qt),ie=i(Qt,"DIV",{class:!0});var Yt=r(ie);T(Io.$$.fragment,Yt),ld=M(Yt),Re=i(Yt,"P",{});var kn=r(Re);id=c(kn,"The "),un=i(kn,"A",{href:!0});var bp=r(un);rd=c(bp,"OVModelForImageClassification"),bp.forEach(o),dd=c(kn," forward method, overrides the "),Vs=i(kn,"CODE",{});var vp=r(Vs);cd=c(vp,"__call__"),vp.forEach(o),pd=c(kn," special method."),kn.forEach(o),md=M(Yt),T(wt.$$.fragment,Yt),hd=M(Yt),T(_t.$$.fragment,Yt),Yt.forEach(o),Qt.forEach(o),$a=M(t),qe=i(t,"H2",{class:!0});var Aa=r(qe);$t=i(Aa,"A",{id:!0,class:!0,href:!0});var wp=r($t);Ts=i(wp,"SPAN",{});var _p=r(Ts);T(xo.$$.fragment,_p),_p.forEach(o),wp.forEach(o),ud=M(Aa),Fs=i(Aa,"SPAN",{});var $p=r(Fs);fd=c($p,"OVModelForCausalLM"),$p.forEach(o),Aa.forEach(o),ka=M(t),R=i(t,"DIV",{class:!0});var be=r(R);T(Go.$$.fragment,be),gd=M(be),Cs=i(be,"P",{});var kp=r(Cs);Md=c(kp,`OpenVINO Model with a causal language modeling head on top (linear layer with weights tied to the input
embeddings).`),kp.forEach(o),yd=M(be),Xo=i(be,"P",{});var Qa=r(Xo);bd=c(Qa,"This model inherits from "),Js=i(Qa,"CODE",{});var Vp=r(Js);vd=c(Vp,"optimum.intel.openvino.modeling.OVBaseModel"),Vp.forEach(o),wd=c(Qa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving)`),Qa.forEach(o),_d=M(be),kt=i(be,"DIV",{class:!0});var Ya=r(kt);T(Ro.$$.fragment,Ya),$d=M(Ya),qo=i(Ya,"P",{});var La=r(qo);kd=c(La,"Returns True to validate the check that the model using "),js=i(La,"CODE",{});var Tp=r(js);Vd=c(Tp,"GenerationMixin.generate()"),Tp.forEach(o),Td=c(La," can indeed generate."),La.forEach(o),Ya.forEach(o),Fd=M(be),N=i(be,"DIV",{class:!0});var ve=r(N);T(No.$$.fragment,ve),Cd=M(ve),Ne=i(ve,"P",{});var Vn=r(Ne);Jd=c(Vn,"The "),fn=i(Vn,"A",{href:!0});var Fp=r(fn);jd=c(Fp,"OVModelForCausalLM"),Fp.forEach(o),Zd=c(Vn," forward method, overrides the "),Zs=i(Vn,"CODE",{});var Cp=r(Zs);Od=c(Cp,"__call__"),Cp.forEach(o),Ud=c(Vn," special method."),Vn.forEach(o),Ed=M(ve),T(Vt.$$.fragment,ve),Wd=M(ve),T(Tt.$$.fragment,ve),Id=M(ve),T(Ft.$$.fragment,ve),ve.forEach(o),be.forEach(o),Va=M(t),Be=i(t,"H2",{class:!0});var Da=r(Be);Ct=i(Da,"A",{id:!0,class:!0,href:!0});var Jp=r(Ct);Os=i(Jp,"SPAN",{});var jp=r(Os);T(Bo.$$.fragment,jp),jp.forEach(o),Jp.forEach(o),xd=M(Da),Us=i(Da,"SPAN",{});var Zp=r(Us);Gd=c(Zp,"OVModelForSeq2SeqLM"),Zp.forEach(o),Da.forEach(o),Ta=M(t),ce=i(t,"DIV",{class:!0});var Tn=r(ce);T(zo.$$.fragment,Tn),Xd=M(Tn),Es=i(Tn,"P",{});var Op=r(Es);Rd=c(Op,"Sequence-to-sequence model with a language modeling head for OpenVINO inference."),Op.forEach(o),qd=M(Tn),B=i(Tn,"DIV",{class:!0});var we=r(B);T(So.$$.fragment,we),Nd=M(we),ze=i(we,"P",{});var Fn=r(ze);Bd=c(Fn,"The "),gn=i(Fn,"A",{href:!0});var Up=r(gn);zd=c(Up,"OVModelForSeq2SeqLM"),Up.forEach(o),Sd=c(Fn," forward method, overrides the "),Ws=i(Fn,"CODE",{});var Ep=r(Ws);Ad=c(Ep,"__call__"),Ep.forEach(o),Qd=c(Fn," special method."),Fn.forEach(o),Yd=M(we),T(Jt.$$.fragment,we),Ld=M(we),T(jt.$$.fragment,we),Dd=M(we),T(Zt.$$.fragment,we),we.forEach(o),Tn.forEach(o),Fa=M(t),Se=i(t,"H2",{class:!0});var Pa=r(Se);Ot=i(Pa,"A",{id:!0,class:!0,href:!0});var Wp=r(Ot);Is=i(Wp,"SPAN",{});var Ip=r(Is);T(Ao.$$.fragment,Ip),Ip.forEach(o),Wp.forEach(o),Pd=M(Pa),xs=i(Pa,"SPAN",{});var xp=r(xs);Hd=c(xp,"OVQuantizer"),xp.forEach(o),Pa.forEach(o),Ca=M(t),P=i(t,"DIV",{class:!0});var Lt=r(P);T(Qo.$$.fragment,Lt),Kd=M(Lt),Gs=i(Lt,"P",{});var Gp=r(Gs);ec=c(Gp,"Handle the NNCF quantization process."),Gp.forEach(o),tc=M(Lt),Ut=i(Lt,"DIV",{class:!0});var Ha=r(Ut);T(Yo.$$.fragment,Ha),oc=M(Ha),Lo=i(Ha,"P",{});var Ka=r(Lo);nc=c(Ka,"Create the calibration "),Xs=i(Ka,"CODE",{});var Xp=r(Xs);sc=c(Xp,"datasets.Dataset"),Xp.forEach(o),ac=c(Ka," to use for the post-training static quantization calibration step."),Ka.forEach(o),Ha.forEach(o),lc=M(Lt),re=i(Lt,"DIV",{class:!0});var Dt=r(re);T(Do.$$.fragment,Dt),ic=M(Dt),Po=i(Dt,"P",{});var el=r(Po);rc=c(el,"Quantize a model given the optimization specifications defined in "),Rs=i(el,"CODE",{});var Rp=r(Rs);dc=c(Rp,"quantization_config"),Rp.forEach(o),cc=c(el,"."),el.forEach(o),pc=M(Dt),T(Et.$$.fragment,Dt),mc=M(Dt),T(Wt.$$.fragment,Dt),Dt.forEach(o),Lt.forEach(o),this.h()},h(){b(n,"name","hf:doc:metadata"),b(n,"content",JSON.stringify(Zm)),b(m,"id","reference"),b(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(m,"href","#reference"),b(p,"class","relative group"),b(Z,"id","optimum.intel.OVModelForFeatureExtraction"),b(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Z,"href","#optimum.intel.OVModelForFeatureExtraction"),b(j,"class","relative group"),b(sn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForFeatureExtraction"),b(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(De,"id","optimum.intel.OVModelForMaskedLM"),b(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(De,"href","#optimum.intel.OVModelForMaskedLM"),b($e,"class","relative group"),b(an,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForMaskedLM"),b(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Ke,"id","optimum.intel.OVModelForQuestionAnswering"),b(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Ke,"href","#optimum.intel.OVModelForQuestionAnswering"),b(Ve,"class","relative group"),b(ln,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForQuestionAnswering"),b(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(ot,"id","optimum.intel.OVModelForSequenceClassification"),b(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(ot,"href","#optimum.intel.OVModelForSequenceClassification"),b(Fe,"class","relative group"),b(rn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForSequenceClassification"),b(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(at,"id","optimum.intel.OVModelForTokenClassification"),b(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(at,"href","#optimum.intel.OVModelForTokenClassification"),b(Je,"class","relative group"),b(dn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForTokenClassification"),b(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(rt,"id","optimum.intel.OVModelForAudioClassification"),b(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(rt,"href","#optimum.intel.OVModelForAudioClassification"),b(Ze,"class","relative group"),b(cn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForAudioClassification"),b(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(pt,"id","optimum.intel.OVModelForAudioFrameClassification"),b(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(pt,"href","#optimum.intel.OVModelForAudioFrameClassification"),b(Ue,"class","relative group"),b(pn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForAudioFrameClassification"),b(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(ut,"id","optimum.intel.OVModelForCTC"),b(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(ut,"href","#optimum.intel.OVModelForCTC"),b(We,"class","relative group"),b(mn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForCTC"),b(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Mt,"id","optimum.intel.OVModelForAudioXVector"),b(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Mt,"href","#optimum.intel.OVModelForAudioXVector"),b(xe,"class","relative group"),b(hn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForAudioXVector"),b(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(vt,"id","optimum.intel.OVModelForImageClassification"),b(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(vt,"href","#optimum.intel.OVModelForImageClassification"),b(Xe,"class","relative group"),b(un,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForImageClassification"),b(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b($t,"id","optimum.intel.OVModelForCausalLM"),b($t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b($t,"href","#optimum.intel.OVModelForCausalLM"),b(qe,"class","relative group"),b(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(fn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForCausalLM"),b(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Ct,"id","optimum.intel.OVModelForSeq2SeqLM"),b(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Ct,"href","#optimum.intel.OVModelForSeq2SeqLM"),b(Be,"class","relative group"),b(gn,"href","/docs/optimum.intel/main/en/reference_ov#optimum.intel.OVModelForSeq2SeqLM"),b(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Ot,"id","optimum.intel.OVQuantizer"),b(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),b(Ot,"href","#optimum.intel.OVQuantizer"),b(Se,"class","relative group"),b(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,y){e(document.head,n),v(t,f,y),v(t,p,y),e(p,m),e(m,u),F(s,u,null),e(p,a),e(p,w),e(w,h),v(t,_,y),v(t,j,y),e(j,Z),e(Z,U),F(fe,U,null),e(j,nn),e(j,Cn),e(Cn,tl),v(t,sa,y),v(t,z,y),F(Pt,z,null),e(z,ol),e(z,Jn),e(Jn,nl),e(z,sl),e(z,Ht),e(Ht,al),e(Ht,jn),e(jn,ll),e(Ht,il),e(z,rl),e(z,H),F(Kt,H,null),e(H,dl),e(H,_e),e(_e,cl),e(_e,sn),e(sn,pl),e(_e,ml),e(_e,Zn),e(Zn,hl),e(_e,ul),e(H,fl),F(Ye,H,null),e(H,gl),F(Le,H,null),v(t,aa,y),v(t,$e,y),e($e,De),e(De,On),F(eo,On,null),e($e,Ml),e($e,Un),e(Un,yl),v(t,la,y),v(t,S,y),F(to,S,null),e(S,bl),e(S,En),e(En,vl),e(S,wl),e(S,oo),e(oo,_l),e(oo,Wn),e(Wn,$l),e(oo,kl),e(S,Vl),e(S,K),F(no,K,null),e(K,Tl),e(K,ke),e(ke,Fl),e(ke,an),e(an,Cl),e(ke,Jl),e(ke,In),e(In,jl),e(ke,Zl),e(K,Ol),F(Pe,K,null),e(K,Ul),F(He,K,null),v(t,ia,y),v(t,Ve,y),e(Ve,Ke),e(Ke,xn),F(so,xn,null),e(Ve,El),e(Ve,Gn),e(Gn,Wl),v(t,ra,y),v(t,A,y),F(ao,A,null),e(A,Il),e(A,Xn),e(Xn,xl),e(A,Gl),e(A,lo),e(lo,Xl),e(lo,Rn),e(Rn,Rl),e(lo,ql),e(A,Nl),e(A,ee),F(io,ee,null),e(ee,Bl),e(ee,Te),e(Te,zl),e(Te,ln),e(ln,Sl),e(Te,Al),e(Te,qn),e(qn,Ql),e(Te,Yl),e(ee,Ll),F(et,ee,null),e(ee,Dl),F(tt,ee,null),v(t,da,y),v(t,Fe,y),e(Fe,ot),e(ot,Nn),F(ro,Nn,null),e(Fe,Pl),e(Fe,Bn),e(Bn,Hl),v(t,ca,y),v(t,Q,y),F(co,Q,null),e(Q,Kl),e(Q,zn),e(zn,ei),e(Q,ti),e(Q,po),e(po,oi),e(po,Sn),e(Sn,ni),e(po,si),e(Q,ai),e(Q,te),F(mo,te,null),e(te,li),e(te,Ce),e(Ce,ii),e(Ce,rn),e(rn,ri),e(Ce,di),e(Ce,An),e(An,ci),e(Ce,pi),e(te,mi),F(nt,te,null),e(te,hi),F(st,te,null),v(t,pa,y),v(t,Je,y),e(Je,at),e(at,Qn),F(ho,Qn,null),e(Je,ui),e(Je,Yn),e(Yn,fi),v(t,ma,y),v(t,Y,y),F(uo,Y,null),e(Y,gi),e(Y,Ln),e(Ln,Mi),e(Y,yi),e(Y,fo),e(fo,bi),e(fo,Dn),e(Dn,vi),e(fo,wi),e(Y,_i),e(Y,oe),F(go,oe,null),e(oe,$i),e(oe,je),e(je,ki),e(je,dn),e(dn,Vi),e(je,Ti),e(je,Pn),e(Pn,Fi),e(je,Ci),e(oe,Ji),F(lt,oe,null),e(oe,ji),F(it,oe,null),v(t,ha,y),v(t,Ze,y),e(Ze,rt),e(rt,Hn),F(Mo,Hn,null),e(Ze,Zi),e(Ze,Kn),e(Kn,Oi),v(t,ua,y),v(t,L,y),F(yo,L,null),e(L,Ui),e(L,es),e(es,Ei),e(L,Wi),e(L,bo),e(bo,Ii),e(bo,ts),e(ts,xi),e(bo,Gi),e(L,Xi),e(L,ne),F(vo,ne,null),e(ne,Ri),e(ne,Oe),e(Oe,qi),e(Oe,cn),e(cn,Ni),e(Oe,Bi),e(Oe,os),e(os,zi),e(Oe,Si),e(ne,Ai),F(dt,ne,null),e(ne,Qi),F(ct,ne,null),v(t,fa,y),v(t,Ue,y),e(Ue,pt),e(pt,ns),F(wo,ns,null),e(Ue,Yi),e(Ue,ss),e(ss,Li),v(t,ga,y),v(t,x,y),F(_o,x,null),e(x,Di),e(x,as),e(as,Pi),e(x,Hi),e(x,$o),e($o,Ki),e($o,ls),e(ls,er),e($o,tr),e(x,or),e(x,is),e(is,nr),e(x,sr),e(x,se),F(ko,se,null),e(se,ar),e(se,Ee),e(Ee,lr),e(Ee,pn),e(pn,ir),e(Ee,rr),e(Ee,rs),e(rs,dr),e(Ee,cr),e(se,pr),F(mt,se,null),e(se,mr),F(ht,se,null),v(t,Ma,y),v(t,We,y),e(We,ut),e(ut,ds),F(Vo,ds,null),e(We,hr),e(We,cs),e(cs,ur),v(t,ya,y),v(t,G,y),F(To,G,null),e(G,fr),e(G,ps),e(ps,gr),e(G,Mr),e(G,Fo),e(Fo,yr),e(Fo,ms),e(ms,br),e(Fo,vr),e(G,wr),e(G,hs),e(hs,_r),e(G,$r),e(G,ae),F(Co,ae,null),e(ae,kr),e(ae,Ie),e(Ie,Vr),e(Ie,mn),e(mn,Tr),e(Ie,Fr),e(Ie,us),e(us,Cr),e(Ie,Jr),e(ae,jr),F(ft,ae,null),e(ae,Zr),F(gt,ae,null),v(t,ba,y),v(t,xe,y),e(xe,Mt),e(Mt,fs),F(Jo,fs,null),e(xe,Or),e(xe,gs),e(gs,Ur),v(t,va,y),v(t,X,y),F(jo,X,null),e(X,Er),e(X,Ms),e(Ms,Wr),e(X,Ir),e(X,Zo),e(Zo,xr),e(Zo,ys),e(ys,Gr),e(Zo,Xr),e(X,Rr),e(X,bs),e(bs,qr),e(X,Nr),e(X,le),F(Oo,le,null),e(le,Br),e(le,Ge),e(Ge,zr),e(Ge,hn),e(hn,Sr),e(Ge,Ar),e(Ge,vs),e(vs,Qr),e(Ge,Yr),e(le,Lr),F(yt,le,null),e(le,Dr),F(bt,le,null),v(t,wa,y),v(t,Xe,y),e(Xe,vt),e(vt,ws),F(Uo,ws,null),e(Xe,Pr),e(Xe,_s),e(_s,Hr),v(t,_a,y),v(t,D,y),F(Eo,D,null),e(D,Kr),e(D,$s),e($s,ed),e(D,td),e(D,Wo),e(Wo,od),e(Wo,ks),e(ks,nd),e(Wo,sd),e(D,ad),e(D,ie),F(Io,ie,null),e(ie,ld),e(ie,Re),e(Re,id),e(Re,un),e(un,rd),e(Re,dd),e(Re,Vs),e(Vs,cd),e(Re,pd),e(ie,md),F(wt,ie,null),e(ie,hd),F(_t,ie,null),v(t,$a,y),v(t,qe,y),e(qe,$t),e($t,Ts),F(xo,Ts,null),e(qe,ud),e(qe,Fs),e(Fs,fd),v(t,ka,y),v(t,R,y),F(Go,R,null),e(R,gd),e(R,Cs),e(Cs,Md),e(R,yd),e(R,Xo),e(Xo,bd),e(Xo,Js),e(Js,vd),e(Xo,wd),e(R,_d),e(R,kt),F(Ro,kt,null),e(kt,$d),e(kt,qo),e(qo,kd),e(qo,js),e(js,Vd),e(qo,Td),e(R,Fd),e(R,N),F(No,N,null),e(N,Cd),e(N,Ne),e(Ne,Jd),e(Ne,fn),e(fn,jd),e(Ne,Zd),e(Ne,Zs),e(Zs,Od),e(Ne,Ud),e(N,Ed),F(Vt,N,null),e(N,Wd),F(Tt,N,null),e(N,Id),F(Ft,N,null),v(t,Va,y),v(t,Be,y),e(Be,Ct),e(Ct,Os),F(Bo,Os,null),e(Be,xd),e(Be,Us),e(Us,Gd),v(t,Ta,y),v(t,ce,y),F(zo,ce,null),e(ce,Xd),e(ce,Es),e(Es,Rd),e(ce,qd),e(ce,B),F(So,B,null),e(B,Nd),e(B,ze),e(ze,Bd),e(ze,gn),e(gn,zd),e(ze,Sd),e(ze,Ws),e(Ws,Ad),e(ze,Qd),e(B,Yd),F(Jt,B,null),e(B,Ld),F(jt,B,null),e(B,Dd),F(Zt,B,null),v(t,Fa,y),v(t,Se,y),e(Se,Ot),e(Ot,Is),F(Ao,Is,null),e(Se,Pd),e(Se,xs),e(xs,Hd),v(t,Ca,y),v(t,P,y),F(Qo,P,null),e(P,Kd),e(P,Gs),e(Gs,ec),e(P,tc),e(P,Ut),F(Yo,Ut,null),e(Ut,oc),e(Ut,Lo),e(Lo,nc),e(Lo,Xs),e(Xs,sc),e(Lo,ac),e(P,lc),e(P,re),F(Do,re,null),e(re,ic),e(re,Po),e(Po,rc),e(Po,Rs),e(Rs,dc),e(Po,cc),e(re,pc),F(Et,re,null),e(re,mc),F(Wt,re,null),Ja=!0},p(t,[y]){const Ho={};y&2&&(Ho.$$scope={dirty:y,ctx:t}),Ye.$set(Ho);const qs={};y&2&&(qs.$$scope={dirty:y,ctx:t}),Le.$set(qs);const Ns={};y&2&&(Ns.$$scope={dirty:y,ctx:t}),Pe.$set(Ns);const Bs={};y&2&&(Bs.$$scope={dirty:y,ctx:t}),He.$set(Bs);const Ko={};y&2&&(Ko.$$scope={dirty:y,ctx:t}),et.$set(Ko);const zs={};y&2&&(zs.$$scope={dirty:y,ctx:t}),tt.$set(zs);const Ss={};y&2&&(Ss.$$scope={dirty:y,ctx:t}),nt.$set(Ss);const As={};y&2&&(As.$$scope={dirty:y,ctx:t}),st.$set(As);const pe={};y&2&&(pe.$$scope={dirty:y,ctx:t}),lt.$set(pe);const Qs={};y&2&&(Qs.$$scope={dirty:y,ctx:t}),it.$set(Qs);const en={};y&2&&(en.$$scope={dirty:y,ctx:t}),dt.$set(en);const Ys={};y&2&&(Ys.$$scope={dirty:y,ctx:t}),ct.$set(Ys);const me={};y&2&&(me.$$scope={dirty:y,ctx:t}),mt.$set(me);const Ae={};y&2&&(Ae.$$scope={dirty:y,ctx:t}),ht.$set(Ae);const Ls={};y&2&&(Ls.$$scope={dirty:y,ctx:t}),ft.$set(Ls);const Ds={};y&2&&(Ds.$$scope={dirty:y,ctx:t}),gt.$set(Ds);const tn={};y&2&&(tn.$$scope={dirty:y,ctx:t}),yt.$set(tn);const Ps={};y&2&&(Ps.$$scope={dirty:y,ctx:t}),bt.$set(Ps);const Hs={};y&2&&(Hs.$$scope={dirty:y,ctx:t}),wt.$set(Hs);const Ks={};y&2&&(Ks.$$scope={dirty:y,ctx:t}),_t.$set(Ks);const he={};y&2&&(he.$$scope={dirty:y,ctx:t}),Vt.$set(he);const ea={};y&2&&(ea.$$scope={dirty:y,ctx:t}),Tt.$set(ea);const on={};y&2&&(on.$$scope={dirty:y,ctx:t}),Ft.$set(on);const ta={};y&2&&(ta.$$scope={dirty:y,ctx:t}),Jt.$set(ta);const ue={};y&2&&(ue.$$scope={dirty:y,ctx:t}),jt.$set(ue);const Qe={};y&2&&(Qe.$$scope={dirty:y,ctx:t}),Zt.$set(Qe);const oa={};y&2&&(oa.$$scope={dirty:y,ctx:t}),Et.$set(oa);const na={};y&2&&(na.$$scope={dirty:y,ctx:t}),Wt.$set(na)},i(t){Ja||($(s.$$.fragment,t),$(fe.$$.fragment,t),$(Pt.$$.fragment,t),$(Kt.$$.fragment,t),$(Ye.$$.fragment,t),$(Le.$$.fragment,t),$(eo.$$.fragment,t),$(to.$$.fragment,t),$(no.$$.fragment,t),$(Pe.$$.fragment,t),$(He.$$.fragment,t),$(so.$$.fragment,t),$(ao.$$.fragment,t),$(io.$$.fragment,t),$(et.$$.fragment,t),$(tt.$$.fragment,t),$(ro.$$.fragment,t),$(co.$$.fragment,t),$(mo.$$.fragment,t),$(nt.$$.fragment,t),$(st.$$.fragment,t),$(ho.$$.fragment,t),$(uo.$$.fragment,t),$(go.$$.fragment,t),$(lt.$$.fragment,t),$(it.$$.fragment,t),$(Mo.$$.fragment,t),$(yo.$$.fragment,t),$(vo.$$.fragment,t),$(dt.$$.fragment,t),$(ct.$$.fragment,t),$(wo.$$.fragment,t),$(_o.$$.fragment,t),$(ko.$$.fragment,t),$(mt.$$.fragment,t),$(ht.$$.fragment,t),$(Vo.$$.fragment,t),$(To.$$.fragment,t),$(Co.$$.fragment,t),$(ft.$$.fragment,t),$(gt.$$.fragment,t),$(Jo.$$.fragment,t),$(jo.$$.fragment,t),$(Oo.$$.fragment,t),$(yt.$$.fragment,t),$(bt.$$.fragment,t),$(Uo.$$.fragment,t),$(Eo.$$.fragment,t),$(Io.$$.fragment,t),$(wt.$$.fragment,t),$(_t.$$.fragment,t),$(xo.$$.fragment,t),$(Go.$$.fragment,t),$(Ro.$$.fragment,t),$(No.$$.fragment,t),$(Vt.$$.fragment,t),$(Tt.$$.fragment,t),$(Ft.$$.fragment,t),$(Bo.$$.fragment,t),$(zo.$$.fragment,t),$(So.$$.fragment,t),$(Jt.$$.fragment,t),$(jt.$$.fragment,t),$(Zt.$$.fragment,t),$(Ao.$$.fragment,t),$(Qo.$$.fragment,t),$(Yo.$$.fragment,t),$(Do.$$.fragment,t),$(Et.$$.fragment,t),$(Wt.$$.fragment,t),Ja=!0)},o(t){k(s.$$.fragment,t),k(fe.$$.fragment,t),k(Pt.$$.fragment,t),k(Kt.$$.fragment,t),k(Ye.$$.fragment,t),k(Le.$$.fragment,t),k(eo.$$.fragment,t),k(to.$$.fragment,t),k(no.$$.fragment,t),k(Pe.$$.fragment,t),k(He.$$.fragment,t),k(so.$$.fragment,t),k(ao.$$.fragment,t),k(io.$$.fragment,t),k(et.$$.fragment,t),k(tt.$$.fragment,t),k(ro.$$.fragment,t),k(co.$$.fragment,t),k(mo.$$.fragment,t),k(nt.$$.fragment,t),k(st.$$.fragment,t),k(ho.$$.fragment,t),k(uo.$$.fragment,t),k(go.$$.fragment,t),k(lt.$$.fragment,t),k(it.$$.fragment,t),k(Mo.$$.fragment,t),k(yo.$$.fragment,t),k(vo.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(wo.$$.fragment,t),k(_o.$$.fragment,t),k(ko.$$.fragment,t),k(mt.$$.fragment,t),k(ht.$$.fragment,t),k(Vo.$$.fragment,t),k(To.$$.fragment,t),k(Co.$$.fragment,t),k(ft.$$.fragment,t),k(gt.$$.fragment,t),k(Jo.$$.fragment,t),k(jo.$$.fragment,t),k(Oo.$$.fragment,t),k(yt.$$.fragment,t),k(bt.$$.fragment,t),k(Uo.$$.fragment,t),k(Eo.$$.fragment,t),k(Io.$$.fragment,t),k(wt.$$.fragment,t),k(_t.$$.fragment,t),k(xo.$$.fragment,t),k(Go.$$.fragment,t),k(Ro.$$.fragment,t),k(No.$$.fragment,t),k(Vt.$$.fragment,t),k(Tt.$$.fragment,t),k(Ft.$$.fragment,t),k(Bo.$$.fragment,t),k(zo.$$.fragment,t),k(So.$$.fragment,t),k(Jt.$$.fragment,t),k(jt.$$.fragment,t),k(Zt.$$.fragment,t),k(Ao.$$.fragment,t),k(Qo.$$.fragment,t),k(Yo.$$.fragment,t),k(Do.$$.fragment,t),k(Et.$$.fragment,t),k(Wt.$$.fragment,t),Ja=!1},d(t){o(n),t&&o(f),t&&o(p),C(s),t&&o(_),t&&o(j),C(fe),t&&o(sa),t&&o(z),C(Pt),C(Kt),C(Ye),C(Le),t&&o(aa),t&&o($e),C(eo),t&&o(la),t&&o(S),C(to),C(no),C(Pe),C(He),t&&o(ia),t&&o(Ve),C(so),t&&o(ra),t&&o(A),C(ao),C(io),C(et),C(tt),t&&o(da),t&&o(Fe),C(ro),t&&o(ca),t&&o(Q),C(co),C(mo),C(nt),C(st),t&&o(pa),t&&o(Je),C(ho),t&&o(ma),t&&o(Y),C(uo),C(go),C(lt),C(it),t&&o(ha),t&&o(Ze),C(Mo),t&&o(ua),t&&o(L),C(yo),C(vo),C(dt),C(ct),t&&o(fa),t&&o(Ue),C(wo),t&&o(ga),t&&o(x),C(_o),C(ko),C(mt),C(ht),t&&o(Ma),t&&o(We),C(Vo),t&&o(ya),t&&o(G),C(To),C(Co),C(ft),C(gt),t&&o(ba),t&&o(xe),C(Jo),t&&o(va),t&&o(X),C(jo),C(Oo),C(yt),C(bt),t&&o(wa),t&&o(Xe),C(Uo),t&&o(_a),t&&o(D),C(Eo),C(Io),C(wt),C(_t),t&&o($a),t&&o(qe),C(xo),t&&o(ka),t&&o(R),C(Go),C(Ro),C(No),C(Vt),C(Tt),C(Ft),t&&o(Va),t&&o(Be),C(Bo),t&&o(Ta),t&&o(ce),C(zo),C(So),C(Jt),C(jt),C(Zt),t&&o(Fa),t&&o(Se),C(Ao),t&&o(Ca),t&&o(P),C(Qo),C(Yo),C(Do),C(Et),C(Wt)}}}const Zm={local:"reference",sections:[{local:"optimum.intel.OVModelForFeatureExtraction",title:"OVModelForFeatureExtraction"},{local:"optimum.intel.OVModelForMaskedLM",title:"OVModelForMaskedLM"},{local:"optimum.intel.OVModelForQuestionAnswering",title:"OVModelForQuestionAnswering"},{local:"optimum.intel.OVModelForSequenceClassification",title:"OVModelForSequenceClassification"},{local:"optimum.intel.OVModelForTokenClassification",title:"OVModelForTokenClassification"},{local:"optimum.intel.OVModelForAudioClassification",title:"OVModelForAudioClassification"},{local:"optimum.intel.OVModelForAudioFrameClassification",title:"OVModelForAudioFrameClassification"},{local:"optimum.intel.OVModelForCTC",title:"OVModelForCTC"},{local:"optimum.intel.OVModelForAudioXVector",title:"OVModelForAudioXVector"},{local:"optimum.intel.OVModelForImageClassification",title:"OVModelForImageClassification"},{local:"optimum.intel.OVModelForCausalLM",title:"OVModelForCausalLM"},{local:"optimum.intel.OVModelForSeq2SeqLM",title:"OVModelForSeq2SeqLM"},{local:"optimum.intel.OVQuantizer",title:"OVQuantizer"}],title:"Reference"};function Om(J){return Ap(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xm extends hc{constructor(n){super();uc(this,n,Om,jm,fc,{})}}export{xm as default,Zm as metadata};
